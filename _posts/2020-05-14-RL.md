---
layout: post
title:  "强化学习-Reinforcement Learning"
date:   2020-05-14 17:34:00
categories: 强化学习
tags: 深度学习 强化学习 增强学习
excerpt: AI=DL+RL，那么RL强化学习是什么，包含哪些内容，有哪些典型应用？
author: 鹤啸九天
mathjax: true
---

* content
{:toc}

# 总结

- 【2021-3-18】[大年初一，一起来深度强化学习](https://zhuanlan.zhihu.com/p/350506752)，Thomas Simonini 的 Deep Reinforcement Learning Course的课程笔记
- 【2020-7-4】【(EEML2020)强化学习教程(Colab)】“[EEML2020 RL Tutorial](https://github.com/eemlcommunity/PracticalSessions2020/blob/master/rl/EEML2020_RL_Tutorial.ipynb)” 
- 【2020-7-5】XRL：可解释强化学习《[XRL: eXplainable Reinforcement Learning](https://towardsdatascience.com/xrl-explainable-reinforcement-learning-4cd065cdec9a)》by Meet Gandhi
- 【2021-2-27】【[杜伦大学10小时强化学习课程](https://www.bilibili.com/video/BV1vN411X7qB/)】“Reinforcement Learning Lectures  Durham University” by Chris G. Willcocks
  - <iframe src="//player.bilibili.com/player.html?aid=501767046&bvid=BV1vN411X7qB&cid=302763703&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"  width="100%" height="600"> </iframe>
- 【2021-3-17】[强化学习的“神话”和“鬼话”](https://zhuanlan.zhihu.com/p/196421049)，Csaba Szepesvári在2020年数据挖掘顶会KDD的Deep Learning Day做了题为Myths and Misconceptions in Reinforcement Learning的讲座。Csaba Szepesvári是阿尔伯塔大学计算机系教授，在Deepmind领导Foundations团队。他于2010年出版Algorithms for Reinforcement Learning. 最近出版Bandit Algorithms. 他于2006年发表题为Bandit based Monte-Carlo Planning的论文，提出UCT算法，对AlphaGo的研发起到了关键作用。
  - 内容：
    - 1）要不要学习RL
      - 误解：有人说，RL就是指一些特定的算法，比如TD、DQN、PPO，等等；而像ES(evolutionary search, 进化搜索)、随机搜索、SSL(self-supervised learning, 自我监督学习)等等这些就不是RL.
      - 三类基本的RL问题：Online RL（智能体直接与环境交互学习）, Batch RL（历史数据中学习）, Planning/simulation optimization（规划/仿真优化）. 当然有很多变种。
    - 2）RL是不是有很多问题
    - 3）RL与相邻学科的关系如何
    - 然后讨论一些“Meta consideration”
  - [原版ppt及youtube视频](https://sites.ualberta.ca/~szepesva/talks.html)，[model based RL ppt](https://www.dropbox.com/s/8b4eivix82tfp2z/ModelBasedRL2.pptx?dl=0)
- 【2021-3-19】人工智能拒绝内卷，AI研习社文章：[开局一头狼六只羊，这个狼吃羊的AI火了！傻狼拒绝内卷：抓羊可太累了，我只想自杀......](https://mp.weixin.qq.com/s?__biz=MzA5ODEzMjIyMA==&mid=2247585920&idx=1&sn=ad78a94ded54c54f092d4694e11ece25&chksm=90959913a7e210053febe37a27bff456a136aec2c8e3dd4fc15d71cdcea648a5663105f553ca&mpshare=1&scene=1&srcid=0319JgBKEY71NSi5iVRaOgZS&sharer_sharetime=1616158813661&sharer_shareid=b8d409494a5439418f4a89712efcd92a&version=3.1.0.6189&platform=mac#rd)
  - ![](https://wx3.sinaimg.cn/mw690/007dt7foly1gog7nqut5hj30v81glqb8.jpg)
  - ![](https://mmbiz.qpic.cn/mmbiz_png/cNFA8C0uVPs54GiaOqXqvGBOQiaLTD7aZfbvfibRcRUqz4PDXWSFTBWn2JApWS5uPsnFqxtRPDEaKsZic0QBGls3ww/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)
  - ![](https://pic2.zhimg.com/80/v2-abc6b3d9a109ae9c1a182f322f069363_1440w.jpg?source=1940ef5c)
    - 知乎曾伊言的回答[这样的AI是真的吗？](https://www.zhihu.com/question/448931860/answer/1779097110) ：这远不是通用人工智能，回报函数(reward function)的设计者若理解期望折扣回报(expected discounted return)，就不会犯这种错，也不需要研究三天。
  - ①狼就是打工人…每秒扣的是青春和时间，羊是永远达不到的“升职、加薪、迎娶白富美、走上人生巅峰”，撞石头就是躺平摸鱼
  - ②阿西莫夫三定律是多么重要啊，不然以效率和结果为一切指标的ai，非常可能在很多场景中计算后发现，人类+自己毁灭后效益最大，然后采取灭世措施。
  - [微博原贴](https://weibo.com/6611961566/K5J8t8XfM?type=comment#_rnd1616126336679)
  - 星尘研表示狼自杀的错误是很多东西共同影响产生的，最主要的一个错误是迭代次数太少，20W次完全不够学，后面提高到100W次起步, 效果直线上升。
  - ![](https://mmbiz.qpic.cn/mmbiz_gif/cNFA8C0uVPs54GiaOqXqvGBOQiaLTD7aZf8IMo27fyqEvd9WG6ZTYc8l4OvicctJD2GWkKDc89VBxHmqBib5ZQKaOw/640?wx_fmt=gif&tp=webp&wxfrom=5&wx_lazy=1)
  - [今天微博上好像有一个内卷AI狼...](https://www.bilibili.com/video/BV16X4y1V7Yu?from=search&seid=10508566856251031934)
  - <iframe src="//player.bilibili.com/player.html?aid=714560902&bvid=BV16X4y1V7Yu&cid=309180940&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"  width="100%" height="600"> </iframe>

- 【2021-3-11】[为什么说强化学习在近年不会被广泛应用？](https://mp.weixin.qq.com/s/keXcOu5CMip-rwFQ_oQLyg)，[知乎帖子](https://www.zhihu.com/question/404471029)
- Richard S. Sutton and Andrew G. Barto的[Reinforcement Learning: An Introduction, Second edition](https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf)


# 什么是强化学习

## 定义

- 强化学习的思想为：**agent**通过与**环境**的交互（其实就是试错）得到**奖励**（可正可负，负的奖励就是惩罚）作为反馈，从而做出对应的**action**。这个思想很符合自然经验，小时候学走路，摔了会痛（奖励为负），走得稳了有糖吃（奖励为正），为了多吃点糖（取得更多的奖励），你最终学会了走路。
  - ![](https://pic1.zhimg.com/80/v2-74f512eaf6fc5f68d424ec37dc7bb144_720w.jpg)
- 接下来给出强化学习的正式定义：

> Reinforcement learning is a framework for solving control tasks (also called decision problems) by building agents that learn from the environment by interacting with it through trial and error and receiving rewards (positive or negative) as unique feedback.

## 基本组件

- 强化学习强化学习系统由**智能体**（Agent）、**状态**（state）、**奖赏**（reward）、**动作**（action）和**环境**（Environment）五部分组成，如下图所示。
  - ![](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz/j3gficicyOvavlKDwRa4UIy5IjcjDAm1CPvPMxyK50fy1B5VEd2pt4NeCRaeLlkXMibyiam9cRsmPo3VLpZfpkuNLw/640?wx_fmt=other)
  - `Agent`：智能体是整个强化学习系统核心。它能够感知环境的状态（State），并且根据环境提供的奖励信号（Reward），通过学习选择一个合适的动作（Action），来最大化长期的Reward值。简而言之，Agent就是根据环境提供的Reward作为反馈，学习一系列的环境状态（State）到**动作**（Action）的映射，动作选择的原则是最大化未来累积的Reward的概率。选择的动作不仅影响当前时刻的Reward，还会影响下一时刻甚至未来的Reward，因此，Agent在学习过程中的基本规则是：如果某个动作（Action）带来了环境的正回报（Reward），那么这一动作会被加强，反之则会逐渐削弱，类似于物理学中条件反射原理。
  - `Environment`：环境会接收Agent执行的一系列的动作（Action），并且对这一系列的动作的好坏进行评价，并转换成一种可量化的（标量信号）Reward反馈给Agent，而不会告诉Agent应该如何去学习动作。Agent只能靠自己的历史（History）经历去学习。同时，环境还像Agent提供它所处的状态（State）信息。
  - `Reward`：环境提供给Agent的一个可量化的标量反馈信号，用于评价Agent在某一个时间步所做action的好坏。强化学习就是基于一种最大化累计奖赏假设：强化学习中，Agent进行一系列的动作选择的目标是最大化未来的累计奖赏。
  - `State`：状态指Agent所处的环境信息，包含了智能体用于进行Action选择的所有信息，它是历史（History）的一个函数：St = f（Ht）。
- 强化学习的主体是Agent和环境Environment。Agent为了适应环境，做出的一系列的动作，使最终的奖励最高，同时在此过程中更新特定的参数。实际上可以把强化学习简单理解成是一种循环，具体的工作方式如下：
  - 智能体从环境中获取一个状态St；
  - 智能体根据状态St采取一个动作at；
  - 受到at的影响，环境发生变化，转换到新的状态St+1；
  - 环境反馈给智能体一个奖励（正向为奖励，负向则为惩罚）。
- 参考：[强化学习在智能对话上的应用](https://blog.csdn.net/Tencent_TEG/article/details/88859179)

- 动作空间
  - 动作空间为在一个环境中所有可能动作的集合，可以是离散空间，比如有的游戏只能上下左右移动，也可以是连续空间，比如自动驾驶时的转角可能是无穷的。
- 奖励与折扣
  - 奖励就是反馈，它能让agent知道动作是好是坏，每个时刻的累积奖励可以写为
    - ![](https://www.zhihu.com/equation?tex=%5C%5BR%28%5Ctau+%29+%3D+%7Br_%7Bt+%2B+1%7D%7D+%2B+%7Br_%7Bt+%2B+2%7D%7D+%2B+%7Br_%7Bt+%2B+3%7D%7D+%2B+...%5C%5D)
  - 其中τ代表状态和动作的序列。但是实际上我们并不直接这样相加，因为预测太远的事情总是不准的，我们一般选择更看重眼前的奖励，所以需要对未来的奖励进行衰减
    - ![](https://www.zhihu.com/equation?tex=%5C%5BR%28%5Ctau+%29+%3D+%7Br_%7Bt+%2B+1%7D%7D+%2B+%5Cgamma+%7Br_%7Bt+%2B+2%7D%7D+%2B+%7B%5Cgamma+%5E2%7D%7Br_%7Bt+%2B+3%7D%7D+%2B+...%5C%5D)
  - 其中γ是衰减因子来权衡我们对长远奖励的重视程度。
- 强化学习假设
  - ① **奖励假说**：所有的目标可以被描述为**最大化期望回报**，为了做出最好的决策，需要去最大化期望回报。
  - ② **马尔可夫决策过程**：agent每次只需根据**当前**的状态就能做出决策，而不依赖于**过去**的状态和动作信息。
- 强化学习的任务有两类：
  - **周期**任务（比如你在玩超级玛丽，每次从一条新的生命开始，直到游戏人物死亡或者到达终点结束）
  - **连续**任务（比如做自动化股票交易，这里就没有起始和终点这一说了，agent在做出做好决策的同时不断和环境进行交互）
- Exploration/ Exploitation的权衡
  - Exploration就是去探索更多的环境信息，Exploitation就是利用已知的环境信息最大化奖励。在强化学习中，我们既要Exploration又要Exploitation，这样才能做出更好的决策。这里举了个饭店选择的例子：Exploration就是去尝试你没吃过的饭店，这样可能会踩雷，但是也可能挖掘到更好吃的饭店！Exploitation就是每天去你知道的好吃的饭店吃饭，但这样可能错过一些更好吃的饭店。

# 算法

## 求解方法

- 既然需要最大化期望回报来做出更好的决策，那么这一过程如何进行的呢？
- 假设agent接收到一个状态然后给出相应的动作，那么这一过程就可以理解成是一个控制律（一个从状态到动作的函数）
  - ![](https://pic3.zhimg.com/80/v2-009413beffb5736398eacebd6811c8fe_720w.jpg)
  - 控制律π好比agent的大脑，也是我们希望去学习出来的，我们的目标就是找到最好的控制律使得期望回报最大化。
- （1）直接法
  - 给定状态并告诉agent应该采取什么动作，也就是直接学习这个控制律，称为policy-based方法。
  - 控制率是确定的（即状态和动作一一对应） ![](https://www.zhihu.com/equation?tex=a%3D%5Cpi%28s%29)，如动作的概率分布![](https://www.zhihu.com/equation?tex=%5Cpi%28a%7Cs%29%3DP%28A%7Cs%29)
  - ![](https://pic1.zhimg.com/80/v2-8350cacc652c29f252d0b27371b0cee8_720w.jpg)
- （2）间接法
  - 不告诉agent什么动作是最好的，而是告诉它什么状态是更有价值的，而能到达更好价值的状态需要采取什么动作也就知道了，称为value-based方法。
  - 需要训练一个函数能将当前状态映射成（某一控制律下）期望回报
    - ![](https://www.zhihu.com/equation?tex=%5C%5B%7Bv_%5Cpi+%7D%28s%29+%3D+%5Cmathbb%7BE%7D%7B_%5Cpi+%7D%5Cleft%5B+%7B%7BR_%7Bt+%2B+1%7D%7D+%2B+%5Cgamma+%7BR_%7Bt+%2B+2%7D%7D+%2B+%7B%5Cgamma+%5E2%7D%7BR_%7Bt+%2B+3%7D%7D+%2B+...%7C%7BS_t%7D+%3D+s%7D+%5Cright%5D%5C%5D)
  - ![](https://pic3.zhimg.com/80/v2-6e1a3720ceeedfdb1d518ddc0f0cc232_720w.jpg)
- 总结
  - 无论是采用policy还是value-based方法，都会有一个policy，在policy-based方法中这个policy是通过训练直接得到的，而在value-based方法中我们不需要去训练policy，这里的policy只是个简单的函数，比如贪婪策略根据最大值来选择动作。两者之间的对应关系可以如下所示
  - ![](https://pic4.zhimg.com/80/v2-40781e074536a63315e3d3230c3e6ac7_720w.jpg)

## 基本概念

### 值函数（value-function）的定义

- Q-learning是一种value-based的强化学习方法，首先我们需要定义两个value函数：
  - 一个是state-value函数，即
    - ![](https://pic1.zhimg.com/80/v2-678f6e45b921862ac2ed112a7e33433c_720w.jpg)
    - 如从期望价值为-7的状态出发，根据贪婪策略，动作为右、右、右、右、下、下、右、右。
    - ![](https://pic1.zhimg.com/80/v2-15d39e8e4870b99693bc56b07dae4bd0_720w.jpg)
  - 另一个是action-value函数，在动作-价值函数中，函数的输入为动作和状态的pair，输出仍然是期望回报
    - ![](https://pic3.zhimg.com/80/v2-f299bc1a427cc355e67893f4d6ac5992_720w.jpg)
- action-value函数与state-value函数的区别
  - 函数值由状态和动作同时决定，也就是说一个状态四个动作对应的value可以是不一样的。
  - ![](https://pic2.zhimg.com/80/v2-6a2a0b713bda8a6f59d87ccc4327133d_720w.jpg)

### Bellman贝尔曼方程

- 函数值如何计算？这里需要引入Bellman方程了，因为期望回报是从当前状态一直计算到终止的，对于每个状态都这样计算是繁琐的，而Bellman方程将相邻状态的函数值关联起来了，即当前状态的函数值，等于从当前状态转移一步的直接奖励加上下一个状态的折扣函数值，这也就是Bellman方程的核心思想，recursive！
  - ![](https://pic1.zhimg.com/80/v2-30a49ed93dcfd3f80076da0c04f77ca8_720w.jpg)
- 贝尔曼方程(Bellman Equation)，百度百科关于贝尔曼方程的介绍

> **贝尔曼方程**（Bellman Equation）也被称作**动态规划方程**（Dynamic Programming Equation），由理查·贝尔曼（Richard Bellman）发现。贝尔曼方程是动态规划（Dynamic Programming）这些数学最佳化方法能够达到最佳化的必要条件。此方程把“决策问题在特定时间怎么的值”以“来自初始选择的报酬比从初始选择衍生的决策问题的值”的形式表示。借此这个方式把动态最佳化问题变成简单的子问题，而这些子问题遵守从贝尔曼所提出来的“最佳化还原理”。

- 总结：贝尔曼方程就是用来简化强化学习或者马尔可夫决策问题
- value function可以分为两部分：
  - 立即回报![](https://www.zhihu.com/equation?tex=R_%7Bt%2B1%7D)
  - 后继状态的折扣价值函数![](https://www.zhihu.com/equation?tex=%5Cgamma+v%28S_%7Bt%2B1%7D%29)

![](https://pic2.zhimg.com/v2-7e5c05147555f1eaf91a257781f4cfd5_b.jpg)
![](https://pic4.zhimg.com/v2-ead0588f9d7e27a5bb0911a7f0fdc1b3_b.jpg)

- [马尔科夫决策过程之Bellman Equation（贝尔曼方程）](https://zhuanlan.zhihu.com/p/35261164)

### Monte Carlo 和 Temporal Difference Learning

- 由于强化学习的agent通过与环境交互得到提升，我们需要考虑如何利用交互的经验信息来更新值函数，从而得到更好的控制律。这里主要考虑两种方法：蒙特卡洛法采用的是完整的一轮经验，而TD法采用的只是一步经验，下面以value-based方法举例说明两种方法的区别
- 蒙特卡洛法利用一条完整的采样来更新函数，比如下图所示，所有的状态函数值为0，采用学习率0.1，不对回报进行衰减，采用贪婪加随机的控制策略，当老鼠走超过十步时停止，最终的路径如箭头所示
  - ![](https://pic4.zhimg.com/80/v2-8f9a0c03bd8803ac3091f20ebc031383_720w.jpg)
- 得到了一系列的状态、动作、奖励，那么我们需要计算出它的回报，从而根据下面公式更新state-value函数
  - ![](https://pic3.zhimg.com/80/v2-0ab47f5e0eda7179929f955f57eb9496_720w.jpg)
- 假设吃到一个奶酪的奖励为1，则回报为 Gt = 1+0+0+0+0+0+1+1+0+0=3，对值函数进行更新为 V(S0)= V(S0)+lr*[Gt- V(S0)]=0.3。
- TD法则不需要等一整个过程结束才更新函数值，而是每一步都能更新，但是由于只走了一步我们并不知道后面的奖励，因此TD采取了自助的方式，利用上一次的估计值V(St+1)来替代。
  - ![](https://pic3.zhimg.com/80/v2-638b08a88f3a37e7c7cb4e95bf03f682_720w.jpg)
  - 当老鼠走了一步后即可更新值函数：V(S0)=V(S0)+lr*[ R1+gamma*V(S1)-V(S0) ]=0.1

## Q-learning


### 什么是Q-learning？

> Q-Learning is an off-policy value-based method that uses a TD approach to train its action-value function

- 本质上，Q-learning就是去训练action-value函数，也就是Q函数（Q的含义是Quality，表示在某一状态下那个动作的质量有多高），从而得到对应的action。
  - ![](https://pic3.zhimg.com/80/v2-448b0e27e650190f8116ae6df18320c2_720w.jpg)
- 对于离散问题来说，Q函数其实就是一张Q表格，每一个cell代表对应的状态-动作的函数值，有限个个状态，动作空间里有有限动作，对应的Q-table就是
  - ![](https://pic3.zhimg.com/80/v2-8e21b6ba5277fa94ce1409236da96406_720w.jpg)
- 只要给定输入（状态+动作），Q表就能输出对应的Q值。当训练完成得到一个最优的Q函数（表）后，也就有了最佳的控制律，因为已经知道了每一个状态下哪个动作的函数值比较大，关于动作空间最大化Q函数即可得到最优的控制律。
  - ![](https://pic4.zhimg.com/80/v2-40781e074536a63315e3d3230c3e6ac7_720w.jpg)

- Q-learning是强化学习中的一种，在Q-learning中，需要维护一张Q值表，表的维数为：状态数S * 动作数A，表中每个数代表在态s下可以采用动作a可以获得的未来收益的折现和——Q值。不断迭代Q值表使其最终收敛，然后根据Q值表我们就可以在每个状态下选取一个最优策略。参照《[极简Q-learning教程](https://zhuanlan.zhihu.com/p/29213893)》
  - 更新公式如下，R+γmax部分称为Q-target，即使用贝尔曼方程加贪心策略认为实际应该得到的奖励，目标是使Q值不断的接近Q-target值
  - ![](https://upload-images.jianshu.io/upload_images/4155986-23e17f9c5b81efce.png?imageMogr2/auto-orient/strip\|imageView2/2/w/992/format/webp)

### 算法流程

- 整体
  - ![](https://pic2.zhimg.com/80/v2-4e606c07b3b240419c8b896f58900f51_720w.jpg)
- 详情
  - 第一步，初始化Q表，比如全部为0
    - ![](https://pic4.zhimg.com/80/v2-09afc546990e75e6d309bf6e2547a56f_720w.jpg)
  - 第二步，就是根据Epsilon Greedy策略选择动作（既要利用已知的又要探索未知的）
    - ![](https://pic3.zhimg.com/80/v2-d4494bb7dc8eb4c09b0ac0f359378c96_720w.jpg)
    - 在训练的一开始，应该多探索，随着训练越久，Q表越好，应该不断减小探索的概率。
    - ![](https://pic3.zhimg.com/80/v2-cf4c7d7a5c7be299daa573f0adb73a92_720w.jpg)
  - 第三步是执行动作，得到新的奖励，进入下一状态
    - ![](https://pic4.zhimg.com/80/v2-ce2990d388cc1a991a5632a78ee24943_720w.jpg)
  - 第四步就是更新Q表
    - ![](https://pic3.zhimg.com/80/v2-4b0911f8cb15e29a8a3547f619d959ca_720w.jpg)
- 总结
  - 计算TD Target的时候采用的是贪婪策略（对动作空间求Q值得最大），由于动作的实施和更新策略不同，所以Q-learning称为off-policy！因此对应的也有on-policy，比如说Sarsa算法
    - ![](https://pic4.zhimg.com/80/v2-519e8fd6d095a3efcd6729fd7ff5a8cf_720w.jpg)
  - Q-learning的核心就是Q表的更新，但是当问题规模一大，这种简单粗暴的方法显然是不太现实的，因此就有了 Deep Q-learning 的出现了。


## DQN

- 经典强化学习的Q-learning需要创建一个Q表来寻找最佳动作，而深度Q-learning则是采用一个神经网络来近似q值，避免了大型表的记录和存储。
  - ![](https://pic4.zhimg.com/80/v2-c545c9ef6c679b899b81e881a6afe297_720w.jpg)
- DQN是深度学习与强化学习的结合，即使用神经网络代替Q-learning中Q表。在普通的Q-learning中，当状态和动作空间是离散且维数不高时可使用Q-Table储存每个状态动作对的Q值，但是当状态和动作空间是**高维**或者**连续**时，使用Q-Table不现实，而神经网络恰好擅长于此。因此DQN将Q-Table的更新问题变成一个函数拟合问题，相近的状态得到相近的输出动作。如有一个Q值表，神经网络的作用就是给定一个状态s和动作a，预测对应的Q值，使得神经网络的结果与Q表中的值接近。不过DQN的方式肯定不能继续维护一个Q表，所以将上次反馈的奖励作为逼近的目标，如下式，通过更新参数 θ 使Q函数逼近最优Q值。因此，DQN就是要设计一个神经网络结构，通过函数来拟合Q值
- 问题：
  - 神经网络需要**大量带标签的样本**进行监督学习，但是强化学习只有reward返回值，如何构造有监督的数据成为第一个问题，而且伴随着噪声、延迟（过了几十毫秒才返回）、稀疏（很多State的reward是0）等问题；
  - 神经网络的前提是样本**独立同分布**，而强化学习前后state状态和反馈有**依赖**关系——**马尔科夫决策**；
  - 神经网络的目标**分布固定**，但是强化学习的分布一直变化，比如你玩一个游戏，一个关卡和下一个关卡的状态分布是不同的，所以训练好了前一个关卡，下一个关卡又要重新训练；
  - 过往的研究表明，使用非线性网络表示值函数时出现不稳定等问题。
- 针对以上问题的具体解决方案如下：
  - **构造标签**：通过Q-Learning使用reward来构造标签（对应问题1），如上所述，用神经网络来预测reward，将问题转化为一个**回归**问题；
  - **经验回放**：通过experience replay（经验池）的方法来解决**相关性**及**非静态分布**问题（对应问题2、3）；具体做法是把每个时间步agent与环境交互得到的转移样本 (st,at,rt,st+1) 储存到回放记忆单元，要训练时就随机拿出一些（minibatch）来训练。（其实就是将游戏的过程打成碎片存储，训练时随机抽取就避免了相关性问题）
  - 双网络结构：使用一个神经网络产生当前Q值，使用另外一个神经网络产生Target Q值（对应问题4）。在Nature 2015版本的DQN中提出了这个改进，使用另一个网络（这里称为target_net）产生Target Q值。具体地，Q(s,a;θi) 表示当前网络eval_net的输出，用来评估当前状态动作对的值函数；Q(s,a;θ−i) 表示target_net的输出，代入上面求 TargetQ 值的公式中得到目标Q值。根据上面的Loss Function更新eval_net的参数，每经过N轮迭代，将MainNet的参数复制给target_net。引入target_net后，再一段时间里目标Q值使保持不变的，一定程度降低了当前Q值和目标Q值的相关性，提高了算法稳定性。
- 参考：[实战深度强化学习DQN-理论和实践](https://www.jianshu.com/p/10930c371cac?from=singlemessage)

### 代码实现

- 两种DQN的实现方式
  - 一种是将s和a输入到网络，得到q值
  - 另一种是只将s输入到网络，输出为s和每个a结合的q值。
- 莫烦的Demo采用第二种，[Github地址](https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow)
  - ![](https://upload-images.jianshu.io/upload_images/4155986-fd3c59c61907c4ad.png?imageMogr2/auto-orient/strip\|imageView2/2/w/542/format/webp)
- DQN的Tensorflow版本实现，Gym环境
    - [用gym库Classic control实现deep Q learning](https://blog.csdn.net/winycg/article/details/79468320)

- （1）CartPole实例
    - 运载体在一根杆子下无摩擦的跟踪。系统通过施加+1和-1推动运载体。杆子的摇摆在初始时垂直的，目标是阻止它掉落运载体。每一步杆子保持垂直可以获得+1的奖励。episode将会终结于杆子的摇摆幅度超过了离垂直方向的15°或者是运载体偏移初始中心超过2.4个单位。
    - ![](https://img-blog.csdn.net/20180307114338879)
    - 效果图：
        - ![](https://img-blog.csdn.net/20180308173917866)

```python
#https://blog.csdn.net/winycg/article/details/79468320
import numpy as np
import random
import tensorflow as tf
import gym
 
max_episode = 100
env = gym.make('CartPole-v0')
env = env.unwrapped
 
class DeepQNetwork(object):
    def __init__(self,
                 n_actions,
                 n_features,
                 learning_rate=0.01,
                 reward_decay=0.9,  # gamma
                 epsilon_greedy=0.9,  # epsilon
                 epsilon_increment = 0.001,
                 replace_target_iter=300,  # 更新target网络的间隔步数
                 buffer_size=500,  # 样本缓冲区
                 batch_size=32,
                 ):
        self.n_actions = n_actions
        self.n_features = n_features
        self.lr = learning_rate
        self.gamma = reward_decay
        self.epsilon_max = epsilon_greedy
        self.replace_target_iter = replace_target_iter
        self.buffer_size = buffer_size
        self.buffer_counter = 0  # 统计目前进入过buffer的数量
        self.batch_size = batch_size
        self.epsilon = 0 if epsilon_increment is not None else epsilon_greedy
        self.epsilon_max = epsilon_greedy
        self.epsilon_increment = epsilon_increment
        self.learn_step_counter = 0  # 学习计步器
        self.buffer = np.zeros((self.buffer_size, n_features * 2 + 2))  # 初始化Experience buffer[s,a,r,s_]
        self.build_net()
        # 将eval网络中参数全部更新到target网络
        target_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='target_net')
        eval_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='eval_net')
        with tf.variable_scope('soft_replacement'):
            self.target_replace_op = [tf.assign(t, e) for t, e in zip(target_params, eval_params)]
        self.sess = tf.Session()
        tf.summary.FileWriter('logs/', self.sess.graph)
        self.sess.run(tf.global_variables_initializer())
 
    def build_net(self):
        self.s = tf.placeholder(tf.float32, [None, self.n_features])
        self.s_ = tf.placeholder(tf.float32, [None, self.n_features])
        self.r = tf.placeholder(tf.float32, [None, ])
        self.a = tf.placeholder(tf.int32, [None, ])
 
        w_initializer = tf.random_normal_initializer(0., 0.3)
        b_initializer = tf.constant_initializer(0.1)
        # q_eval网络架构，输入状态属性，输出4种动作
        with tf.variable_scope('eval_net'):
            eval_layer = tf.layers.dense(self.s, 20, tf.nn.relu, kernel_initializer=w_initializer,
                                         bias_initializer=b_initializer, name='eval_layer')
            self.q_eval = tf.layers.dense(eval_layer, self.n_actions, kernel_initializer=w_initializer,
                                          bias_initializer=b_initializer, name='output_layer1')
        with tf.variable_scope('target_net'):
            target_layer = tf.layers.dense(self.s_, 20, tf.nn.relu, kernel_initializer=w_initializer,
                                           bias_initializer=b_initializer, name='target_layer')
            self.q_next = tf.layers.dense(target_layer, self.n_actions, kernel_initializer=w_initializer,
                                          bias_initializer=b_initializer, name='output_layer2')
        with tf.variable_scope('q_target'):
            # 计算期望价值，并使用stop_gradient函数将其不计算梯度，也就是当做常数对待
            self.q_target = tf.stop_gradient(self.r + self.gamma * tf.reduce_max(self.q_next, axis=1))
        with tf.variable_scope('q_eval'):
            # 将a的值对应起来，
            a_indices = tf.stack([tf.range(tf.shape(self.a)[0]), self.a], axis=1)
            self.q_eval_a = tf.gather_nd(params=self.q_eval, indices=a_indices)
        with tf.variable_scope('loss'):
            self.loss = tf.reduce_mean(tf.squared_difference(self.q_target, self.q_eval_a))
        with tf.variable_scope('train'):
            self.train_op = tf.train.RMSPropOptimizer(self.lr).minimize(self.loss)
 
            # 存储训练数据
 
    def store_transition(self, s, a, r, s_):
        transition = np.hstack((s, a, r, s_))
        index = self.buffer_counter % self.buffer_size
        self.buffer[index, :] = transition
        self.buffer_counter += 1
 
    def choose_action_by_epsilon_greedy(self, status):
        status = status[np.newaxis, :]
        if random.random() < self.epsilon:
            actions_value = self.sess.run(self.q_eval, feed_dict={self.s: status})
            action = np.argmax(actions_value)
        else:
            action = np.random.randint(0, self.n_actions)
        return action
 
    def learn(self):
        # 每学习self.replace_target_iter步，更新target网络的参数
        if self.learn_step_counter % self.replace_target_iter == 0:
            self.sess.run(self.target_replace_op)
            # 从Experience buffer中选择样本
        sample_index = np.random.choice(min(self.buffer_counter, self.buffer_size), size=self.batch_size)
        batch_buffer = self.buffer[sample_index, :]
        _, cost = self.sess.run([self.train_op, self.loss], feed_dict={
            self.s: batch_buffer[:, :self.n_features],
            self.a: batch_buffer[:, self.n_features],
            self.r: batch_buffer[:, self.n_features + 1],
            self.s_: batch_buffer[:, -self.n_features:]
        })
        self.epsilon = min(self.epsilon_max, self.epsilon + self.epsilon_increment)
        self.learn_step_counter += 1
        return cost
 

RL = DeepQNetwork(n_actions=env.action_space.n,
                  n_features=env.observation_space.shape[0])
total_step = 0
for episode in range(max_episode):
    observation = env.reset()
    episode_reward = 0
    while True:
        env.render()  # 表达环境
        action = RL.choose_action_by_epsilon_greedy(observation)
        observation_, reward, done, info = env.step(action)
        # x是车的水平位移，theta是杆离垂直的角度
        x, x_dot, theta, theta_dot = observation_
        # reward1是车越偏离中心越少
        reward1 = (env.x_threshold - abs(x))/env.x_threshold - 0.8
        # reward2为杆越垂直越高
        reward2 = (env.theta_threshold_radians - abs(theta))/env.theta_threshold_radians - 0.5
        reward = reward1 + reward2
        RL.store_transition(observation, action, reward, observation_)
        if total_step > 100:
            cost = RL.learn()
            print('cost: %.3f' % cost)
        episode_reward += reward
        observation = observation_
        if done:
            print('episode:', episode,
                  'episode_reward %.2f' % episode_reward,
                  'epsilon %.2f' % RL.epsilon)
            break
        total_step += 1

# mountain car
RL = DeepQNetwork(n_actions=env.action_space.n,
                  n_features=env.observation_space.shape[0])
total_step = 0
for episode in range(max_episode):
    observation = env.reset()
    episode_reward = 0
    while True:
        env.render()  # 表达环境
        action = RL.choose_action_by_epsilon_greedy(observation)
        observation_, reward, done, info = env.step(action)
        #
        position, velocity = observation_
        reward=abs(position+0.5)
        RL.store_transition(observation, action, reward, observation_)
        if total_step > 100:
            cost_ = RL.learn()
            cost.append(cost_)
        episode_reward += reward
        observation = observation_
        if done:
            print('episode:', episode,
                  'episode_reward %.2f' % episode_reward,
                  'epsilon %.2f' % RL.epsilon)
            break
        total_step += 1
plt.plot(np.arange(len(cost)), cost)
plt.show()

```

- （2） MountainCar实例
    - car的轨迹是一维的，定位在两山之间，目标是爬上右边的山顶。可是car的发动机不足以一次性攀登到山顶，唯一的方式是car来回摆动增加动量。
    - ![](https://img-blog.csdn.net/20180308211123582)
    - 输出图：
        - ![](https://img-blog.csdn.net/2018032423074127)
    - 代码如下：

```python
RL = DeepQNetwork(n_actions=env.action_space.n,
                  n_features=env.observation_space.shape[0])
total_step = 0
for episode in range(max_episode):
    observation = env.reset()
    episode_reward = 0
    while True:
        env.render()  # 表达环境
        action = RL.choose_action_by_epsilon_greedy(observation)
        observation_, reward, done, info = env.step(action)
        #
        position, velocity = observation_
        reward=abs(position+0.5)
        RL.store_transition(observation, action, reward, observation_)
        if total_step > 100:
            cost_ = RL.learn()
            cost.append(cost_)
        episode_reward += reward
        observation = observation_
        if done:
            print('episode:', episode,
                  'episode_reward %.2f' % episode_reward,
                  'epsilon %.2f' % RL.epsilon)
            break
        total_step += 1
 
plt.plot(np.arange(len(cost)), cost)
plt.show()

```

## PG（Policy Gradients）策略梯度算法

- 基于Policy Gradients（策略梯度法，简称PG）的深度强化学习方法，思想上与基于Q-learning的系列算法有本质的不同
- 普通的PG算法，只能用于解决一些小的问题，比如经典的让杆子竖起来，让小车爬上山等。如果想应用到更复杂的问题上，比如玩星际争霸，就需要更复杂的一些方法，比如后期出现的Actor Critic，Asynchronous Advantage Actor-Critic (A3C)等等

### 解决什么问题

- （1）很多决策的行动空间是**高维甚至连续（无限）**的
  - 比如自动驾驶中，汽车下一个决策中方向盘的行动空间，就是一个从[-900°，900°]（假设方向盘是两圈半打满）的无限空间中选一个值，如果我们用Q系列算法来进行学习，则需要对每一个行动都计算一次reward，那么对无限行动空间而言，哪怕是把行动空间离散化，针对每个离散行动计算一次reward的计算成本也是当前算力所吃不消的。这是对Q系列算法提出的第一个挑战：无法遍历行动空间中所有行动的reward值。
- （2）决策往往是带有**多阶段**属性的，“不到最后时刻不知输赢”。
  - 以即时策略游戏（如：星际争霸，或者国内流行的王者荣耀）为例，玩家的输赢只有在最后游戏结束时才能知晓，谁也没法在游戏进行过程中笃定哪一方一定能够赢。甚至有可能发生：某个玩家的每一步行动看起来都很傻，但是最后却能够赢得比赛，比如，Dota游戏中，有的玩家虽然死了很多次，己方的塔被拆了也不管，但是却靠着偷塔取胜（虽然这种行为可能是不受欢迎的）。诸如此类的情形就对Q系列算法提出了第二个挑战，Agent每执行一个动作（action）之后的奖励（reward）难以确定，这就导致Q值无法更新。

- 由此衍生出了基于PG的系列深度强化学习算法

### 算法介绍

- 流程
  - ![](https://img-blog.csdnimg.cn/20200823095929950.png)
- 图解
  - ![](https://img-blog.csdnimg.cn/20200823150933700.png)
- 注意
  - 神经网络设计过程中，最后一层一般采用Softmax函数(离散动作) 或者 高斯函数激活（连续动作）。
- 奖励分配代码示例

```python
def _discount_and_norm_rewards(self):
##该函数将最后的奖励，依次分配给前面的回合，越往前，分配的越少。除此之外，还将分配后的奖励归一化为符合正太分布的形式。
    discounted_ep_rs = np.zeros_like(self.ep_rs) #self.ep_rs就是一局中每一回合的奖励，一般前面回合都是0，只有最后一个回合有奖励（一局结束）
    running_add = 0
    for t in reversed(range(0, len(self.ep_rs))):
        running_add = running_add * self.gamma + self.ep_rs[t] #self.gamma，是衰减系数，该系数越大，前面回合分配到的奖励越少（都衰减了嘛）
        discounted_ep_rs[t] = running_add

    discounted_ep_rs = discounted_ep_rs - np.mean(discounted_ep_rs)
    discounted_ep_rs = discounted_ep_rs / np.std(discounted_ep_rs)
    return discounted_ep_rs
```

### PG与Q系列算法

- PG深度强化学习算法与Q系列算法相比，优势主要有[2]：
  - 可以处理连续动作空间或者高维离散动作空间的问题。
  - 容易收敛，在学习过程中，策略梯度法每次更新策略函数时，参数只发生细微的变化，但参数的变化是朝着正确的方向进行迭代，使得算法有更好的收敛性。而价值函数在学习的后期，参数会围绕着最优值附近持续小幅度地波动，导致算法难以收敛。
- 缺点主要有：
  - 容易收敛到局部最优解，而非全局最优解；
  - 策略学习效率低；
  - 方差较高：这个可谓是普通PG深度强化学习算法最不可忍受的缺点了，由于PG算法参数更新幅度较小，导致神经网络有很大的随机性，在探索过程中会产生较多的无效尝试。另外，在处理回合结束才奖励的问题时，会出现不一致的问题：回合开始时，同样的状态下，采取同样的动作，但是由于后期采取动作不同，导致奖励值不同，从而导致神经网络参数来回变化，最终导致Loss函数的方差较大。



- 参考
  - [一图看懂Policy Gradients深度强化学习算法](https://blog.csdn.net/xz15873139854/article/details/108179193)


## [Deep Reinforcement Learning Algorithms with PyTorch](https://github.com/p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch)

This repository contains PyTorch implementations of deep reinforcement learning algorithms and environments. 

## **Algorithms Implemented** 

1. *Deep Q Learning (DQN)* <sub><sup> ([Mnih et al. 2013](https://arxiv.org/pdf/1312.5602.pdf)) </sup></sub>  
1. *DQN with Fixed Q Targets* <sub><sup> ([Mnih et al. 2013](https://arxiv.org/pdf/1312.5602.pdf)) </sup></sub>
1. *Double DQN (DDQN)* <sub><sup> ([Hado van Hasselt et al. 2015](https://arxiv.org/pdf/1509.06461.pdf)) </sup></sub>
1. *DDQN with Prioritised Experience Replay* <sub><sup> ([Schaul et al. 2016](https://arxiv.org/pdf/1511.05952.pdf)) </sup></sub>
1. *Dueling DDQN* <sub><sup> ([Wang et al. 2016](http://proceedings.mlr.press/v48/wangf16.pdf)) </sup></sub>
1. *REINFORCE* <sub><sup> ([Williams et al. 1992](http://www-anw.cs.umass.edu/~barto/courses/cs687/williams92simple.pdf)) </sup></sub>
1. *Deep Deterministic Policy Gradients (DDPG)* <sub><sup> ([Lillicrap et al. 2016](https://arxiv.org/pdf/1509.02971.pdf) ) </sup></sub>
1. *Twin Delayed Deep Deterministic Policy Gradients (TD3)* <sub><sup> ([Fujimoto et al. 2018](https://arxiv.org/abs/1802.09477)) </sup></sub>
1. *Soft Actor-Critic (SAC)* <sub><sup> ([Haarnoja et al. 2018](https://arxiv.org/pdf/1812.05905.pdf)) </sup></sub>
1. *Soft Actor-Critic for Discrete Actions (SAC-Discrete)* <sub><sup> ([Christodoulou 2019](https://arxiv.org/abs/1910.07207)) </sup></sub> 
1. *Asynchronous Advantage Actor Critic (A3C)* <sub><sup> ([Mnih et al. 2016](https://arxiv.org/pdf/1602.01783.pdf)) </sup></sub>
1. *Syncrhonous Advantage Actor Critic (A2C)*
1. *Proximal Policy Optimisation (PPO)* <sub><sup> ([Schulman et al. 2017](https://openai-public.s3-us-west-2.amazonaws.com/blog/2017-07/ppo/ppo-arxiv.pdf)) </sup></sub>
1. *DQN with Hindsight Experience Replay (DQN-HER)* <sub><sup> ([Andrychowicz et al. 2018](https://arxiv.org/pdf/1707.01495.pdf)) </sup></sub>
1. *DDPG with Hindsight Experience Replay (DDPG-HER)* <sub><sup> ([Andrychowicz et al. 2018](https://arxiv.org/pdf/1707.01495.pdf) ) </sup></sub>
1. *Hierarchical-DQN (h-DQN)* <sub><sup> ([Kulkarni et al. 2016](https://arxiv.org/pdf/1604.06057.pdf)) </sup></sub>
1. *Stochastic NNs for Hierarchical Reinforcement Learning (SNN-HRL)* <sub><sup> ([Florensa et al. 2017](https://arxiv.org/pdf/1704.03012.pdf)) </sup></sub>
1. *Diversity Is All You Need (DIAYN)* <sub><sup> ([Eyensbach et al. 2018](https://arxiv.org/pdf/1802.06070.pdf)) </sup></sub>

All implementations are able to quickly solve Cart Pole (discrete actions), Mountain Car Continuous (continuous actions), 
Bit Flipping (discrete actions with dynamic goals) or Fetch Reach (continuous actions with dynamic goals). I plan to add more hierarchical RL algorithms soon.

## **Environments Implemented**

1. *Bit Flipping Game* <sub><sup> (as described in [Andrychowicz et al. 2018](https://arxiv.org/pdf/1707.01495.pdf)) </sup></sub>
1. *Four Rooms Game* <sub><sup> (as described in [Sutton et al. 1998](http://www-anw.cs.umass.edu/~barto/courses/cs687/Sutton-Precup-Singh-AIJ99.pdf)) </sup></sub>
1. *Long Corridor Game* <sub><sup> (as described in [Kulkarni et al. 2016](https://arxiv.org/pdf/1604.06057.pdf)) </sup></sub>
1. *Ant-{Maze, Push, Fall}* <sub><sup> (as desribed in [Nachum et al. 2018](https://arxiv.org/pdf/1805.08296.pdf) and their accompanying [code](https://github.com/tensorflow/models/tree/master/research/efficient-hrl)) </sup></sub>

## **Results**

#### 1. Cart Pole and Mountain Car

Below shows various RL algorithms successfully learning discrete action game [Cart Pole](https://github.com/openai/gym/wiki/CartPole-v0)
 or continuous action game [Mountain Car](https://github.com/openai/gym/wiki/MountainCarContinuous-v0). The mean result from running the algorithms 
 with 3 random seeds is shown with the shaded area representing plus and minus 1 standard deviation. Hyperparameters
 used can be found in files `results/Cart_Pole.py` and `results/Mountain_Car.py`. 
 
![Cart Pole and Mountain Car Results](results/data_and_graphs/CartPole_and_MountainCar_Graph.png) 


#### 2. Hindsight Experience Replay (HER) Experiements

Below shows the performance of DQN and DDPG with and without Hindsight Experience Replay (HER) in the Bit Flipping (14 bits) 
and Fetch Reach environments described in the papers [Hindsight Experience Replay 2018](https://arxiv.org/pdf/1707.01495.pdf) 
and [Multi-Goal Reinforcement Learning 2018](https://arxiv.org/abs/1802.09464). The results replicate the results found in 
the papers and show how adding HER can allow an agent to solve problems that it otherwise would not be able to solve at all. Note that the same hyperparameters were used within each pair of agents and so the only difference 
between them was whether hindsight was used or not. 

![HER Experiment Results](results/data_and_graphs/HER_Experiments.png)

#### 3. Hierarchical Reinforcement Learning Experiments

The results on the left below show the performance of DQN and the algorithm hierarchical-DQN from [Kulkarni et al. 2016](https://arxiv.org/pdf/1604.06057.pdf)
on the Long Corridor environment also explained in [Kulkarni et al. 2016](https://arxiv.org/pdf/1604.06057.pdf). The environment
requires the agent to go to the end of a corridor before coming back in order to receive a larger reward. This delayed 
gratification and the aliasing of states makes it a somewhat impossible game for DQN to learn but if we introduce a 
meta-controller (as in h-DQN) which directs a lower-level controller how to behave we are able to make more progress. This 
aligns with the results found in the paper. 

The results on the right show the performance of DDQN and algorithm Stochastic NNs for Hierarchical Reinforcement Learning 
(SNN-HRL) from [Florensa et al. 2017](https://arxiv.org/pdf/1704.03012.pdf). DDQN is used as the comparison because
the implementation of SSN-HRL uses 2 DDQN algorithms within it. Note that the first 300 episodes of training
for SNN-HRL were used for pre-training which is why there is no reward for those episodes. 
 
![Long Corridor and Four Rooms](results/data_and_graphs/Four_Rooms_and_Long_Corridor.png)


# 问题

## 为什么说强化学习在近年不会被广泛应用

- 【2021-3-11】[为什么说强化学习在近年不会被广泛应用？](https://mp.weixin.qq.com/s/keXcOu5CMip-rwFQ_oQLyg)，[知乎帖子](https://www.zhihu.com/question/404471029)
  - （1）**数据收集过程不可控**
    - 不同于监督学习，强化学习的数据来自agent跟环境的各种交互。对于数据平衡性问题，监督学习可以通过各种补数据加标签来达到数据平衡。但这件事情对强化学习确实非常难以解决的，因为数据收集是由policy来做的，无论是DQN的Q-network还是AC架构里的actor，它们在学习过程中，对于任务激励信号的理解的不完善的，很可能绝大部分时间都在收集一些无用且重复的数据。虽然有prioritized replay buffer来解决训练优先级的问题，但实际上是把重复的经验数据都丢弃了。在实际应用中，面对一些稍微复杂点的任务还是需要收集一大堆重复且无用的数据。这也是DRL sample efficiency差的原因之一。
  - （2）**环境限制**
    - DRL问题中，环境都是从初始状态开始，这限制了很多可能的优化方向。比如在状态空间中，可以针对比较“新”的状态重点关注，控制环境都到这个状态。但目前的任务，很多环境的state-transition function都是stochastic的，都是概率函数。即便记录下来之前的action序列，由于环境状态转移的不确定性，也很难到达类似的状态。更别提policy本身也是stochastic的，这种双重stochastic叠加，不可能针对“重点”状态进行优化。
    - 同时这种限制也使得一些测试场景成为不可能。比如自动驾驶需要测试某个弯道，很难基于当前的policy在状态空间中达到类似的状态来重复测试policy在此状态下的鲁棒性。
  - （3）玄之又玄，**可解释性较差**
    - 本来Q-learning就是一个通过逐步学习来完善当前动作对未来收益影响作出估计的过程。加入DNN后，还涉及到了神经网络近似Q的训练。这就是“不靠谱”上又套了一层“不靠谱”。如何验证策略是正确的？如何验证Q function是最终收敛成为接近真实的估计？这些问题对于查表型的Q-learning来说，是可以解决的，无非就是工作量的问题。但对于大规模连续状态空间的DQN来说，基本上没法做。论证一个policy有效，也就是看看render以后的效果，看看reward曲线，看看tensorborad上的各个参数。连监督学习基本的正确率都没有。然后还要根据这些结果来调reward function，基本上都在避免回答why这个问题。
  - （4）**随机探索**
    - DRL的探索过程还是比较原始。现在大多数探索，epsilon-greedy，UCB都是从多臂老虎机来的，只针对固定state的action选择的探索。扩展到连续状态空间上，这种随机探索还是否有效，在实际落地过程中，还是要打个问号。因为也不能太随机了。大家都说PPO好，SAC强，探索过程也只不过是用了stochastic policy，做了个策略分布的熵的最大化。本质还是纯随机。虽然有些用好奇心做探索的工作，但也还是只把探索任务强加给了reward，指标不治本。
  - 当前DRL的实际科研的进步速度要远远慢于大众对于AI=DL+RL的期望。能解决的问题：
    - **固定场景**：状态空间不大，整个trajectory不长
    - **问题不复杂**：没有太多层次化的任务目标，奖励好设计
    - **试错成本低**：咋作都没事
    - **数据收集容易**：百万千万级别的数据量，如果不能把数据收集做到小时级别，那整个任务的时间成本就不太能跟传统的监督相比
    - **目标单纯**：容易被reward function量化，比如各种super-human的游戏。对于一些复杂的目标，比如几大公司都在强调拟人化，目前没有靠谱的解决方案
  - 落地领域也就游戏了，而且是简单游戏，比如固定场景、小地图上的格斗，比如街霸、王者之类。要是大地图、开放世界的话，光捡个枪、开个宝箱就能探索到猴年马月了。也没想象中那么fancy，基本没有图像类输入，全是传感器类的内部数据，所以同类型任务的训练难度还没到Atari级别，这几年时间，DOTA2和星际基本上是游戏领域内到顶的落地
    - DeepMind近期的一篇论文：[Rainbow: Combining Improvements in Deep Reinforcement Learning](https://arxiv.org/abs/1710.02298)。这篇论文对原始DQN框架做了一些渐进式改进，证明他们的RainbowDQN性能更优。
  - 做强化的同学们一点信息：
    - 强化学习岗位很少，因为落地难+烧钱，基本只有几个头部游戏公司会养一个规模不大的团队。
    - 纯强化的技术栈不太好跳槽，除了游戏外，别的领域很难有应用。
    - 20年huawei的强化夏令营，同时在线也有好几万人，想想这规模，未来几年这些研究生到job market会多卷。
- **强化学习是唯一一个可以明目张胆地在测试集上进行训练的机器学习网络**。都2020了强化学习除了能玩游戏还能做什么？强化学习的特点是面向目标的算法，过程基本很难拆解，没法管控，如果目标没法在商业公司被很好的认可
- 每当有人问我强化学习能否解决他们的问题时，我会说“不能”。而且我发现这个回答起码在70%的场合下是正确的。
- 更多参考：[深度强化学习的弱点和局限](https://zhuanlan.zhihu.com/p/34089913)，[原文:Deep Reinforcement Learning Doesn't Work Yet](https://www.alexirpan.com/2018/02/14/rl-hard.html)
- Deep RL实现进一步发展的一些条件：[原文地址](www.alexirpan.com/2018/02/14/rl-hard.html)，[中文版本: 深度强化学习的弱点和局限](https://mp.weixin.qq.com/s?__biz=MzI3ODkxODU3Mg==&mid=2247485609&idx=1&sn=6b71f5f8ebd4e920384f07b97ce92a9c&chksm=eb4eec6adc39657c81169f1ae9ce477e4da692941238c35deb26a11ed7ec70073784cfd935a8#rd)
  - 易于产生近乎无限的经验；
  - 把问题简化称更简单的形式；
  - 将自我学习引入强化学习；
  - 有一个清晰的方法来定义什么是可学习的、不可取消的奖励；
  - 如果奖励必须形成，那它至少应该是种类丰富的。
- 下面是我列出的一些关于未来研究趋势的合理猜测 ，希望未来Deep RL能带给我们更多惊喜。
  - 局部最优就足够了。我们一直以来都追求全局最优，但这个想法会不会有些自大呢？毕竟人类进化也只是朝着少数几个方向发展。也许未来我们发现局部最优就够了，不用盲目追求全局最优；
  - 代码不能解决的问题，硬件来。我确信有一部分人认为人工智能的成就源于硬件技术的突破，虽然我觉得硬件不能解决所有问题，但还是要承认，硬件在这之中扮演着重要角色。机器运行越快，我们就越不需要担心效率问题，探索也更简单；
  - 添加更多的learning signal。稀疏奖励很难学习，因为我们无法获得足够已知的有帮助的信息；
  - 基于模型的学习可以释放样本效率。原则上来说，一个好模型可以解决一系列问题，就像AlphaGo一样，也许加入基于模型的方法值得一试；
  - 像参数微调一样使用强化学习。第一篇AlphaGo论文从监督学习开始，然后在其上进行RL微调。这是一个很好的方法，因为它可以让我们使用一种速度更快但功能更少的方法来加速初始学习；
  - 奖励是可以学习的。如果奖励设计如此困难，也许我们能让系统自己学习设置奖励，现在模仿学习和反强化学习都有不错的发展，也许这个思路也行得通；
  - 用迁移学习帮助提高效率。迁移学习意味着我们能用以前积累的任务知识来学习新知识，这绝对是一个发展趋势；
  - 良好的先验知识可以大大缩短学习时间。这和上一点有相通之处。有一种观点认为，迁移学习就是利用过去的经验为学习其他任务打下一个良好的基础。RL算法被设计用于任何马尔科夫决策过程，这可以说是万恶之源。那么如果我们接受我们的解决方案只能在一小部分环境中表现良好，我们应该能够利用共享来解决所有问题。之前Pieter Abbeel在演讲中称Deep RL只需解决现实世界中的任务，我赞同这个观点，所以我们也可以通过共享建立一个现实世界的先验，让Deep RL可以快速学习真实的任务，作为代价，它可以不太擅长虚拟任务的学习；
  - 难与易的辨证转换。这是BAIR（Berkeley AI Research）提出的一个观点，他们从DeepMind的工作中发现，如果我们向环境中添加多个智能体，把任务变得很复杂，其实它们的学习过程反而被大大简化了。让我们回到ImageNet：在ImageNet上训练的模型将比在CIFAR-100上训练的模型更好地推广。所以可能我们并不需要一个高度泛化的强化学习系统，只要把它作为通用起点就好了。



# 资料

- 更多[Demo地址](http://wqw547243068.github.io/demo)
- 强化学习圣经：《强化学习导论》第二版（附[PDF](http://www.incompleteideas.net/book/the-book.html)下载），配套[python代码](https://github.com/ShangtongZhang/reinforcement-learning-an-introduction)
- [无需博士学位的TensorFlow深度强化学习教程](https://www.bilibili.com/video/av23286922/) by Martin Gorner

<iframe src="//player.bilibili.com/player.html?aid=23286922&bvid=BV1MW411F7yA&cid=38785810&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" height="600" width="100%"> </iframe>

- 李宏毅深度强化学习(国语)课程(2018)，[B站视频](https://www.bilibili.com/video/av24724071)

<iframe src="//player.bilibili.com/player.html?aid=24724071&bvid=BV1MW411w79n&cid=41583412&page=4" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" height="600" width="100%"> </iframe>


# 结束


