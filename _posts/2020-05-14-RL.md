---
layout: post
title:  "强化学习-Reinforcement Learning"
date:   2020-05-14 17:34:00
categories: 强化学习
tags: 深度学习 强化学习 增强学习
excerpt: AI=DL+RL，那么RL强化学习是什么，包含哪些内容，有哪些典型应用？
author: 鹤啸九天
mathjax: true
---

* content
{:toc}

# 总结

- 【2020-7-4】【(EEML2020)强化学习教程(Colab)】“[EEML2020 RL Tutorial](https://github.com/eemlcommunity/PracticalSessions2020/blob/master/rl/EEML2020_RL_Tutorial.ipynb)” 
- 【2020-7-5】XRL：可解释强化学习《[XRL: eXplainable Reinforcement Learning](https://towardsdatascience.com/xrl-explainable-reinforcement-learning-4cd065cdec9a)》by Meet Gandhi
- 【2021-2-27】【[杜伦大学10小时强化学习课程](https://www.bilibili.com/video/BV1vN411X7qB/)】“Reinforcement Learning Lectures  Durham University” by Chris G. Willcocks
  - <iframe src="//player.bilibili.com/player.html?aid=501767046&bvid=BV1vN411X7qB&cid=302763703&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"  width="100%" height="600"> </iframe>
- 【2021-3-17】[强化学习的“神话”和“鬼话”](https://zhuanlan.zhihu.com/p/196421049)，Csaba Szepesvári在2020年数据挖掘顶会KDD的Deep Learning Day做了题为Myths and Misconceptions in Reinforcement Learning的讲座。Csaba Szepesvári是阿尔伯塔大学计算机系教授，在Deepmind领导Foundations团队。他于2010年出版Algorithms for Reinforcement Learning. 最近出版Bandit Algorithms. 他于2006年发表题为Bandit based Monte-Carlo Planning的论文，提出UCT算法，对AlphaGo的研发起到了关键作用。
  - 内容：
    - 1）要不要学习RL
      - 误解：有人说，RL就是指一些特定的算法，比如TD、DQN、PPO，等等；而像ES(evolutionary search, 进化搜索)、随机搜索、SSL(self-supervised learning, 自我监督学习)等等这些就不是RL.
      - 三类基本的RL问题：Online RL（智能体直接与环境交互学习）, Batch RL（历史数据中学习）, Planning/simulation optimization（规划/仿真优化）. 当然有很多变种。
    - 2）RL是不是有很多问题
    - 3）RL与相邻学科的关系如何
    - 然后讨论一些“Meta consideration”
  - [原版ppt及youtube视频](https://sites.ualberta.ca/~szepesva/talks.html)，[model based RL ppt](https://www.dropbox.com/s/8b4eivix82tfp2z/ModelBasedRL2.pptx?dl=0)
- 【2021-3-11】[为什么说强化学习在近年不会被广泛应用？](https://mp.weixin.qq.com/s/keXcOu5CMip-rwFQ_oQLyg)，[知乎帖子](https://www.zhihu.com/question/404471029)
  - （1）**数据收集过程不可控**
    - 不同于监督学习，强化学习的数据来自agent跟环境的各种交互。对于数据平衡性问题，监督学习可以通过各种补数据加标签来达到数据平衡。但这件事情对强化学习确实非常难以解决的，因为数据收集是由policy来做的，无论是DQN的Q-network还是AC架构里的actor，它们在学习过程中，对于任务激励信号的理解的不完善的，很可能绝大部分时间都在收集一些无用且重复的数据。虽然有prioritized replay buffer来解决训练优先级的问题，但实际上是把重复的经验数据都丢弃了。在实际应用中，面对一些稍微复杂点的任务还是需要收集一大堆重复且无用的数据。这也是DRL sample efficiency差的原因之一。
  - （2）**环境限制**
    - DRL问题中，环境都是从初始状态开始，这限制了很多可能的优化方向。比如在状态空间中，可以针对比较“新”的状态重点关注，控制环境都到这个状态。但目前的任务，很多环境的state-transition function都是stochastic的，都是概率函数。即便记录下来之前的action序列，由于环境状态转移的不确定性，也很难到达类似的状态。更别提policy本身也是stochastic的，这种双重stochastic叠加，不可能针对“重点”状态进行优化。
    - 同时这种限制也使得一些测试场景成为不可能。比如自动驾驶需要测试某个弯道，很难基于当前的policy在状态空间中达到类似的状态来重复测试policy在此状态下的鲁棒性。
  - （3）玄之又玄，**可解释性较差**
    - 本来Q-learning就是一个通过逐步学习来完善当前动作对未来收益影响作出估计的过程。加入DNN后，还涉及到了神经网络近似Q的训练。这就是“不靠谱”上又套了一层“不靠谱”。如何验证策略是正确的？如何验证Q function是最终收敛成为接近真实的估计？这些问题对于查表型的Q-learning来说，是可以解决的，无非就是工作量的问题。但对于大规模连续状态空间的DQN来说，基本上没法做。论证一个policy有效，也就是看看render以后的效果，看看reward曲线，看看tensorborad上的各个参数。连监督学习基本的正确率都没有。然后还要根据这些结果来调reward function，基本上都在避免回答why这个问题。
  - （4）**随机探索**
    - DRL的探索过程还是比较原始。现在大多数探索，epsilon-greedy，UCB都是从多臂老虎机来的，只针对固定state的action选择的探索。扩展到连续状态空间上，这种随机探索还是否有效，在实际落地过程中，还是要打个问号。因为也不能太随机了。大家都说PPO好，SAC强，探索过程也只不过是用了stochastic policy，做了个策略分布的熵的最大化。本质还是纯随机。虽然有些用好奇心做探索的工作，但也还是只把探索任务强加给了reward，指标不治本。
  - 当前DRL的实际科研的进步速度要远远慢于大众对于AI=DL+RL的期望。能解决的问题：
    - **固定场景**：状态空间不大，整个trajectory不长
    - **问题不复杂**：没有太多层次化的任务目标，奖励好设计
    - **试错成本低**：咋作都没事
    - **数据收集容易**：百万千万级别的数据量，如果不能把数据收集做到小时级别，那整个任务的时间成本就不太能跟传统的监督相比
    - **目标单纯**：容易被reward function量化，比如各种super-human的游戏。对于一些复杂的目标，比如几大公司都在强调拟人化，目前没有靠谱的解决方案
  - 落地领域也就游戏了，而且是简单游戏，比如固定场景、小地图上的格斗，比如街霸、王者之类。要是大地图、开放世界的话，光捡个枪、开个宝箱就能探索到猴年马月了。也没想象中那么fancy，基本没有图像类输入，全是传感器类的内部数据，所以同类型任务的训练难度还没到Atari级别，这几年时间，DOTA2和星际基本上是游戏领域内到顶的落地
    - DeepMind近期的一篇论文：[Rainbow: Combining Improvements in Deep Reinforcement Learning](https://arxiv.org/abs/1710.02298)。这篇论文对原始DQN框架做了一些渐进式改进，证明他们的RainbowDQN性能更优。
  - 做强化的同学们一点信息：
    - 强化学习岗位很少，因为落地难+烧钱，基本只有几个头部游戏公司会养一个规模不大的团队。
    - 纯强化的技术栈不太好跳槽，除了游戏外，别的领域很难有应用。
    - 20年huawei的强化夏令营，同时在线也有好几万人，想想这规模，未来几年这些研究生到job market会多卷。
- **强化学习是唯一一个可以明目张胆地在测试集上进行训练的机器学习网络**。都2020了强化学习除了能玩游戏还能做什么？强化学习的特点是面向目标的算法，过程基本很难拆解，没法管控，如果目标没法在商业公司被很好的认可
- 每当有人问我强化学习能否解决他们的问题时，我会说“不能”。而且我发现这个回答起码在70%的场合下是正确的。
- 更多参考：[深度强化学习的弱点和局限](https://zhuanlan.zhihu.com/p/34089913)，[原文:Deep Reinforcement Learning Doesn't Work Yet](https://www.alexirpan.com/2018/02/14/rl-hard.html)
- Deep RL实现进一步发展的一些条件：[原文地址](www.alexirpan.com/2018/02/14/rl-hard.html)，[中文版本: 深度强化学习的弱点和局限](https://mp.weixin.qq.com/s?__biz=MzI3ODkxODU3Mg==&mid=2247485609&idx=1&sn=6b71f5f8ebd4e920384f07b97ce92a9c&chksm=eb4eec6adc39657c81169f1ae9ce477e4da692941238c35deb26a11ed7ec70073784cfd935a8#rd)
  - 易于产生近乎无限的经验；
  - 把问题简化称更简单的形式；
  - 将自我学习引入强化学习；
  - 有一个清晰的方法来定义什么是可学习的、不可取消的奖励；
  - 如果奖励必须形成，那它至少应该是种类丰富的。
- 下面是我列出的一些关于未来研究趋势的合理猜测 ，希望未来Deep RL能带给我们更多惊喜。
  - 局部最优就足够了。我们一直以来都追求全局最优，但这个想法会不会有些自大呢？毕竟人类进化也只是朝着少数几个方向发展。也许未来我们发现局部最优就够了，不用盲目追求全局最优；
  - 代码不能解决的问题，硬件来。我确信有一部分人认为人工智能的成就源于硬件技术的突破，虽然我觉得硬件不能解决所有问题，但还是要承认，硬件在这之中扮演着重要角色。机器运行越快，我们就越不需要担心效率问题，探索也更简单；
  - 添加更多的learning signal。稀疏奖励很难学习，因为我们无法获得足够已知的有帮助的信息；
  - 基于模型的学习可以释放样本效率。原则上来说，一个好模型可以解决一系列问题，就像AlphaGo一样，也许加入基于模型的方法值得一试；
  - 像参数微调一样使用强化学习。第一篇AlphaGo论文从监督学习开始，然后在其上进行RL微调。这是一个很好的方法，因为它可以让我们使用一种速度更快但功能更少的方法来加速初始学习；
  - 奖励是可以学习的。如果奖励设计如此困难，也许我们能让系统自己学习设置奖励，现在模仿学习和反强化学习都有不错的发展，也许这个思路也行得通；
  - 用迁移学习帮助提高效率。迁移学习意味着我们能用以前积累的任务知识来学习新知识，这绝对是一个发展趋势；
  - 良好的先验知识可以大大缩短学习时间。这和上一点有相通之处。有一种观点认为，迁移学习就是利用过去的经验为学习其他任务打下一个良好的基础。RL算法被设计用于任何马尔科夫决策过程，这可以说是万恶之源。那么如果我们接受我们的解决方案只能在一小部分环境中表现良好，我们应该能够利用共享来解决所有问题。之前Pieter Abbeel在演讲中称Deep RL只需解决现实世界中的任务，我赞同这个观点，所以我们也可以通过共享建立一个现实世界的先验，让Deep RL可以快速学习真实的任务，作为代价，它可以不太擅长虚拟任务的学习；
  - 难与易的辨证转换。这是BAIR（Berkeley AI Research）提出的一个观点，他们从DeepMind的工作中发现，如果我们向环境中添加多个智能体，把任务变得很复杂，其实它们的学习过程反而被大大简化了。让我们回到ImageNet：在ImageNet上训练的模型将比在CIFAR-100上训练的模型更好地推广。所以可能我们并不需要一个高度泛化的强化学习系统，只要把它作为通用起点就好了。

# 什么是强化学习

- 强化学习强化学习系统由**智能体**（Agent）、**状态**（state）、**奖赏**（reward）、**动作**（action）和**环境**（Environment）五部分组成，如下图所示。
  - ![](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz/j3gficicyOvavlKDwRa4UIy5IjcjDAm1CPvPMxyK50fy1B5VEd2pt4NeCRaeLlkXMibyiam9cRsmPo3VLpZfpkuNLw/640?wx_fmt=other)
  - `Agent`：智能体是整个强化学习系统核心。它能够感知环境的状态（State），并且根据环境提供的奖励信号（Reward），通过学习选择一个合适的动作（Action），来最大化长期的Reward值。简而言之，Agent就是根据环境提供的Reward作为反馈，学习一系列的环境状态（State）到**动作**（Action）的映射，动作选择的原则是最大化未来累积的Reward的概率。选择的动作不仅影响当前时刻的Reward，还会影响下一时刻甚至未来的Reward，因此，Agent在学习过程中的基本规则是：如果某个动作（Action）带来了环境的正回报（Reward），那么这一动作会被加强，反之则会逐渐削弱，类似于物理学中条件反射原理。
  - `Environment`：环境会接收Agent执行的一系列的动作（Action），并且对这一系列的动作的好坏进行评价，并转换成一种可量化的（标量信号）Reward反馈给Agent，而不会告诉Agent应该如何去学习动作。Agent只能靠自己的历史（History）经历去学习。同时，环境还像Agent提供它所处的状态（State）信息。
  - `Reward`：环境提供给Agent的一个可量化的标量反馈信号，用于评价Agent在某一个时间步所做action的好坏。强化学习就是基于一种最大化累计奖赏假设：强化学习中，Agent进行一系列的动作选择的目标是最大化未来的累计奖赏。
  - `State`：状态指Agent所处的环境信息，包含了智能体用于进行Action选择的所有信息，它是历史（History）的一个函数：St = f（Ht）。
- 强化学习的主体是Agent和环境Environment。Agent为了适应环境，做出的一系列的动作，使最终的奖励最高，同时在此过程中更新特定的参数。实际上可以把强化学习简单理解成是一种循环，具体的工作方式如下：
  - 智能体从环境中获取一个状态St；
  - 智能体根据状态St采取一个动作at；
  - 受到at的影响，环境发生变化，转换到新的状态St+1；
  - 环境反馈给智能体一个奖励（正向为奖励，负向则为惩罚）。
- 参考：[强化学习在智能对话上的应用](https://blog.csdn.net/Tencent_TEG/article/details/88859179)


# 算法

- 待定

## DQN

- Q-learning是强化学习中的一种，在Q-learning中，我们维护一张Q值表，表的维数为：状态数S * 动作数A，表中每个数代表在态s下可以采用动作a可以获得的未来收益的折现和——Q值。我们不断的迭代我们的Q值表使其最终收敛，然后根据Q值表我们就可以在每个状态下选取一个最优策略。由于这里主要介绍强化学习的在任务型对话中的应用，不进行详细的介绍，具体的可参照《[极简Q-learning教程](https://zhuanlan.zhihu.com/p/29213893)》
  - 更新公式如下，R+γmax部分称为Q-target，即使用贝尔曼方程加贪心策略认为实际应该得到的奖励，目标是使Q值不断的接近Q-target值
  - ![](https://upload-images.jianshu.io/upload_images/4155986-23e17f9c5b81efce.png?imageMogr2/auto-orient/strip\|imageView2/2/w/992/format/webp)
- DQN是深度学习与强化学习的结合，即使用神经网络代替Q-learning中Q表。在普通的Q-learning中，当状态和动作空间是离散且维数不高时可使用Q-Table储存每个状态动作对的Q值，但是当状态和动作空间是**高维**或者**连续**时，使用Q-Table不现实，而神经网络恰好擅长于此。因此DQN将Q-Table的更新问题变成一个函数拟合问题，相近的状态得到相近的输出动作。如有一个Q值表，神经网络的作用就是给定一个状态s和动作a，预测对应的Q值，使得神经网络的结果与Q表中的值接近。不过DQN的方式肯定不能继续维护一个Q表，所以将上次反馈的奖励作为逼近的目标，如下式，通过更新参数 θ 使Q函数逼近最优Q值。因此，DQN就是要设计一个神经网络结构，通过函数来拟合Q值
- 问题：
  - 神经网络需要**大量带标签的样本**进行监督学习，但是强化学习只有reward返回值，如何构造有监督的数据成为第一个问题，而且伴随着噪声、延迟（过了几十毫秒才返回）、稀疏（很多State的reward是0）等问题；
  - 神经网络的前提是样本**独立同分布**，而强化学习前后state状态和反馈有**依赖**关系——**马尔科夫决策**；
  - 神经网络的目标**分布固定**，但是强化学习的分布一直变化，比如你玩一个游戏，一个关卡和下一个关卡的状态分布是不同的，所以训练好了前一个关卡，下一个关卡又要重新训练；
  - 过往的研究表明，使用非线性网络表示值函数时出现不稳定等问题。
- 针对以上问题的具体解决方案如下：
  - **构造标签**：通过Q-Learning使用reward来构造标签（对应问题1），如上所述，用神经网络来预测reward，将问题转化为一个**回归**问题；
  - **经验回放**：通过experience replay（经验池）的方法来解决**相关性**及**非静态分布**问题（对应问题2、3）；具体做法是把每个时间步agent与环境交互得到的转移样本 (st,at,rt,st+1) 储存到回放记忆单元，要训练时就随机拿出一些（minibatch）来训练。（其实就是将游戏的过程打成碎片存储，训练时随机抽取就避免了相关性问题）
  - 双网络结构：使用一个神经网络产生当前Q值，使用另外一个神经网络产生Target Q值（对应问题4）。在Nature 2015版本的DQN中提出了这个改进，使用另一个网络（这里称为target_net）产生Target Q值。具体地，Q(s,a;θi) 表示当前网络eval_net的输出，用来评估当前状态动作对的值函数；Q(s,a;θ−i) 表示target_net的输出，代入上面求 TargetQ 值的公式中得到目标Q值。根据上面的Loss Function更新eval_net的参数，每经过N轮迭代，将MainNet的参数复制给target_net。引入target_net后，再一段时间里目标Q值使保持不变的，一定程度降低了当前Q值和目标Q值的相关性，提高了算法稳定性。
- 参考：[实战深度强化学习DQN-理论和实践](https://www.jianshu.com/p/10930c371cac?from=singlemessage)

# 代码实现

## DQN

- 两种DQN的实现方式
  - 一种是将s和a输入到网络，得到q值
  - 另一种是只将s输入到网络，输出为s和每个a结合的q值。
- 莫烦的Demo采用第二种，[Github地址](https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow)
  - ![](https://upload-images.jianshu.io/upload_images/4155986-fd3c59c61907c4ad.png?imageMogr2/auto-orient/strip\|imageView2/2/w/542/format/webp)
- DQN的Tensorflow版本实现，Gym环境
    - [用gym库Classic control实现deep Q learning](https://blog.csdn.net/winycg/article/details/79468320)

- （1）CartPole实例
    - 运载体在一根杆子下无摩擦的跟踪。系统通过施加+1和-1推动运载体。杆子的摇摆在初始时垂直的，目标是阻止它掉落运载体。每一步杆子保持垂直可以获得+1的奖励。episode将会终结于杆子的摇摆幅度超过了离垂直方向的15°或者是运载体偏移初始中心超过2.4个单位。
    - ![](https://img-blog.csdn.net/20180307114338879)
    - 效果图：
        - ![](https://img-blog.csdn.net/20180308173917866)

```python
#https://blog.csdn.net/winycg/article/details/79468320
import numpy as np
import random
import tensorflow as tf
import gym
 
max_episode = 100
env = gym.make('CartPole-v0')
env = env.unwrapped
 
class DeepQNetwork(object):
    def __init__(self,
                 n_actions,
                 n_features,
                 learning_rate=0.01,
                 reward_decay=0.9,  # gamma
                 epsilon_greedy=0.9,  # epsilon
                 epsilon_increment = 0.001,
                 replace_target_iter=300,  # 更新target网络的间隔步数
                 buffer_size=500,  # 样本缓冲区
                 batch_size=32,
                 ):
        self.n_actions = n_actions
        self.n_features = n_features
        self.lr = learning_rate
        self.gamma = reward_decay
        self.epsilon_max = epsilon_greedy
        self.replace_target_iter = replace_target_iter
        self.buffer_size = buffer_size
        self.buffer_counter = 0  # 统计目前进入过buffer的数量
        self.batch_size = batch_size
        self.epsilon = 0 if epsilon_increment is not None else epsilon_greedy
        self.epsilon_max = epsilon_greedy
        self.epsilon_increment = epsilon_increment
        self.learn_step_counter = 0  # 学习计步器
        self.buffer = np.zeros((self.buffer_size, n_features * 2 + 2))  # 初始化Experience buffer[s,a,r,s_]
        self.build_net()
        # 将eval网络中参数全部更新到target网络
        target_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='target_net')
        eval_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='eval_net')
        with tf.variable_scope('soft_replacement'):
            self.target_replace_op = [tf.assign(t, e) for t, e in zip(target_params, eval_params)]
        self.sess = tf.Session()
        tf.summary.FileWriter('logs/', self.sess.graph)
        self.sess.run(tf.global_variables_initializer())
 
    def build_net(self):
        self.s = tf.placeholder(tf.float32, [None, self.n_features])
        self.s_ = tf.placeholder(tf.float32, [None, self.n_features])
        self.r = tf.placeholder(tf.float32, [None, ])
        self.a = tf.placeholder(tf.int32, [None, ])
 
        w_initializer = tf.random_normal_initializer(0., 0.3)
        b_initializer = tf.constant_initializer(0.1)
        # q_eval网络架构，输入状态属性，输出4种动作
        with tf.variable_scope('eval_net'):
            eval_layer = tf.layers.dense(self.s, 20, tf.nn.relu, kernel_initializer=w_initializer,
                                         bias_initializer=b_initializer, name='eval_layer')
            self.q_eval = tf.layers.dense(eval_layer, self.n_actions, kernel_initializer=w_initializer,
                                          bias_initializer=b_initializer, name='output_layer1')
        with tf.variable_scope('target_net'):
            target_layer = tf.layers.dense(self.s_, 20, tf.nn.relu, kernel_initializer=w_initializer,
                                           bias_initializer=b_initializer, name='target_layer')
            self.q_next = tf.layers.dense(target_layer, self.n_actions, kernel_initializer=w_initializer,
                                          bias_initializer=b_initializer, name='output_layer2')
        with tf.variable_scope('q_target'):
            # 计算期望价值，并使用stop_gradient函数将其不计算梯度，也就是当做常数对待
            self.q_target = tf.stop_gradient(self.r + self.gamma * tf.reduce_max(self.q_next, axis=1))
        with tf.variable_scope('q_eval'):
            # 将a的值对应起来，
            a_indices = tf.stack([tf.range(tf.shape(self.a)[0]), self.a], axis=1)
            self.q_eval_a = tf.gather_nd(params=self.q_eval, indices=a_indices)
        with tf.variable_scope('loss'):
            self.loss = tf.reduce_mean(tf.squared_difference(self.q_target, self.q_eval_a))
        with tf.variable_scope('train'):
            self.train_op = tf.train.RMSPropOptimizer(self.lr).minimize(self.loss)
 
            # 存储训练数据
 
    def store_transition(self, s, a, r, s_):
        transition = np.hstack((s, a, r, s_))
        index = self.buffer_counter % self.buffer_size
        self.buffer[index, :] = transition
        self.buffer_counter += 1
 
    def choose_action_by_epsilon_greedy(self, status):
        status = status[np.newaxis, :]
        if random.random() < self.epsilon:
            actions_value = self.sess.run(self.q_eval, feed_dict={self.s: status})
            action = np.argmax(actions_value)
        else:
            action = np.random.randint(0, self.n_actions)
        return action
 
    def learn(self):
        # 每学习self.replace_target_iter步，更新target网络的参数
        if self.learn_step_counter % self.replace_target_iter == 0:
            self.sess.run(self.target_replace_op)
            # 从Experience buffer中选择样本
        sample_index = np.random.choice(min(self.buffer_counter, self.buffer_size), size=self.batch_size)
        batch_buffer = self.buffer[sample_index, :]
        _, cost = self.sess.run([self.train_op, self.loss], feed_dict={
            self.s: batch_buffer[:, :self.n_features],
            self.a: batch_buffer[:, self.n_features],
            self.r: batch_buffer[:, self.n_features + 1],
            self.s_: batch_buffer[:, -self.n_features:]
        })
        self.epsilon = min(self.epsilon_max, self.epsilon + self.epsilon_increment)
        self.learn_step_counter += 1
        return cost
 

RL = DeepQNetwork(n_actions=env.action_space.n,
                  n_features=env.observation_space.shape[0])
total_step = 0
for episode in range(max_episode):
    observation = env.reset()
    episode_reward = 0
    while True:
        env.render()  # 表达环境
        action = RL.choose_action_by_epsilon_greedy(observation)
        observation_, reward, done, info = env.step(action)
        # x是车的水平位移，theta是杆离垂直的角度
        x, x_dot, theta, theta_dot = observation_
        # reward1是车越偏离中心越少
        reward1 = (env.x_threshold - abs(x))/env.x_threshold - 0.8
        # reward2为杆越垂直越高
        reward2 = (env.theta_threshold_radians - abs(theta))/env.theta_threshold_radians - 0.5
        reward = reward1 + reward2
        RL.store_transition(observation, action, reward, observation_)
        if total_step > 100:
            cost = RL.learn()
            print('cost: %.3f' % cost)
        episode_reward += reward
        observation = observation_
        if done:
            print('episode:', episode,
                  'episode_reward %.2f' % episode_reward,
                  'epsilon %.2f' % RL.epsilon)
            break
        total_step += 1

# mountain car
RL = DeepQNetwork(n_actions=env.action_space.n,
                  n_features=env.observation_space.shape[0])
total_step = 0
for episode in range(max_episode):
    observation = env.reset()
    episode_reward = 0
    while True:
        env.render()  # 表达环境
        action = RL.choose_action_by_epsilon_greedy(observation)
        observation_, reward, done, info = env.step(action)
        #
        position, velocity = observation_
        reward=abs(position+0.5)
        RL.store_transition(observation, action, reward, observation_)
        if total_step > 100:
            cost_ = RL.learn()
            cost.append(cost_)
        episode_reward += reward
        observation = observation_
        if done:
            print('episode:', episode,
                  'episode_reward %.2f' % episode_reward,
                  'epsilon %.2f' % RL.epsilon)
            break
        total_step += 1
plt.plot(np.arange(len(cost)), cost)
plt.show()

```

- （2） MountainCar实例
    - car的轨迹是一维的，定位在两山之间，目标是爬上右边的山顶。可是car的发动机不足以一次性攀登到山顶，唯一的方式是car来回摆动增加动量。
    - ![](https://img-blog.csdn.net/20180308211123582)
    - 输出图：
        - ![](https://img-blog.csdn.net/2018032423074127)
    - 代码如下：

```python
RL = DeepQNetwork(n_actions=env.action_space.n,
                  n_features=env.observation_space.shape[0])
total_step = 0
for episode in range(max_episode):
    observation = env.reset()
    episode_reward = 0
    while True:
        env.render()  # 表达环境
        action = RL.choose_action_by_epsilon_greedy(observation)
        observation_, reward, done, info = env.step(action)
        #
        position, velocity = observation_
        reward=abs(position+0.5)
        RL.store_transition(observation, action, reward, observation_)
        if total_step > 100:
            cost_ = RL.learn()
            cost.append(cost_)
        episode_reward += reward
        observation = observation_
        if done:
            print('episode:', episode,
                  'episode_reward %.2f' % episode_reward,
                  'epsilon %.2f' % RL.epsilon)
            break
        total_step += 1
 
plt.plot(np.arange(len(cost)), cost)
plt.show()

```


## [Deep Reinforcement Learning Algorithms with PyTorch](https://github.com/p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch)

This repository contains PyTorch implementations of deep reinforcement learning algorithms and environments. 

## **Algorithms Implemented** 

1. *Deep Q Learning (DQN)* <sub><sup> ([Mnih et al. 2013](https://arxiv.org/pdf/1312.5602.pdf)) </sup></sub>  
1. *DQN with Fixed Q Targets* <sub><sup> ([Mnih et al. 2013](https://arxiv.org/pdf/1312.5602.pdf)) </sup></sub>
1. *Double DQN (DDQN)* <sub><sup> ([Hado van Hasselt et al. 2015](https://arxiv.org/pdf/1509.06461.pdf)) </sup></sub>
1. *DDQN with Prioritised Experience Replay* <sub><sup> ([Schaul et al. 2016](https://arxiv.org/pdf/1511.05952.pdf)) </sup></sub>
1. *Dueling DDQN* <sub><sup> ([Wang et al. 2016](http://proceedings.mlr.press/v48/wangf16.pdf)) </sup></sub>
1. *REINFORCE* <sub><sup> ([Williams et al. 1992](http://www-anw.cs.umass.edu/~barto/courses/cs687/williams92simple.pdf)) </sup></sub>
1. *Deep Deterministic Policy Gradients (DDPG)* <sub><sup> ([Lillicrap et al. 2016](https://arxiv.org/pdf/1509.02971.pdf) ) </sup></sub>
1. *Twin Delayed Deep Deterministic Policy Gradients (TD3)* <sub><sup> ([Fujimoto et al. 2018](https://arxiv.org/abs/1802.09477)) </sup></sub>
1. *Soft Actor-Critic (SAC)* <sub><sup> ([Haarnoja et al. 2018](https://arxiv.org/pdf/1812.05905.pdf)) </sup></sub>
1. *Soft Actor-Critic for Discrete Actions (SAC-Discrete)* <sub><sup> ([Christodoulou 2019](https://arxiv.org/abs/1910.07207)) </sup></sub> 
1. *Asynchronous Advantage Actor Critic (A3C)* <sub><sup> ([Mnih et al. 2016](https://arxiv.org/pdf/1602.01783.pdf)) </sup></sub>
1. *Syncrhonous Advantage Actor Critic (A2C)*
1. *Proximal Policy Optimisation (PPO)* <sub><sup> ([Schulman et al. 2017](https://openai-public.s3-us-west-2.amazonaws.com/blog/2017-07/ppo/ppo-arxiv.pdf)) </sup></sub>
1. *DQN with Hindsight Experience Replay (DQN-HER)* <sub><sup> ([Andrychowicz et al. 2018](https://arxiv.org/pdf/1707.01495.pdf)) </sup></sub>
1. *DDPG with Hindsight Experience Replay (DDPG-HER)* <sub><sup> ([Andrychowicz et al. 2018](https://arxiv.org/pdf/1707.01495.pdf) ) </sup></sub>
1. *Hierarchical-DQN (h-DQN)* <sub><sup> ([Kulkarni et al. 2016](https://arxiv.org/pdf/1604.06057.pdf)) </sup></sub>
1. *Stochastic NNs for Hierarchical Reinforcement Learning (SNN-HRL)* <sub><sup> ([Florensa et al. 2017](https://arxiv.org/pdf/1704.03012.pdf)) </sup></sub>
1. *Diversity Is All You Need (DIAYN)* <sub><sup> ([Eyensbach et al. 2018](https://arxiv.org/pdf/1802.06070.pdf)) </sup></sub>

All implementations are able to quickly solve Cart Pole (discrete actions), Mountain Car Continuous (continuous actions), 
Bit Flipping (discrete actions with dynamic goals) or Fetch Reach (continuous actions with dynamic goals). I plan to add more hierarchical RL algorithms soon.

## **Environments Implemented**

1. *Bit Flipping Game* <sub><sup> (as described in [Andrychowicz et al. 2018](https://arxiv.org/pdf/1707.01495.pdf)) </sup></sub>
1. *Four Rooms Game* <sub><sup> (as described in [Sutton et al. 1998](http://www-anw.cs.umass.edu/~barto/courses/cs687/Sutton-Precup-Singh-AIJ99.pdf)) </sup></sub>
1. *Long Corridor Game* <sub><sup> (as described in [Kulkarni et al. 2016](https://arxiv.org/pdf/1604.06057.pdf)) </sup></sub>
1. *Ant-{Maze, Push, Fall}* <sub><sup> (as desribed in [Nachum et al. 2018](https://arxiv.org/pdf/1805.08296.pdf) and their accompanying [code](https://github.com/tensorflow/models/tree/master/research/efficient-hrl)) </sup></sub>

## **Results**

#### 1. Cart Pole and Mountain Car

Below shows various RL algorithms successfully learning discrete action game [Cart Pole](https://github.com/openai/gym/wiki/CartPole-v0)
 or continuous action game [Mountain Car](https://github.com/openai/gym/wiki/MountainCarContinuous-v0). The mean result from running the algorithms 
 with 3 random seeds is shown with the shaded area representing plus and minus 1 standard deviation. Hyperparameters
 used can be found in files `results/Cart_Pole.py` and `results/Mountain_Car.py`. 
 
![Cart Pole and Mountain Car Results](results/data_and_graphs/CartPole_and_MountainCar_Graph.png) 


#### 2. Hindsight Experience Replay (HER) Experiements

Below shows the performance of DQN and DDPG with and without Hindsight Experience Replay (HER) in the Bit Flipping (14 bits) 
and Fetch Reach environments described in the papers [Hindsight Experience Replay 2018](https://arxiv.org/pdf/1707.01495.pdf) 
and [Multi-Goal Reinforcement Learning 2018](https://arxiv.org/abs/1802.09464). The results replicate the results found in 
the papers and show how adding HER can allow an agent to solve problems that it otherwise would not be able to solve at all. Note that the same hyperparameters were used within each pair of agents and so the only difference 
between them was whether hindsight was used or not. 

![HER Experiment Results](results/data_and_graphs/HER_Experiments.png)

#### 3. Hierarchical Reinforcement Learning Experiments

The results on the left below show the performance of DQN and the algorithm hierarchical-DQN from [Kulkarni et al. 2016](https://arxiv.org/pdf/1604.06057.pdf)
on the Long Corridor environment also explained in [Kulkarni et al. 2016](https://arxiv.org/pdf/1604.06057.pdf). The environment
requires the agent to go to the end of a corridor before coming back in order to receive a larger reward. This delayed 
gratification and the aliasing of states makes it a somewhat impossible game for DQN to learn but if we introduce a 
meta-controller (as in h-DQN) which directs a lower-level controller how to behave we are able to make more progress. This 
aligns with the results found in the paper. 

The results on the right show the performance of DDQN and algorithm Stochastic NNs for Hierarchical Reinforcement Learning 
(SNN-HRL) from [Florensa et al. 2017](https://arxiv.org/pdf/1704.03012.pdf). DDQN is used as the comparison because
the implementation of SSN-HRL uses 2 DDQN algorithms within it. Note that the first 300 episodes of training
for SNN-HRL were used for pre-training which is why there is no reward for those episodes. 
 
![Long Corridor and Four Rooms](results/data_and_graphs/Four_Rooms_and_Long_Corridor.png)


# 资料

- 更多[Demo地址](http://wqw547243068.github.io/demo)
- 强化学习圣经：《强化学习导论》第二版（附[PDF](http://www.incompleteideas.net/book/the-book.html)下载），配套[python代码](https://github.com/ShangtongZhang/reinforcement-learning-an-introduction)
- [无需博士学位的TensorFlow深度强化学习教程](https://www.bilibili.com/video/av23286922/) by Martin Gorner

<iframe src="//player.bilibili.com/player.html?aid=23286922&bvid=BV1MW411F7yA&cid=38785810&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" height="600" width="100%"> </iframe>

- 李宏毅深度强化学习(国语)课程(2018)，[B站视频](https://www.bilibili.com/video/av24724071)

<iframe src="//player.bilibili.com/player.html?aid=24724071&bvid=BV1MW411w79n&cid=41583412&page=4" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" height="600" width="100%"> </iframe>


# 结束


