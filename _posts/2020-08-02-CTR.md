---
layout: post
title:  "CTR预估笔记-CTR Prediction"
date:   2020-08-02 00:11:00
categories: 机器学习
tags: CTR 机器学习
excerpt: CTR预估相关知识点汇总
author: 鹤啸九天
mathjax: true
---

* content
{:toc}

# 排序

互联网的出现，意味着"信息大爆炸"。用户担心的，不再是信息太少，而是信息太多。如何从大量信息之中，快速有效地找出最重要的内容，成了互联网的一大核心问题。

各种各样的排名算法，是目前过滤信息的主要手段之一。对信息进行排名，意味着将信息按照重要性依次排列，并且及时进行更新。排列的依据，可以基于信息本身的特征，也可以基于用户的投票，即让用户决定，什么样的信息可以排在第一位。

下面整理和分析一些基于用户投票的排名算法。

- 参考资料
    - 【2020-8-5】[十张图带你领略知乎答案排序算法之美](https://www.jianshu.com/p/97d08220ff13)
    - [基于用户投票的 6 个排名算法【转载+整理】](https://www.cnblogs.com/liuning8023/p/3209741.html)

## 基于用户投票的排名算法应用


### Delicious

- 最直接、最简单的算法，它按单位时间内用户的投票数进行排名。得票最多的 item 排在第一位。旧版 [Delicious](https://delicious.com/)，有一个“热门书签排行榜”，就是这样统计出来的
    - ![](https://images0.cnblogs.com/blog/321721/201307/24001546-e08b5972448d44aa883ddf0aaaf7ca32.gif)
- 每过 60 分钟，就统计一次。按照“60分钟内被收藏的次数”进行排名。

- 该算法
    - 优点：简单、易部署、内容更新相当快
    - 但缺点主要有两个：
        - 1）排名变化不够平滑，前一个小时还排名靠前的item，第二个小时就一落千丈
        - 2）缺乏自动淘汰旧  item 的机制，即某些热门内容可能会长期占据排行榜前列。


### [Hacker News](http://news.ycombinator.com/)

- Hacker News 是一个网络社区，可以张贴链接，或者讨论某个主题。
    - ![](https://images0.cnblogs.com/blog/321721/201307/24001546-51e3c797985e49de9a76983f4ed6a63b.gif)
    - 点击小三角，点赞
- 并非得票最多的文章就应该排在第一位，还要考虑时间因素，新文章应该比旧文章更容易得到好的排名。
- 算法数学公式
    - ![](https://images0.cnblogs.com/blog/321721/201307/24001547-29f492969a6643ea9919fc34e7d9bb18.gif)
    - P  表示帖子得票数，“减去1”表示忽略发帖人的投票。
        - 在其他条件不变的情况下，得票越多，排名越高
        - 如果不想让“高票帖子”与“低票帖子”的差距过大，可以在得票数上加一个小于 1 的指数![](https://images0.cnblogs.com/blog/321721/201307/24001549-f589378fa6cf4cbfab9c6e4d597e8e39.gif)
    - T  表示距离发帖的时间（单位为小时），“加上 2”表示防止对于最新的帖子，导致分母过小（之所以选择 2，可能是因为从原始文章出现在其他网站，到转贴至 Hacker News，平均需要两个小时）。
        - 在其他条件不变的情况下，越是新发表的帖子，排名越高。或者说，一个帖子的排名，会随着时间而不断下降。因此，从图 4 看到，经过 24 小时后，所有帖子的得分基本上都小于 1，这意味着它们都将跌到排行榜的末尾，保证了排名前列的都将是较新的内容。
    - G  表示“重力因子（gravityth power）”，即将帖子排名往下拉的力量，默认值为 1.8，后文会详细讨论这个值。
        - 它的数值大小决定了排名随时间下降的速度。从图 4 右边看到，三个曲线的其他参数都一样，G 的值分别为 1.5、1.8 和 2.0。G 值越大，曲线越陡峭，排名下降得越快，意味着排行榜的更新速度越快。



### [Reddit](http://www.reddit.com/)

- 上面 [Hacker News](http://www.ruanyifeng.com/blog/2012/02/ranking_algorithm_hacker_news.html) 排名算法只能投赞成票，可用户当然也需要投反对票。或者说，除了好评以外，还可以给差评。
- Reddit 是美国最大的网上社区，每个帖子前面都有“向上”和“向下”箭头，分别表示“赞成”和“反对”。用户点击进行投票，Reddit根据投票结果，计算出最新的“热点文章排行榜”。

![](https://images0.cnblogs.com/blog/321721/201307/24001549-e12283b3729e41cd98c6bf53fd1cbe47.jpg)

- 如何将赞成票和反对票结合起来，计算出一段时间内最受欢迎的文章？
- 如果文章 A 有 100 张赞成票、5 张反对票；文章 B 有 1000 张赞成票、950 张反对票，谁应该排在前面呢？

- Reddit 程序是[开源的](https://github.com/reddit/reddit)，使用 Python 语言编写。
    - ![](https://images0.cnblogs.com/blog/321721/201307/24001550-d216f81eea4245c1be0bc197aaca2fa2.gif)

- 算法公式
    - ![](https://images0.cnblogs.com/blog/321721/201307/24001552-3985179bca384fdeb0dee78b959982f2.gif)
    - 该算法考虑了 4 个因素，包括
        - 帖子**新旧程度**
        - **投票赞成与反对的差**
        - **投票倾向**
        - **帖子受肯定（否定）程度**

- 问题是，对于那些有争议的文章（赞成票和反对票非常接近），它们不可能排到前列。假定同一时间有两个帖子发表，文章A有 1 张赞成票（发帖人投的）、0 张反对票；文章 B 有1000 张赞成票、1000 张反对票，那么 A 的排名会高于 B，这显然不合理。
- 因此，Reddit 排名，基本上由发帖时间决定，超级受欢迎的文章会排在最前面，一般性受欢迎的文章、有争议的文章都不会很靠前。这决定了 Reddit 是一个符合大众口味的社区，不是一个很激进、可以展示少数派想法的地方。

### [Stack Overflow](http://stackoverflow.com/?tab=hot)

- Reddit 算法，用户可以投赞成票，也可以投反对票。即除了时间外，只需要考虑两个因素。
- 但是，对于一些特定的网站，必须考虑更多的因素。
- 如世界排名第一的程序员问答社区 Stack Overflow。
    - ![](https://images0.cnblogs.com/blog/321721/201307/24001554-d222230373634a5c971878884566492e.png)
    - 在上面提出各种关于编程的问题，然后等待别人回答。
    - 访问者可以对你的问题进行投票（赞成或反对），以表示这个问题是否有价值。
    - 一旦有人回答了你的问题，那么，其他人也可以对这个回答投票（赞成或反对），以表明对这个回答的态度。
- 排名算法的作用是，找出某段时间内的热点问题，即哪些问题最被关注、得到了最多的讨论。
- 在Stack Overflow 页面上，每个问题前面有三个数字，分别表示问题的得分、回答数，以及该问题的浏览次数。
- 创始人之一 Jeff Atwood，几年前，公开了该算法的[计算公式](http://meta.stackoverflow.com/questions/11602/what-formula-should-be-used-to-determine-hot-questions)
    - ![](https://images0.cnblogs.com/blog/321721/201307/24001555-6050f66d612b4ea1a15c25de4f0039d7.gif)
    - 1）问题的浏览次数Qviews
    - 2）问题得分Qanswers和回答的数量Qscore
    - 3）回答得分 Ascores
- Stack Overflow 热点问题的排名，与参与度（Qviews 和 Qanswers）和质量（Qscore 和 Ascores）成正比，与时间（Qage 和 Qupdated）成反比。

### 牛顿冷却定律

- 一个通用的数学模型。
- 可以把"热文排名"想象成一个"自然冷却"的过程：
    - 1）任一时刻，网站中所有的文章，都有一个“当前温度”，温度最高的文章就排在第一位。
    - 2）如果一个用户对某篇文章投了赞成票，该文章的温度就上升一度。
    - 3）随着时间流逝，所有文章的温度都逐渐“冷却”。
- 这样假设的意义在于，照搬物理学的冷却定律，使用现成的公式，建立“温度”与“时间”之间的函数关系，构建一个“[指数式衰减](http://en.wikipedia.org/wiki/Exponential_decay)（Exponential decay）”的过程。
    - ![](https://images0.cnblogs.com/blog/321721/201307/24001557-5ed5f6c72f944a9ead82381ff2b956c6.jpg)
- 牛顿早在 17 世纪就提出了温度冷却的数学公式，被后人称作“牛顿冷却定律（Newton's Law of Cooling）”，用这个定律构建排名算法。
- 用一句话概括“牛顿冷却定律”：物体的冷却速度，跟其当前温度与室温之间的温差成正比。写成微分方程如下：
    - ![](https://images0.cnblogs.com/blog/321721/201307/24001558-71580b66a13443eb9fffab2aef6204d2.gif)
    - T(t) 是温度T关于时间 t 的函数，则温度变化（冷却）的速率就是温度函数的导数 T'(t)。
    - H 代表室温，T(t)-H 是当前温度与室温之间的温差。由于当前温度高于室温，所以这是一个正值。
    - 常数 α(α>0) 表示室温与降温速率之间的比例关系。前面的负号表示降温。不同的物质有不同的 α 值。
- 方程求解
    - ![](https://images0.cnblogs.com/blog/321721/201307/24001602-be780ab62b004b2289e947af96e03186.gif)
- 套用在“排名算法”，就相当于（假定本期没有增加净赞成票）
    - ![](https://images0.cnblogs.com/blog/321721/201307/24001603-e7b50ef38b1f4ada9904da7735e352c6.gif)
- "冷却系数"是一个你自己决定的值。如果假定一篇新文章的初始分数是100分，24小时之后"冷却"为1分，那么可以计算得到"冷却系数"约等于0.192。如果你想放慢"热文排名"的更新率，"冷却系数"就取一个较小的值，否则就取一个较大的值
    - ![](https://images0.cnblogs.com/blog/321721/201307/24001603-047cda25d53f4326955ba878204be6b4.jpg)


------

> 迄今为止，这个系列都在讨论，如何给出"某个时段"的排名，比如"过去24小时最热门的文章"。但是，很多场合需要的是"所有时段"的排名，比如"最受用户好评的产品"。这时，时间因素就不需要考虑了。这个系列的最后两篇，就研究不考虑时间因素的情况下，如何给出排名。

### 威尔逊区间

- 一种常见的错误算法是：得分 = 赞成票 - 反对票
    - 假定有两个项目，项目 A 是 60 张赞成票，40 张反对票，项目 B 是 550 张赞成票，450 张反对票。请问，谁应该排在前面？按照上面的公式，B 会排在前面，因为它的得分（550 - 450 = 100）高于A（60 - 40 = 20）。但是实际上，B的好评率只有 55%（550 / 1000），而A为 60%（60 / 100），所以正确的结果应该是 A 排在前面。Urban Dictionary 就是这种错误算法的实例
- 另一种常见的错误算法是：得分 = 赞成票 / 总票数
    - 如果"总票数"很大，那么这种算法是有效的。否则，就会出错。假定 A 有 2 张赞成票、0 张反对票；B 有 100 张赞成票、1 张反对票。这种算法会使得 A 排在 B 前面。这显然是错误的。Amazon 就是这种错误算法的实例

![](https://images0.cnblogs.com/blog/321721/201307/24001604-1a46430bd3954d11826ad8dec8510ede.png)

那么，正确的算法是什么呢？

先做如下设定：
- 1）每个用户的投票都是独立事件。
- 2）用户只有两个选择，要么投赞成票，要么投反对票。
- 3）如果投票总人数为 n，其中赞成票为 k，那么赞成票的比例 p 就等于 k/n。

如果了解概率，这统计分布，叫“二项分布（binomial distribution）”

- 思路是，p 越大，就代表这个项目的好评比例越高，越应该排在前面。但 p 的可信性，取决于有多少人投票，如果样本太小，p 就不可信。好在我们已经知道，p 是二项分布中某个事件的发生概率，因此我们可以计算出p的置信区间。所谓“置信区间”，就是说，以某个概率而言，p 会落在的那个区间。比如，某个产品的好评率是 80%，但是这个值不一定可信。根据统计学，我们只能说，有 95% 的把握可以断定，好评率在 75%~85% 之间，即置信区间是 [75%, 85%]。
- 这样一来，排名算法就比较清晰了：
    - 第一步，计算每个项目的“好评率”（即赞成票的比例）。
    - 第二步，计算每个“好评率”的置信区间（以95%的概率）。
    - 第三步，根据置信区间的下限值，进行排名。这个值越大，排名就越高。
- 这样做的原理是，置信区间的宽窄与样本的数量有关。比如，A 有 8 张赞成票，2 张反对票；B 有 80 张赞成票，20 张反对票。这两个项目的赞成票比例都是 80%，但是 B 的置信区间（假定 [75%, 85%]）会比 A 的置信区间（假定 [70%, 90%]）窄得多，因此，B 的置信区间的下限值（75%）会比 A（70%）大，所以 B 应该排在 A 前面。
- 置信区间的实质，就是进行可信度的修正，弥补样本量过小的影响。如果样本多，就说明比较可信，不需要很大的修正，所以置信区间会比较窄，下限值会比较大；如果样本少，就说明不一定可信，必须进行较大的修正，所以置信区间会比较宽，下限值会比较小。
- 二项分布的置信区间有多种计算公式，最常见的”正态区间（Normal approximation interval）"，但它只适用于样本较多的情况（np > 5 且 n(1 − p) > 5），对于小样本，它的准确性很差。

1927年，美国数学家 Edwin Bidwell Wilson 提出了一个修正公式，被称为"威尔逊区间（Wilson score interval）"，很好地解决了小样本的准确性问题。
![](https://images0.cnblogs.com/blog/321721/201307/24001605-f762aafe9aa54074a7e096efd058de0e.gif)

- 当 n 的值足够大时，这个下限值会趋向p^。如果 n 非常小（投票人很少），这个下限值会大大小于p^。实际上，起到了降低"赞成票比例"的作用，使得该项目的得分变小、排名下降。
- Reddit 的评论排名，目前就使用这个算法。


### 贝叶斯平均

- “威尔逊区间”解决了投票人数过少、导致结果不可信的问题。
- 举例来说，如果只有 2 个人投票，“威尔逊区间”的下限值会将赞成票的比例大幅拉低。这样做固然保证了排名的可信性，但也带来了另一个问题：排行榜前列总是那些票数最多的项目，新项目或者冷门的项目，很难有出头机会，排名可能会长期靠后。
- 以 IMDB 为例，它是世界最大的电影数据库，观众可以对每部电影投票，最低为 1 分，最高为 10 分。系统根据投票结果，计算出每部电影的平均得分。然后，再根据平均得分，排出最受欢迎的前250名的电影。
- ![](https://images0.cnblogs.com/blog/321721/201307/24001609-4ddaa9affd2b4371b0b36d9fa0a63719.png)

问题：热门电影与冷门电影的平均得分，是否真的可比？举例来说，一部好莱坞大片有 10000 个观众投票，一部小成本的文艺片只有 100 个观众投票。这两者的投票结果，怎么比较？如果使用“威尔逊区间”，后者的得分将被大幅拉低，这样处理是否公平，能不能反映它们真正的质量？
 
-  一个合理的思路是，如果要比较两部电影的好坏，至少应该请同样多的观众观看和评分。既然文艺片的观众人数偏少，那么应该设法为它增加一些观众。
 - 在[排名页面](http://www.imdb.com/chart/top)的底部，IMDB 给出了它的计算方法。
 
[![clip_image002[19]](https://images0.cnblogs.com/blog/321721/201307/24001610-6cccd030f7364831af35c3b6ab95722d.gif "clip_image002[19]")](https://images0.cnblogs.com/blog/321721/201307/24001610-b18a83b672aa4550a1dc8f90d04aea24.gif)
 
其中，
*   WR，加权得分（weighted rating）。
*   R，该电影的用户投票的平均得分（Rating）。
*   v，该电影的投票人数（votes）。
*   m，排名前250名的电影的最低投票数（现在为3000）。
*   C， 所有电影的平均得分（现在为6.9）。
 
仔细研究这个公式，你会发现，IMDB 为每部电影增加了 3000 张选票，并且这些选票的评分都为 6.9。这样做的原因是，假设所有电影都至少有 3000 张选票，那么就都具备了进入前250名的评选条件；然后假设这 3000 张选票的评分是所有电影的平均得分（即假设这部电影具有平均水准）；最后，用现有的观众投票进行修正，长期来看，v/(v+m)这部分的权重将越来越大，得分将慢慢接近真实情况。
 
这样做拉近了不同电影之间投票人数的差异，使得投票人数较少的电影也有可能排名前列。
 
把这个公式写成更一般的形式：
 
[![clip_image002[21]](https://images0.cnblogs.com/blog/321721/201307/24001610-5daecf7b9bec4dce9cb03f7b0d43ad21.gif "clip_image002[21]")](https://images0.cnblogs.com/blog/321721/201307/24001610-9e9467db68104b88a17e58dc715a7d74.gif)
 
其中，
*   C，投票人数扩展的规模，是一个自行设定的常数，与整个网站的总体用户人数有关，可以等于每个项目的平均投票数。
*   n，该项目的现有投票人数。
*   x，该项目的每张选票的值。
*   m，总体平均分，即整个网站所有选票的算术平均值。
 
这种算法被称为["贝叶斯平均"](http://en.wikipedia.org/wiki/Bayesian_average)（Bayesian average）。因为某种程度上，它借鉴了["贝叶斯推断"](http://www.ruanyifeng.com/blog/2011/08/bayesian_inference_part_one.html)（Bayesian inference）的思想：既然不知道投票结果，那就先估计一个值，然后不断用新的信息修正，使得它越来越接近正确的值。
 
在这个公式中，m（总体平均分）是"先验概率"，每一次新的投票都是一个调整因子，使总体平均分不断向该项目的真实投票结果靠近。投票人数越多，该项目的"贝叶斯平均"就越接近算术平均，对排名的影响就越小。
 
因此，这种方法可以给一些投票人数较少的项目，以相对公平的排名。
 
"贝叶斯平均"也有缺点，主要问题是它假设用户的投票是正态分布。比如，电影A有10个观众评分，5个为五星，5个为一星；电影B也有 10 个观众评分，都给了三星。这两部电影的平均得分（无论是算术平均，还是贝叶斯平均）都是三星，但是电影A可能比电影B更值得看。
 
解决这个问题的思路是，假定每个用户的投票都是独立事件，每次投票只有n个选项可以选择，那么这就服从["多项分布"](http://en.wikipedia.org/wiki/Multinomial_distribution)（Multinomial distribution），就可以结合贝叶斯定理，估计该分布的期望值。由于这涉及复杂的统计学知识，这里就不深入了，感兴趣的朋友可以继续阅读William Morgan的 [How to rank products based on user input](http://masanjin.net/blog/how-to-rank-products-based-on-user-input)。


# CTR预估

- 天雨粟：[万字长文梳理CTR预估模型发展过程与关系图谱](https://zhuanlan.zhihu.com/p/104307718)
 

## 背景
 
在推荐、搜索、广告等领域，CTR（click-through rate）预估是一项非常核心的技术，这里引用阿里妈妈资深算法专家朱小强大佬的一句话：“它（CTR预估）是镶嵌在互联网技术上的明珠”。
 
本篇文章主要是对CTR预估中的常见模型进行梳理与总结，并分成模块进行概述。每个模型都会从「模型结构」、「优势」、「不足」三个方面进行探讨，在最后对所有模型之间的关系进行比较与总结。本篇文章讨论的模型如下图所示（原创图），这个图中展示了本篇文章所要讲述的算法以及之间的关系，在文章的最后总结会对这张图进行详细地说明。
 
![](https://pic3.zhimg.com/80/v2-5ca2b90a89c9719b1116906634e888c6_720w.jpg)
 
![](https://picb.zhimg.com/80/v2-8b631910f00b332b9291d4bfa8e6a968_720w.jpg)
 
 
## 一. 分布式线性模型
----------
 
### Logistic Regression
 
Logistic Regression是每一位算法工程师再也熟悉不过的基本算法之一了，毫不夸张地说，LR作为最经典的统计学习算法几乎统治了早期工业机器学习时代。这是因为其具备简单、时间复杂度低、可大规模并行化等优良特性。在早期的CTR预估中，算法工程师们通过手动设计交叉特征以及特征离散化等方式，赋予LR这样的线性模型对数据集的非线性学习能力，高维离散特征+手动交叉特征构成了CTR预估的基础特征。LR在工程上易于大规模并行化训练恰恰适应了这个时代的要求。
 
模型结构：
 
![[公式]](https://www.zhihu.com/equation?tex=f%28x%29%3Dw_0%2B%5Csum_%7Bi%3D1%7D%5En+w_i+x_i)
 
优势：
*   模型简单，具备一定可解释性
*   计算时间复杂度低
*   工程上可大规模并行化
 
不足：
*   依赖于人工大量的特征工程，例如需要根据业务背知识通过特征工程融入模型
*   特征交叉难以穷尽
*   对于训练集中没有出现的交叉特征无法进行参数学习
 
* * *

## 二. 自动化特征工程
----------
 
### GBDT + LR（2014）—— 特征自动化时代的初探索
 
Facebook在2014年提出了GBDT+LR的组合模型来进行CTR预估，其本质上是通过Boosting Tree模型本身的特征组合能力来替代原先算法工程师们手动组合特征的过程。GBDT等这类Boosting Tree模型本身具备了特征筛选能力（每次分裂选取增益最大的分裂特征与分裂点）以及高阶特征组合能力（树模型天然优势），因此通过GBDT来自动生成特征向量就成了一个非常自然的思路。注意这里虽然是两个模型的组合，但实际并非是端到端的模型，而是两阶段的、解耦的，即先通过GBDT训练得到特征向量后，再作为下游LR的输入，LR的在训练过程中并不会对GBDT进行更新。
 
模型结构：
 
通过GBDT训练模型，得到组合的特征向量。例如训练了两棵树，每棵树有5个叶子结点，对于某个特定样本来说，落在了第一棵树的第3个结点，此时我们可以得到向量 ![[公式]](https://www.zhihu.com/equation?tex=%5B0%2C0%2C1%2C0%2C0%5D) ；落在第二棵树的第4个结点，此时的到向量 ![[公式]](https://www.zhihu.com/equation?tex=%5B0%2C0%2C0%2C1%2C0%5D) ；那么最终通过concat所有树的向量，得到这个样本的最终向量 ![[公式]](https://www.zhihu.com/equation?tex=%5B0%2C0%2C1%2C0%2C0%2C0%2C0%2C0%2C1%2C0%5D) 。将这个向量作为下游LR模型的inputs，进行训练。
 
![](https://pic4.zhimg.com/80/v2-5152835e605985e026fc74fe42185c7b_720w.jpg)
 
优势：
*   特征工程自动化，通过Boosting Tree模型的天然优势自动探索特征组合
 
不足：
*   两阶段的、非端到端的模型
*   CTR预估场景涉及到大量高维稀疏特征，树模型并不适合处理（因此实际上会将dense特征或者低维的离散特征给GBDT，剩余高维稀疏特征在LR阶段进行训练）
*   GBDT模型本身比较复杂，无法做到online learning，模型对数据的感知相对较滞后（必须提高离线模型的更新频率）
 
* * *
 
## 三. FM模型以及变体
-----------
 
### （1）FM：Factorization Machines, 2010 —— 隐向量学习提升模型表达
 
FM是在2010年提出的一种可以学习二阶特征交叉的模型，通过在原先线性模型的基础上，枚举了所有特征的二阶交叉信息后融入模型，提高了模型的表达能力。但不同的是，模型在二阶交叉信息的权重学习上，采用了隐向量内积（也可看做embedding）的方式进行学习。
 
模型结构：
 
![](https://pic4.zhimg.com/80/v2-02627959debfaf325293833ef7a873cd_720w.jpg)
 
FM的公式包含了一阶线性部分与二阶特征交叉部分：
 
![[公式]](https://www.zhihu.com/equation?tex=f%28x%29%3Dw_0%2B%5Csum_%7Bi%3D1%7D%5En+w_i+x_i%2B%5Csum_%7Bi%3D1%7D%5En+%5Csum_%7Bj%3Di%2B1%7D%5En+%5Clangle+v_i%2Cv_j+%5Crangle+x_i+x_j)
 
在LR中，一般是通过手动构造交叉特征后，喂给模型进行训练，例如我们构造性别与广告类别的交叉特征： ![[公式]](https://www.zhihu.com/equation?tex=x_i%3D) (gender='女' & ad_category='美妆')，此时我们会针对这个交叉特征学习一个参数 ![[公式]](https://www.zhihu.com/equation?tex=w_i) 。但是在LR中，参数梯度更新公式与该特征取值 ![[公式]](https://www.zhihu.com/equation?tex=x_i) 关系密切：![[公式]](https://www.zhihu.com/equation?tex=w_i%3A%3Dw_i%2B%5Calpha%28y-%5Chat%7By%7D%29x_i) ，当 ![[公式]](https://www.zhihu.com/equation?tex=x_i) 取值为0时，参数 ![[公式]](https://www.zhihu.com/equation?tex=w_i) 就无法得到更新，而 ![[公式]](https://www.zhihu.com/equation?tex=x_i) 要非零就要求交叉特征的两项都要非零，但实际在数据高度稀疏，一旦两个特征只要有一个取0，参数 ![[公式]](https://www.zhihu.com/equation?tex=w_i) 不能得到有效更新；除此之外，对于训练集中没有出现的交叉特征，也没办法学习这类权重，泛化性能不够好。
 
另外，在FM中通过将特征隐射到k维空间求内积的方式，打破了交叉特征权重间的隔离性（break the independence of the interaction parameters），增加模型在稀疏场景下学习交叉特征的能力。一个交叉特征参数的估计，可以帮助估计其他相关的交叉特征参数。例如，假设我们有交叉特征gender=male & movie_genre=war，我们需要估计这个交叉特征前的参数 ![[公式]](https://www.zhihu.com/equation?tex=w_%7Bmale%5C_and%5C_war%7D)，FM通过将 ![[公式]](https://www.zhihu.com/equation?tex=w_%7Bmale%5C_and%5C_war%7D) 分解为 ![[公式]](https://www.zhihu.com/equation?tex=%5Clangle+v_%7Bmale%7D%2Cv_%7Bwar%7D%5Crangle) 的方式进行估计，那么对于每次更新male或者war的隐向量 ![[公式]](https://www.zhihu.com/equation?tex=v) 时，都会影响其他与male或者war交叉的特征参数估计，使得特征权重的学习不再互相独立。这样做的好处是，对于traindata set中没有出现过的交叉特征，FM仍然可以给到一个较好的非零预估值。
 
优势：
*   可以有效处理稀疏场景下的特征学习
*   具有线性时间复杂度（化简思路： ![[公式]](https://www.zhihu.com/equation?tex=ab%3D%5Cfrac%7B1%7D%7B2%7D%5B%28a%2Bb%29%5E2-%28a%5E2%2Bb%5E2%29%5D) ）
*   对训练集中未出现的交叉特征信息也可进行泛化
 
### 不足：
 
*   2-way的FM仅枚举了所有特征的二阶交叉信息，没有考虑高阶特征的信息

> FFM（Field-aware Factorization Machine）是Yuchin Juan等人在2015年的比赛中提出的一种对FM改进算法，主要是引入了field概念，即认为每个feature对于不同field的交叉都有不同的特征表达。FFM相比于FM的计算时间复杂度更高，但同时也提高了本身模型的表达能力。FM也可以看成只有一个field的FFM，这里不做过多赘述。
 
### （2）AFM：Attentional Factorization Machines, 2017 —— 引入Attention机制的FM
 
AFM全称Attentional Factorization Machines，顾名思义就是引入Attention机制的FM模型。我们知道FM模型枚举了所有的二阶交叉特征（second-order interactions），即 ![[公式]](https://www.zhihu.com/equation?tex=%5Csum_%7Bi%3D1%7D%5En%5Csum_%7Bj%3Di%2B1%7D%5En+%5Clangle+v_i%2Cv_j%5Crangle+x_i+x_j) ，实际上有一些交叉特征可能与我们的预估目标关联性不是很大；AFM就是通过Attention机制来学习不同二阶交叉特征的重要性（这个思路与FFM中不同field特征交叉使用不同的embedding实际上是一致的，都是通过引入额外信息来表达不同特征交叉的重要性）。
 
举例来说，在预估用户是否会点击广告时，我们假设有用户性别、广告版位尺寸大小、广告类型三个特征，分别对应三个embedding： ![[公式]](https://www.zhihu.com/equation?tex=v_%7Bgender%7D) ， ![[公式]](https://www.zhihu.com/equation?tex=v_%7Bad%5C_size%7D) ， ![[公式]](https://www.zhihu.com/equation?tex=v_%7Bad%5C_category%7D) ，对于用户“是否点击”这一目标 ![[公式]](https://www.zhihu.com/equation?tex=y) 来说，显然性别与ad_size的交叉特征对于 ![[公式]](https://www.zhihu.com/equation?tex=y) 的相关度不大，但性别与ad_category的交叉特征（如gender=女性&category=美妆）就会与 ![[公式]](https://www.zhihu.com/equation?tex=y) 更加相关；换句话说，我们认为当性别与ad\_category交叉时，重要性应该要高于性别与ad\_size的交叉；FFM中通过引入Field-aware的概念来量化这种与不同特征交叉时的重要性，AFM则是通过加入Attention机制，赋予重要交叉特征更高的重要性。
 
模型结构：
 
![](https://pic1.zhimg.com/80/v2-280a60b1ad44bdf3b193c6b9d818ff0c_720w.jpg)
 
AFM在FM的二阶交叉特征上引入Attention权重，公式如下：
 
![[公式]](https://www.zhihu.com/equation?tex=%5Chat%7By%7D%3Dw_0%2B%5Csum_%7Bi%3D1%7D%5En+w_i+x_i%2Bp%5ET%5Csum_%7Bi%3D1%7D%5En+%5Csum_%7Bj%3Di%2B1%7D%5En+%5Calpha_%7Bij%7D%28v_i%5Codot+v_j%29x_ix_j)
 
> 其中 ![[公式]](https://www.zhihu.com/equation?tex=%5Codot) 代表element-wise的向量相乘，下同。
 
其中， ![[公式]](https://www.zhihu.com/equation?tex=%5Calpha_%7Bij%7D) 是模型所学习到的 ![[公式]](https://www.zhihu.com/equation?tex=x_i) 与 ![[公式]](https://www.zhihu.com/equation?tex=x_j) 特征交叉的重要性，其公式如下：
 
![[公式]](https://www.zhihu.com/equation?tex=e_%7Bij%7D%3Dh%5ETReLU%28W%28v_i%5Codot+v_j%29x_ix_j%2Bb%29)
 
![[公式]](https://www.zhihu.com/equation?tex=%5Calpha_%7Bij%7D%3D%5Cfrac%7B%5Cexp%7B%28e_%7Bij%7D%29%7D%7D%7B%5Csum_%7Bi%2Cj%7D%5Cexp%28e_%7Bij%7D%29%7D)
 
我们可以看到这里的权重 ![[公式]](https://www.zhihu.com/equation?tex=%5Calpha_%7Bij%7D) 实际是通过输入 ![[公式]](https://www.zhihu.com/equation?tex=v_i) 和 ![[公式]](https://www.zhihu.com/equation?tex=v_j) 训练了一个一层隐藏层的NN网络，让模型自行去学习这个权重。
 
对比AFM和FM的公式我们可以发现，AFM实际上是FM的更加泛化的一种形式。当我们令向量 ![[公式]](https://www.zhihu.com/equation?tex=p%3D%5B1%2C1%2C1%2C%5Ccdots%2C1%5D%5ET) ，权重 ![[公式]](https://www.zhihu.com/equation?tex=%5Calpha_%7Bij%7D%3D1+) 时，AFM就会退化成FM模型。
 
优势：
*   在FM的二阶交叉项上引入Attention机制，赋予不同交叉特征不同的重要度，增加了模型的表达能力
*   Attention的引入，一定程度上增加了模型的可解释性
 
不足：
*   仍然是一种浅层模型，模型没有学习到高阶的交叉特征
 
* * *
 
## 四. Embedding+MLP结构下的浅层改造
------------------------
 
本章所介绍的都是具备Embedding+MLP这样结构的模型，之所以称作浅层改造，主要原因在于这些模型都是在embedding层进行的一些改变，例如FNN的预训练Embedding、PNN的Product layer、NFM的Bi-Interaction Layer等等，这些改变背后的思路可以归纳为：使用复杂的操作让模型在浅层尽可能包含更多的信息，降低后续下游MLP的学习负担。
 
### （1）FNN： Factorisation Machine supported Neural Network, 2016 —— 预训练Embedding的NN模型
 
FNN是2016年提出的一种基于FM预训练Embedding的NN模型，其思路也比较简单；FM本身具备学习特征Embedding的能力，DNN具备高阶特征交叉的能力，因此将两者结合是很直接的思路。FM预训练的Embedding可以看做是“先验专家知识”，直接将专家知识输入NN来进行学习。注意，FNN本质上也是两阶段的模型，与Facebook在2014年提出GBDT+LR模型在思想上一脉相承。
 
模型结构：
 
![](https://pic2.zhimg.com/80/v2-36bfe310f784f721755ad98a09009f8e_720w.jpg)
 
FNN本身在结构上并不复杂，如上图所示，就是将FM预训练好的Embedding向量直接喂给下游的DNN模型，让DNN来进行更高阶交叉信息的学习。
 
优势：
*   离线训练FM得到embedding，再输入NN，相当于引入先验专家经验
*   加速模型的训练和收敛
*   NN模型省去了学习feature embedding的步骤，训练开销低
 
不足：
*   非端到端的两阶段模型，不利于online learning
*   预训练的Embedding受到FM模型的限制
*   FNN中只考虑了特征的高阶交叉，并没有保留低阶特征信息
 
### （2）PNN：Product-based Neural Network, 2016 —— 引入不同Product操作的Embedding层
 
PNN是2016年提出的一种在NN中引入Product Layer的模型，其本质上和FNN类似，都属于Embedding+MLP结构。作者认为，在DNN中特征Embedding通过简单的concat或者add都不足以学习到特征之间复杂的依赖信息，因此PNN通过引入Product Layer来进行更复杂和充分的特征交叉关系的学习。PNN主要包含了IPNN和OPNN两种结构，分别对应特征之间Inner Product的交叉计算和Outer Product的交叉计算方式。
 
模型结构：
 
![](https://picb.zhimg.com/80/v2-8da07619b36c07e7a15c41e54a6dcebe_720w.jpg)
 
PNN结构显示通过Embedding Lookup得到每个field的Embedding向量，接着将这些向量输入Product Layer，在Product Layer中包含了两部分，一部分是左边的 ![[公式]](https://www.zhihu.com/equation?tex=z+) ，就是将特征原始的Embedding向量直接保留；另一部分是右侧的 ![[公式]](https://www.zhihu.com/equation?tex=p) ，即对应特征之间的product操作；可以看到PNN相比于FNN一个优势就是保留了原始的低阶embedding特征。
 
在PNN中，由于引入Product操作，会使模型的时间和空间复杂度都进一步增加。这里以IPNN为例，其中 ![[公式]](https://www.zhihu.com/equation?tex=p+) 是pair-wise的特征交叉向量，假设我们共有N个特征，每个特征的embedding信息 ![[公式]](https://www.zhihu.com/equation?tex=f_i%5Cin+R%5EM) ；在Inner Product的情况下，通过交叉项公式 ![[公式]](https://www.zhihu.com/equation?tex=g%28f_i%2Cf_j%29) 会得到 ![[公式]](https://www.zhihu.com/equation?tex=p%5Cin+R%5E%7BN%5Ctimes+N%7D) （其中 ![[公式]](https://www.zhihu.com/equation?tex=p) 是对称矩阵），此时从Product层到 ![[公式]](https://www.zhihu.com/equation?tex=L1+) 层（假设 ![[公式]](https://www.zhihu.com/equation?tex=L1) 层有 ![[公式]](https://www.zhihu.com/equation?tex=D_1) 个结点），对于 ![[公式]](https://www.zhihu.com/equation?tex=L1) 层的每个结点我们有： ![[公式]](https://www.zhihu.com/equation?tex=W_p%5En%5Codot+p%3D%5Csum_%7Bi%3D1%7D%5EN%5Csum_%7Bj%3D1%7D%5EN%28W_p%5En%29_%7Bi%2Cj%7Dp_%7Bi%2Cj%7D) ，因此这里从product layer到L1层参数空间复杂度为 ![[公式]](https://www.zhihu.com/equation?tex=O%28D_1N%5E2%29) ；作者借鉴了FM的思想对参数 ![[公式]](https://www.zhihu.com/equation?tex=W_p%5En) 进行了矩阵分解： ![[公式]](https://www.zhihu.com/equation?tex=W_p%5En%3D%5Ctheta%5En%5Ctheta%5E%7BnT%7D) ，此时L1层每个结点的计算可以化简为： ![[公式]](https://www.zhihu.com/equation?tex=W_p%5En%5Codot+p%3D%5Csum_%7Bi%3D1%7D%5EN%5Csum_%7Bj%3D1%7D%5EN%5Clangle+%5Ctheta_i%5En%2C%5Ctheta_j%5En%5Crangle+%5Clangle+f_i%2Cf_j%5Crangle) ，空间复杂度退化 ![[公式]](https://www.zhihu.com/equation?tex=O%28D_1NM%29) 。
 
优势：
*   PNN通过 ![[公式]](https://www.zhihu.com/equation?tex=z) 保留了低阶Embedding特征信息
*   通过Product Layer引入更复杂的特征交叉方式，
 
不足：
*   计算时间复杂度相对较高
    
 
### （3）NFM：Neural Factorization Machines, 2017 —— 引入Bi-Interaction Pooling结构的NN模型
 
NFM全程为Neural Factorization Machines，它与FNN一样，都属于将FM与NN进行结合的模型。但不同的是NFM相比于FNN是一种端到端的模型。NFM与PNN也有很多相似之出，本质上也属于Embedding+MLP结构，只是在浅层的特征交互上采用了不同的结构。NFM将PNN的Product Layer替换成了Bi-interaction Pooling结构来进行特征交叉的学习。
 
模型结构：
 
NFM的整个模型公式为：
 
![[公式]](https://www.zhihu.com/equation?tex=%5Chat%7By%7D%3Dw_0%2B%5Csum_%7Bi%3D1%7Dw_ix_i%2Bf%28%5Coverrightarrow%7Bx%7D%29)
 
其中 ![[公式]](https://www.zhihu.com/equation?tex=f%28%5Coverrightarrow%7Bx%7D%29) 是Bi-Interaction Pooling+NN部分的输出结果。我们重点关注NFM中的Bi-Interaction Pooling层：
 
![](https://picb.zhimg.com/80/v2-e155730e107679eac235efdf90336e38_720w.jpg)
 
NFM的结构如上图所示，通过对特征Embedding之后，进入Bi-Interaction Pooling层。这里注意一个小细节，NFM的对Dense Feature，Embedding方式于AFM相同，将Dense Feature Embedding以后再用dense feature原始的数据进行了scale，即 ![[公式]](https://www.zhihu.com/equation?tex=v_ix_i) 。
 
NFM的Bi-Interaction Pooling层是对两两特征的embedding进行element-wise的乘法，公式如下：
 
![[公式]](https://www.zhihu.com/equation?tex=f_%7BBI%7D%28V_x%29%3D%5Csum_%7Bi%3D1%7D%5En%5Csum_%7Bj%3Di%2B1%7D%5En+x_iv_i%5Codot+x_jv_j)
 
假设我们每个特征Embedding向量的维度为 ![[公式]](https://www.zhihu.com/equation?tex=k+) ，则 ![[公式]](https://www.zhihu.com/equation?tex=f_%7BBI%7D%5Cin+R%5Ek) ，Bi-Interaction Pooling的操作简单来说就是将所有二阶交叉的结果向量进行sum pooling后再送入NN进行训练。对比AFM的Attention层，Bi-Interaction Pooling层采用直接sum的方式，缺少了Attention机制；对比FM莫明星，NFM如果将后续DNN隐藏层删掉，就会退化为一个FM模型。
 
> NFM在输入层以及Bi-Interaction Pooling层后都引入了BN层，也加速了模型了收敛。
 
优势：
*   相比于Embedding的concat操作，NFM在low level进行interaction可以提高模型的表达能力
*   具备一定高阶特征交叉的能力
*   Bi-Interaction Pooling的交叉具备线性计算时间复杂度
 
不足：
*   直接进行sum pooling操作会损失一定的信息，可以参考AFM引入Attention
 
### （4）ONN：Operation-aware Neural Network, 2019 —— FFM与NN的结合体
 
ONN是2019年发表的CTR预估，我们知道PNN通过引入不同的Product操作来进行特征交叉，ONN认为针对不同的特征交叉操作，应该用不同的Embedding，如果用同样的Embedding，那么各个不同操作之间就会互相影响而最终限制了模型的表达。
 
我们会发现ONN的思路在本质上其实和FFM、AFM都有异曲同工之妙，这三个模型都是通过引入了额外的信息来区分不同field之间的交叉应该具备不同的信息表达。总结下来：
*   FFM：引入Field-aware，对于field a来说，与field b交叉和field c交叉应该用不同的embedding
*   AFM：引入Attention机制，a与b的交叉特征重要度与a与c的交叉重要度不同
*   ONN：引入Operation-aware，a与b进行内积所用的embedding，不同于a与b进行外积用的embedding
 
对比上面三个模型，本质上都是给模型增加更多的表达能力，个人觉得ONN就是FFM与NN的结合。
 
模型结构：
 
![](https://pic2.zhimg.com/80/v2-13519298002270ce240bb00c444cec53_720w.jpg)
 
ONN沿袭了Embedding+MLP结构。在Embedding层采用Operation-aware Embedding，可以看到对于一个feature，会得到多个embedding结果；在图中以红色虚线为分割，第一列的embedding是feature本身的embedding信息，从第二列开始往后是当前特征与第n个特征交叉所使用的embedding。
 
在Embedding features层中，我们可以看到包含了两部分：
*   左侧部分为每个特征本身的embedding信息，其代表了一阶特征信息
*   右侧部分是与FFM相同的二阶交叉特征部分
 
这两部分concat之后接入MLP得到最后的预测结果。
 
优势：
*   引入Operation-aware，进一步增加了模型的表达能力
*   同时包含了特征一阶信息与高阶交叉信息
 
不足：
*   模型复杂度相对较高，每个feature对应多个embedding结果
 
## 五. 双路并行的模型组合
------------
 
这一部分将介绍双路并行的模型结构，之所以称为双路并行，是因为在这一部分的模型中，以Wide&Deep和DeepFM为代表的模型架构都是采用了双路的结构。例如Wide&Deep的左路为Embedding+MLP，右路为Cross Feature LR；DeepFM的左路为FM，右路为Embedding+MLP。这类模型通过使用不同的模型进行联合训练，不同子模型之间互相弥补，增加整个模型信息表达和学习的多样性。
 
### （1）WDL：Wide and Deep Learning, 2016 —— Memorization与Generalization的信息互补
 
Wide And Deep是2016年Google提出的用于Google Play app推荐业务的一种算法。其核心思想是通过结合Wide线性模型的记忆性（memorization）和Deep深度模型的泛化性（generalization）来对用户行为信息进行学习建模。
 
模型结构：
 
![](https://picb.zhimg.com/80/v2-6f70ee1ad3e145b7eb12d13200776678_720w.jpg)
 
优势：
*   Wide层与Deep层互补互利，Deep层弥补Memorization层泛化性不足的问题
*   wide和deep的joint training可以减小wide部分的model size（即只需要少数的交叉特征）
*   可以同时学习低阶特征交叉（wide部分）和高阶特征交叉（deep部分）
 
### 不足：
 
*   仍需要手动设计交叉特征
    
 
### （2）DeepFM：Deep Factorization Machines, 2017 —— FM基础上引入NN隐式高阶交叉信息
 
我们知道FM只能够去显式地捕捉二阶交叉信息，而对于高阶的特征组合却无能为力。DeepFM就是在FM模型的基础上，增加DNN部分，进而提高模型对于高阶组合特征的信息提取。DeepFM能够做到端到端的、自动的进行高阶特征组合，并且不需要人工干预。
 
模型结构：
 
DeepFM包含了FM和NN两部分，这两部分共享了Embedding层：
 
![](https://pic1.zhimg.com/80/v2-255d5964372aec4bbd388e5dff6307aa_720w.jpg)
 
左侧FM部分就是2-way的FM：包含了线性部分和二阶交叉部分右侧NN部分与FM共享Embedding，将所有特征的embedding进行concat之后作为NN部分的输入，最终通过NN得到。
 
![[公式]](https://www.zhihu.com/equation?tex=%5Chat%7By%7D%3D%5Csigma%28logits_%7BFM%7D%2Blogits_%7BNN%7D%29)
 
优势：
*   模型具备同时学习低阶与高阶特征的能力
*   共享embedding层，共享了特征的信息表达


不足：
*   DNN部分对于高阶特征的学习仍然是隐式的
 
* * *
 
## 六. 复杂的显式特征交叉网络
--------------
 
无论是以FNN、PNN、NFM、ONN为代表的Embedding+MLP，还是以Wide&Deep和DeepFM为代表的双路模型，基本都是通过DNN来学习高阶特征交叉信息。但DNN本身对于特征交叉是隐式的（Implicit）、bit-wise的，因此在这一阶段，以DCN、xDeepFM、AutoInt为代表的模型均把思路放在如何以Explicit的方式学习有限阶（bounded-degree）的特征交叉信息上。
 
> Bit-wise：even the elements within the same field embedding vector will influence each other.
 
### （1）Deep&Cross：Deep and Cross Network, 2017 —— 显式交叉网络Cross Net的诞生
 
Deep&Cross其实也属于双路并行的模型结构，只不过提出了一种新的模型叫做Cross Net来替代DeepFM中的FM部分。DNN本身虽然具备高阶交叉特征的学习能力，但其对于特征交叉的学习是隐式的、高度非线性的一种方式，因此作者提出了Cross Net，它可以显式地进行特征的高阶交叉，CrossNet相比于DNN的优势主要在于：
*   可以显式地（Explicitly）学习有限阶（bounded-degree）的特征交叉
*   计算时间复杂度相比于DNN更加低

 
模型结构：

DCN模型包含了两部分，左边一路是通过CrossNet来显式地学习有限阶特征交叉，右边一路是通过DNN来隐式学习交叉特征，进一步提高模型的多样性和表达能力。
 
![](https://pic3.zhimg.com/80/v2-e52fc238149b68a503f178c2b9ead18f_720w.jpg)
 
CrossNet的主要思想是显式地计算内积来进行层与层之间的信息交叉；另外，CrossNet在设计上还借鉴了残差网络的思想，使得每一层的输出结果能够包含原始的输入信息。
 
![](https://pic1.zhimg.com/80/v2-02cb11f94ffbcfe2536601a8a73fdb38_720w.jpg)
 
对于CrossNet中的某一层，其计算方法如上图所示。分为三部分：

*   Feature Crossing：对input embeddings与上一层的输出进行交叉
*   Bias：偏置项
*   Input：上一层的输出（也是本层的输入）
 
公式可以表达为：
 
![[公式]](https://www.zhihu.com/equation?tex=x_%7Bl%2B1%7D%3Dx_0x_l%5ETw_l%2Bb_l%2Bx_l%3Df%28x_l%2Cw_l%2Cb_l%29%2Bx_l)
 
其中 ![[公式]](https://www.zhihu.com/equation?tex=x_0%2Cx_l%2Cw_l%2Cb_l%5Cin+R%5Ed) ，通过上式得到 ![[公式]](https://www.zhihu.com/equation?tex=f%28x_l%2Cw_l%2Cb_l%29%3Dx_%7Bl%2B1%7D-x_l) ，我们可以发现 mapping function ![[公式]](https://www.zhihu.com/equation?tex=f%3AR%5Ed%5Crightarrow+R%5Ed) 正好在拟合两层网络之间的残差。对于CrossNet中的第 ![[公式]](https://www.zhihu.com/equation?tex=l) 层，其能够捕捉到的特征交叉的最高阶为 ![[公式]](https://www.zhihu.com/equation?tex=l%2B1) 。
 
CrossNet本身在计算消耗上也不大，假设CrossNet共有 ![[公式]](https://www.zhihu.com/equation?tex=L) 层，输入的input vector是一个 ![[公式]](https://www.zhihu.com/equation?tex=d) 维向量，那么对于每一层来说有 ![[公式]](https://www.zhihu.com/equation?tex=w_l%2Cb_l) 两个参数，即 ![[公式]](https://www.zhihu.com/equation?tex=d%5Ctimes+2) 个参数，总共 ![[公式]](https://www.zhihu.com/equation?tex=L) 层，共有 ![[公式]](https://www.zhihu.com/equation?tex=L%5Ctimes+d%5Ctimes+2) 个参数，参数规模与输入的维度 ![[公式]](https://www.zhihu.com/equation?tex=d) 呈线性相关。
 
优势：
*   具备显式高阶特征交叉的能力
*   结合ResNet的思想，可以将原始信息在CrossNet中进行传递
 
不足：
*   CrossNet在进行交叉时是bit-wise方式
*   CrossNet最终的输出有一定的局限性，CrossNet的每一层输出都是输入向量 ![[公式]](https://www.zhihu.com/equation?tex=x_0) 的标量倍，这种形式在一定程度上限制了模型的表达能力
    
 
> 证明：  
> 我们令CrossNet的输入为 ![[公式]](https://www.zhihu.com/equation?tex=x_0%5Cin+R%5Ed) ，忽略每一层中的bias项，对于第一次cross，有：  
> ![[公式]](https://www.zhihu.com/equation?tex=x_1%3Dx_0x_0%5ETw_1%2Bx_0%3Dx_0%28x_0%5ETw_1%2B1%29%3D%5Calpha_1x_0) ，其中 ![[公式]](https://www.zhihu.com/equation?tex=%5Calpha_1%3Dx_0%5ETw_1%2B1) ；  
> 对于第二次cross，有：  
> ![[公式]](https://www.zhihu.com/equation?tex=x_2%3Dx_0x_1%5ETw_2%2Bx_1%3Dx_0%28%5Calpha_1x_0%29%5ETw_2%2B%5Calpha_1x_0%3D%5Calpha_1x_0%28x_0%5ETw_2%2B1%29%3D%5Calpha_2x_0) ，其中 ![[公式]](https://www.zhihu.com/equation?tex=%5Calpha_2%3D%5Calpha_1%28x_0%5ETw_2%2B1%29)  
> 基于上式进行推广可以得到 ![[公式]](https://www.zhihu.com/equation?tex=x_k%3D%5Calpha_kx_0) ，即证得CrossNet的输出是输入 ![[公式]](https://www.zhihu.com/equation?tex=x_0) 的标量倍。  
> 这里要注意的是， ![[公式]](https://www.zhihu.com/equation?tex=x_k) 与 ![[公式]](https://www.zhihu.com/equation?tex=x_0) 并不是线性关系，这是因为 ![[公式]](https://www.zhihu.com/equation?tex=%5Calpha_i) 也是关于 ![[公式]](https://www.zhihu.com/equation?tex=x_0) 的函数。
 
### （2）xDeepFM：eXtreme Deep Factorization Machine, 2018 —— Compressed Interaction Network的诞生
 
xDeepFM全称为eXtreme Deep Factorization Machine，可以看出其是在DeepFM基础上进行了改进。xDeepFM的贡献主要在于提出了压缩交互网络（Compressed Interaction Network），与DCN相同的是，都提出了要cross feature explicitly；但不同的是，DCN中的特征交叉是element-wise的，而CIN中的特征交叉是vector-wise的。
 
![](https://pic1.zhimg.com/80/v2-d8cca9bb1bbe48e65a77c86451aa3e1c_720w.jpg)
 
模型结构：
 
xDeepFM模型结构如下，整个模型分为三个部分：
*   Linear Part：捕捉线性特征
*   CIN Part：压缩交互网络，显式地、vector-wise地学习高阶交叉特征
*   DNN Part：隐式地、bit-wise地学习高阶交叉特征
  
 
![](https://picb.zhimg.com/80/v2-71206b222b3d286a722293491b675be7_720w.jpg)
 
CIN网络的设计主要分为两步：交互（interaction）与压缩（compression）。
 
在交互部分，如下图（a）所示，将第 ![[公式]](https://www.zhihu.com/equation?tex=k) 层的feature map与![[公式]](https://www.zhihu.com/equation?tex=x_0) （输入层，这里将输入层表示为一个 ![[公式]](https://www.zhihu.com/equation?tex=m%5Ctimes+D) 的tensor，其中m为特征个数，D为embedding的size）。在D的每一个维度上，进行外积计算，得到 ![[公式]](https://www.zhihu.com/equation?tex=z_%7Bk%2B1%7D%5Cin+R%5E%7BH_k%7D%5Ctimes+m%5Ctimes+D) 。
 
在压缩部分，借鉴了CNN卷积+Pooling的思想，先通过 ![[公式]](https://www.zhihu.com/equation?tex=H_%7Bk%2B1%7D) 个filter将三维的 ![[公式]](https://www.zhihu.com/equation?tex=z_%7Bk%2B1%7D) （可看做一张图片）进行压缩计算，得到 ![[公式]](https://www.zhihu.com/equation?tex=x_%7Bk%2B1%7D%5Cin+R%5E%7BH_%7Bk%2B1%7D%5Ctimes+D%7D) 。紧接着在D维上进行sum pooling操作，得到最后输出向量（如c图中的黄色小圆圈）。
 
经过多个串行的压缩与交互步骤，可以得到多个输出向量，最终将这些向量concat起来，作为CIN的输出结果。
 
![](https://picb.zhimg.com/80/v2-f561091ee742864eda3924d37a07e134_720w.jpg)
 
可以看出CIN在计算上相对比较复杂，但是由于CNN参数共享机制以及sum pooling层的存在，CIN部分的参数规模与特征的Embedding size大小 ![[公式]](https://www.zhihu.com/equation?tex=D) 是无关的。假设输入field有 ![[公式]](https://www.zhihu.com/equation?tex=m) 个，共有 ![[公式]](https://www.zhihu.com/equation?tex=L) 层，每层有 ![[公式]](https://www.zhihu.com/equation?tex=H) 个feature map，那么CIN部分的参数规模为 ![[公式]](https://www.zhihu.com/equation?tex=H%5Ctimes+H%5Ctimes+m+%5Ctimes+L) 。
 
但是在时间复杂度上，CIN存在很大劣势，CIN的时间复杂度为 ![[公式]](https://www.zhihu.com/equation?tex=O%28mH%5E2DL%29) 。
 
优势：
*   xDeepFM可以同时学习到显式的高阶特征交叉（CIN）与隐式的高阶特征交叉（DNN）
*   在交叉特征的学习上，CIN采用了vector-wise的交叉（而不是DCN中的bit-wise交叉）
 
不足：
*   CIN在实际计算中时间复杂度过高
*   CIN的sum-pooling操作会损失一定的信息
 
### （3）AutoInt：Automatic Feature Interaction Learning, 2019 —— 跨领域NLP技术的引入：Multi-head Self-attention提升模型表达
 
AutoInt是2019年发表的比较新的论文，它的思路和DCN以及xDeepFM相似，都是提出了能够显式学习高阶特征交叉的网络。除此之外，AutoInt算法借鉴了NLP模型中Transformer的Multi-head self-attention机制，给模型的交叉特征引入了可解释性，可以让模型知道哪些特征交叉的重要性更大。
 
AutoInt的Attention机制采用了NLP中标准的Q,K,V形式，即给定Query词和候选的Key词，计算相关性 ![[公式]](https://www.zhihu.com/equation?tex=%5Calpha) ，再用 ![[公式]](https://www.zhihu.com/equation?tex=%5Calpha) 对Value进行加权得到结果。
 
模型结构：
 
相比于DCN和xDeepFM采用交叉网络+DNN的双路结构，AutoInt直接采用了单路的模型结构，将原始特征Embedding之后，直接采用多层Interacting Layer进行学习（作者在论文的实验部分也列出了AutoInt+DNN的双路模型结构：AutoInt+）。
 
![](https://picb.zhimg.com/80/v2-bc9273d3a93930afa65ee82265769347_720w.jpg)
 
AutoInt中的Interacting Layer包含了两部分：Multi-head Self-Attention和ResNet部分。
 
在self-attention中，采用的是Q,K,V形式，具体来说：我们只考虑1个head self-attention的情况，假设我们共有 ![[公式]](https://www.zhihu.com/equation?tex=M) 个特征，对于输入的第 ![[公式]](https://www.zhihu.com/equation?tex=m) 个feature embedding来说，AutoInt认为它与![[公式]](https://www.zhihu.com/equation?tex=M) 个特征交叉后的特征拥有不同的权重，对于我们第 ![[公式]](https://www.zhihu.com/equation?tex=m) 个特征，它与第 ![[公式]](https://www.zhihu.com/equation?tex=k) 个特征交叉的权重为：
 
![[公式]](https://www.zhihu.com/equation?tex=%5Calpha_%7Bm%2Ck%7D%3D%5Cfrac%7B%5Cexp+%28%5Cphi%28e_m%2Ce_k%29%29%7D%7B%5Csum_%7Bl%3D1%7D%5EM%5Cexp%28%5Cphi%28e_m%2Ce_l%29%29%7D)
 
其中 ![[公式]](https://www.zhihu.com/equation?tex=%5Cphi%28e_m%2Ce_k%29%3D%5Clangle+W_%7BQuery%7De_m%2CW_%7BKey%7De_k+%5Crangle) ，函数 ![[公式]](https://www.zhihu.com/equation?tex=%5Cphi) 是衡量两个向量距离的函数，在AutoInt中作者采用了简单高效的向量内积来计算距离。得到权重信息后，我们对M个特征的Value进行加权： ![[公式]](https://www.zhihu.com/equation?tex=%5Ctilde%7Be%7D_m%3D%5Csum_%7Bk%3D1%7D%5EM%5Calpha_%7Bm%2Ck%7D%28W_%7BValue%7De_k%29) ，得到向量m与其余特征的加权二阶交叉信息。
 
![](https://pic1.zhimg.com/80/v2-c6d0712e7f1ca95e957fca195a434ba2_720w.jpg)
 
进一步地，作者使用了多个self-attention（multi-head self-attention）来计算不同subspaces中的特征交叉，其实就是进一步增加了模型的表达能力。采用h个multi-head之后，我们会得到h个 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctilde%7Be%7D_m%5E%7B%28h%29%7D) ，将这h个 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctilde%7Be%7D_m%5E%7B%28h%29%7D) concat起来，得到 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctilde%7Be%7D_m%3D%5B%5Ctilde%7Be%7D%5E%7B%281%29%7D%2C%5Ctilde%7Be%7D%5E%7B%282%29%7D%2C%5Ccdots%2C%5Ctilde%7Be%7D%5E%7B%28h%29%7D%5D) 。
 
为了保留上一步学到的交叉信息，AutoInt和CrossNet一样，都使用了ResNet的思想：
 
![[公式]](https://www.zhihu.com/equation?tex=e_m%5E%7BRes%7D%3DReLU%28%5Ctilde%7Be%7D_m%2BW_%7BRes%7De_m%29)
 
使用ResNet可以使得之前学习到的信息也被更新到新的层中，例如第一层原始的embedding也可以被融入到最终的输出中。
 
剩余的特征也以同样的方式进行multi-head attention计算，得到 ![[公式]](https://www.zhihu.com/equation?tex=e_1%5E%7BRes%7D%2Ce%5E%7BRes%7D_2%2C%5Ccdots%2Ce_M%5E%7BRes%7D) ，将这M个向量concat之后连接输出层得到最终的预估值。
 
优势：
*   AutoInt可以显示地、以vector-wise的方式地学习有限阶（bounded-degree）特征交叉信息
*   可以以low interacting layer学习到higher-order feature interaction
 
> 原文这里给出了一个例子，两层Interacting Layer就可以学习到4阶特征交叉。定义交叉函数为 ![[公式]](https://www.zhihu.com/equation?tex=g%28%5Ccdot%29)， 假如我们有4个特征 ![[公式]](https://www.zhihu.com/equation?tex=x_1%2Cx_2%2Cx_3%2Cx_4) ，第一层Interacting Layer之后，我们可以得到 ![[公式]](https://www.zhihu.com/equation?tex=g%28x_1%2Cx_2%29%2Cg%28x_1%2Cx_3%29%2Cg%28x_2%2Cx_3%29) 等二阶交叉信息，即两两特征的二阶交叉；将二阶交叉送入下一层Interacting Layer之后，由于输入第一层网络融入了二阶交叉信息，那么在本层中就可以得到四阶交叉，如 ![[公式]](https://www.zhihu.com/equation?tex=g%28x_1%2Cx_2%2Cx_3%2Cx_4%29) 就可以通过 ![[公式]](https://www.zhihu.com/equation?tex=g%28g%28x_1%2Cx_3%29%2Cg%28x_2%2Cx_4%29%29) 得到。
 
*   Interacting Layer的参数规模与输入特征个数 ![[公式]](https://www.zhihu.com/equation?tex=M) 无关。
    
 
* * *
 
## 七. CTR预估模型总结与比较
---------------
 
至此我们基本介绍完成了大多数常见的CTR预估模型，当然还有MLR、DIN、DIEN等其它的模型，由于篇幅限制暂时没有进行介绍。纵观整个CTR预估模型的发展过程，我们可以总结出一定的规律，这一部分主要是对上述模型的关系图谱以及特征进行总结。
 
### （1）CTR预估模型关系图谱
 
现在我们再回头来看开篇的这张关系图：
 
![](https://pic4.zhimg.com/80/v2-487aa1bb3473fba349827a6b99a7b7b7_720w.jpg)
 
从上往下，代表了整个CTR预估的发展趋势：
*   LR的主要限制在于需要大量手动特征工程来间接提高模型表达，此时出现了两个发展方向：
*   以FM为代表的端到端的隐向量学习方式，通过embedding来学习二阶交叉特征
*   以GBDT+LR为代表的两阶段模型，第一阶段利用树模型优势自动化提取高阶特征交叉，第二阶段交由LR进行最终的学习
*   以FM为结点，出现了两个方向：
*   以FFM与AFM为代表的浅层模型改进。这两个模型本质上还是学习低阶交叉特征，只是在FM基础上为不同的交叉特征赋予的不同重要度
*   深度学习时代到来，依附于DNN高阶交叉特征能力的Embedding+MLP结构开始流行
 
*   以Embedding+MLP为结点：
*   Embedding层的改造+DNN进行高阶隐式学习，出现了以PNN、NFM为代表的product layer、bi-interaction layer等浅层改进，这一类模型都是对embedding层进行改造来提高模型在浅层表达，减轻后续DNN的学习负担
*   以W&D和DeepFM为代表的双路模型结构，将各个子模块算法的优势进行互补，例如DeepFM结合了FM的低阶交叉信息和DNN的高阶交叉信息学习能力
*   显式高阶特征交叉网络的提出，这一阶段以更复杂的网络方式来进行显式交叉特征的学习，例如DCN的CrossNet、xDeepFM的CIN、AutoInt的Multi-head Self-attention结构
 
从整个宏观趋势来看，每一阶段新算法的提出都是在不断去提升模型的表达能力，从二阶交叉，到高阶隐式交叉，再到如今的高阶显示交叉，模型对于原始信息的学习方式越来越复杂的同时，也越来越准确。
 
图中右侧红色字体提取了部分模型之间的共性：
*   Hand-crafted features：LR与W&D都需要进行手动的特征工程
*   Non-end-to-end：GBDT+LR通过树模型提取特征+LR建模的两阶段，FNN则是FM预训练embedding+DNN建模的两阶段方式，这两者都是非端到端的模型
*   Multi-embeddings：这里是指对于同一个特征，使用多个embedding来提升信息表达。包括FFM的Field-aware，ONN的Operation-aware
*   Attention：Attention机制为CTR预估中的交叉特征赋予了不同的重要性，也增加了一定的可解释性。AFM中采用单个隐藏层的神经网络构建attention层，AutoInt在Interacting Layer中采用NLP中QKV形式学习multi-head self-attention
*   Explicitly Interactions：DNN本身学习的是隐式特征交叉，DCN、xDeepFM、AutoInt则都提出了显式特征交叉的网络结构
*   ResNet：ResNet的引入是为了保留历史的学习到的信息，CrossNet与AutoInt中都采用了ResNet结构


### （2）CTR预估模型特性对比
 
这里对比主要包含了一下几个方面：
*   No Pretraining：是否需要预训练
*   Automatic Feature Engineering：是否自动进行特征组合与特征工程
*   End-To-End：是否是端到端的模型
*   Low-Order Features：是否包含低阶特征信息
*   High-Order Features：是否包含高阶特征信息
*   Explicitly High-Order Crossing：是否包含显式特征交叉   
 
![](https://pic1.zhimg.com/80/v2-7daeeceabca034bfd72133bef520500f_720w.jpg)
 
## 结语

至此我们对于常见的CTR预估模型的演进过程与关系就讲解完毕，纵观整个过程，CTR预估模型从开始的LR，到利用树模型自动化组合特征，再发展到端到端的Embedding+MLP结构，再到如今越来越复杂的显式交叉网络等，每一次发展都是在不断提升模型对于用户行为的表达与学习能力。CTR预估不仅是一个数学优化问题，更是一个工程问题，因此如何能够以较低的计算成本，高效地提高模型表达能力将是未来需要努力的方向。


# 结束


