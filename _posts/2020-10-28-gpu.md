---
layout: post
title:  "并行计算系列 GPU/TPU"
date:   2020-10-28 19:25:00
categories: 编程语言
tags: CPU GPU TPU Tensorflow Pytorch
excerpt: 高性能计算知识点，如CPU/GPU/TPU，及代码实现
author: 鹤啸九天
mathjax: true
---

* content
{:toc}

# 总结

- 【2020-10-28】[CUDA编程入门极简教程](https://blog.csdn.net/xiaohu2022/article/details/79599947)

# 简介

- 2006年，NVIDIA公司发布了`CUDA`，CUDA是建立在NVIDIA的CPUs上的一个通用并行计算平台和编程模型，基于CUDA编程可以利用GPUs的并行计算引擎来更加高效地解决比较复杂的计算难题。
- 近年来，`GPU`最成功的一个应用就是**深度学习**领域，基于GPU的并行计算已经成为训练深度学习模型的标配。截止2018年3月，最新的CUDA版本为CUDA 9。
- GPU并不是一个独立运行的计算平台，而需要与CPU协同工作，可以看成是CPU的协处理器，因此说GPU并行计算时，其实是指的**基于CPU+GPU的异构计算架构**。
- 在异构计算架构中，GPU与CPU通过PCIe总线连接在一起来协同工作，CPU所在位置称为为**主机端**（host），而GPU所在位置称为**设备端**（device），如下图所示。
  - ![](https://img-blog.csdn.net/20180318132344300?watermark/2/text/Ly9ibG9nLmNzZG4ubmV0L3hpYW9odTIwMjI=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
- GPU包括更多的运算核心，其特别适合**数据并行**的**计算密集型**任务，如大型矩阵运算，而CPU的运算核心较少，但是其可以实现复杂的逻辑运算，因此其适合控制密集型任务。
- 另外，CPU上的线程是重量级的，**上下文切换开销大**，但是GPU由于存在很多核心，其线程是轻量级的。
- 因此，基于CPU+GPU的异构计算平台可以优势互补，CPU负责处理逻辑复杂的串行程序，而GPU重点处理数据密集型的并行计算程序，从而发挥最大功效。
- ![](https://img-blog.csdn.net/20180318132422473?watermark/2/text/Ly9ibG9nLmNzZG4ubmV0L3hpYW9odTIwMjI=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)


- [在显卡界，对于「A 卡」和「N 卡」的信仰是如何形成的？](https://www.zhihu.com/question/28520691)
- ![](https://pic4.zhimg.com/80/8bfb07ebbf27f2e486a0ae933515a58d_720w.jpg?source=1940ef5c)

- 显卡性能天梯图
    - [如何通过显卡名字判断显卡的好坏？](https://www.zhihu.com/question/34970298)


## CPU

- CPU 如何来执行这样的大型矩阵运算任务呢？一般 CPU 是基于冯诺依曼架构的通用处理器，这意味着 CPU 与软件和内存的运行方式如下

## CUDA

- CUDA是NVIDIA公司所开发的GPU编程模型，它提供了GPU编程的简易接口，基于CUDA编程可以构建基于GPU计算的应用程序。CUDA提供了对其它编程语言的支持，如C/C++，Python，Fortran等语言，这里我们选择CUDA C/C++接口对CUDA编程进行讲解。开发平台为Windows 10 + VS 2013，Windows系统下的CUDA安装教程可以参考[这里](http://docs.nvidia.com/cuda/cuda-installation-guide-microsoft-windows/index.html)
- CUDA编程模型支持的编程语言
    - ![](https://img-blog.csdn.net/2018031813244714?watermark/2/text/Ly9ibG9nLmNzZG4ubmV0L3hpYW9odTIwMjI=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
- CUDA编程模型基础
    - 在给出CUDA的编程实例之前，这里先对CUDA编程模型中的一些概念及基础知识做个简单介绍。CUDA编程模型是一个异构模型，需要CPU和GPU协同工作。在CUDA中，host和device是两个重要的概念，我们用host指代CPU及其内存，而用device指代GPU及其内存。CUDA程序中既包含host程序，又包含device程序，它们分别在CPU和GPU上运行。同时，host与device之间可以进行通信，这样它们之间可以进行数据拷贝。典型的CUDA程序的执行流程如下：
        - 分配host内存，并进行数据初始化；
        - 分配device内存，并从host将数据拷贝到device上；
        - 调用CUDA的核函数在device上完成指定的运算；
        - 将device上的运算结果拷贝到host上；
        - 释放device和host上分配的内存。
- 参考：[CUDA编程入门极简教程](https://blog.csdn.net/xiaohu2022/article/details/79599947)

## GPU

### 行情

GPU市场上，NVIDIA占了大部分（**N卡**），AMD（**A卡**）次之，接着是**苹果**（好像是intel），详见[图](https://upload-images.jianshu.io/upload_images/64542-4715aee0485de0ea.jpeg)
- <img src="https://upload-images.jianshu.io/upload_images/64542-4715aee0485de0ea.jpeg" width = "400" height = "300" alt="设备对比" align=center />
- [2019显卡天梯图汇总](http://www.ppnames.com/html/360.html)
- ![](http://www.ppnames.com/img2019/20181017/102.png)

### GPU验证

① NVIDIA自带
- NVIDIA自带的驱动检测方法，能看到GPU的配置，可以看到Google Colab的GPU是Tesla T4，显存15G，强于上一版本K80 

```shell
#命令
nvidia-smi
# 结果如下：
Wed Jun  5 03:02:36 2019       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 418.67       Driver Version: 410.79       CUDA Version: 10.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |
| N/A   57C    P8    16W /  70W |      0MiB / 15079MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+

# 动态监控
watch -n 1 nvidia-smi
```

GPU监控
- 动态监控GPU使用率
- 按Ctrl+C退出监控
- [参考GPU进程查看管理](https://blog.csdn.net/Kaige_Zhao/article/details/79748079)

② GPU监控工具-[gpustat](https://www.ctolib.com/wookayin-gpustat.html)

- 仅适用于N卡（NVIDIA ），A卡不行（AMD）
- 效果示例：[图](https://github.com/wookayin/gpustat/raw/master/screenshot.png)
   - ![](https://github.com/wookayin/gpustat/raw/master/screenshot.png)
   - []里的数字是GPU编号，即常见的多机多卡里的“卡”

```shell
#pip install gpustat # 安装
gpustat -cp # 检测
# 结果如下
528e504a63a3  Wed Jun  5 03:16:16 2019
[0] Tesla T4         | 48'C,   0 % |     0 / 15079 MB |
# Google的Colab配置了Tesla T4的显卡一个，显存15G
```

③ Pytorch的GPU测试方法

```python
import torch

torch.cuda.is_available() # true
torch.backends.cudnn.enabled # true
```

④ Tensorflow的GPU测试代码

```python
import tensorflow as tf
device_name = tf.test.gpu_device_name()
print('Detect GPU:', device_name) # ('Detect GPU:', '/device:GPU:0')
if device_name != '/device:GPU:0':
  raise SystemError('GPU device not found')
print('Found GPU at: {}'.format(device_name)) # Found GPU at: /device:GPU:0
```

### tf里GPU参数设置

GPU与CPU的不同
- GPU:计算密集型任务
- CPU: I/O, 逻辑运算

- [gpu使用方法](https://www.jianshu.com/p/26ac409dfb38)
- 通过tf.device指定运行设备，不管CPU多少个，一律标记为/cpu0，而GPU不同，分别是/gpu:1-n
- Tensorflow使用GPU时默认占满所有可用GPU的显存，但只在第一个GPU上进行计算，多GPU时，只有一块GPU真正在工作，如果不加以利用，另一块GPU就白白浪费了
- GPU是一种相对昂贵的计算资源，虽然正值矿难，相比之前动辄八九千一块1080Ti的价格低了不少，但也不是一般人能浪费的起的，因此如何有效提高GPU特别是tensorflow上的利用率就成为了一项重要的考量。
- 解决一：设置可见的GPU，CUDA_VISIBLE_DEVICES, 0表示GPU编号(0~n-1)

Shell代码：

```sh
export CUDA_VISIBLE_DEVICES=0
```
Python代码：

```python
import os
os.environ["CUDA_VISIBLE_DEVICES"]="0"
```

- 缓解tf占满整个GPU

```python
config = tf.ConfigProto(allow_soft_placement=True,allow_grouth=True)
config.gpu_options.per_process_gpu_memory_fraction = 0.9 #占用90%显存
sess = tf.Session(config=config)
```

- 解决二：有时候我们更需要利用好已有的卡，来或得线性的加速比，以便在更短的时候获取参考结果，上面的方法就无能无力，需要自己写代码实现多GPU编程
- 参考：[tensorflow 多GPU编程 完全指南](https://blog.csdn.net/minstyrain/article/details/80986397)


### GPU 如何工作

- 为了获得比 CPU 更高的吞吐量，GPU 使用一种简单的策略：在单个处理器中使用成千上万个 ALU。现代 GPU 通常在单个处理器中拥有 2500-5000 个 ALU，意味着同时执行数千次乘法和加法运算。

### Parallelism-GPU并行

- There are two types of parallelism:
- 模型并行，Model parallelism - Different GPUs run different part of the code. Batches of data pass through all GPUs.
    - 适用于大规模
- 数据并行，Data parallelism - We use multiple GPUs to run the same TensorFlow code. Each GPU is feed with different batch of data. 每个节点是各有一份模型copy，称为tower
    - 同步+异步，同步适用于设备差异不大，小数据，异步抖动，适用大数据
    - 图内+图间

## TPU

- 资料
    - 【2019-08-11】[TPU灵魂三问：What？Why？How？](https://www.sohu.com/a/333008988_505915)
    - [TPU是如何超越GPU，成为深度学习首选处理器的](https://www.toutiao.com/a6596935089408442883/?tt_from=mobile_qq&utm_campaign=client_share&timestamp=1536065103&app=news_article&utm_source=mobile_qq&iid=42461222229&utm_medium=toutiao_android&group_id=6596935089408442883)
- 张量处理单元（TPU）是一种定制化的 ASIC 芯片，它由谷歌从头设计，并专门用于机器学习工作负载。TPU 为谷歌的主要产品提供了计算支持，包括翻译、照片、搜索助理和 Gmail 等。Cloud TPU 将 TPU 作为可扩展的云计算资源，并为所有在 Google Cloud 上运行尖端 ML 模型的开发者与数据科学家提供计算资源
- 在 Google Next’18 中，我们宣布 TPU v2 现在已经得到用户的广泛使用，包括那些免费试用用户，而 TPU v3 目前已经发布了内部测试版。

- 谷歌设计 TPU 时，构建了一种领域特定的架构。这意味着，没有设计一种通用的处理器，而是专用于神经网络工作负载的矩阵处理器。TPU 不能运行文本处理软件、控制火箭引擎或执行银行业务，但它们可以为神经网络处理大量的乘法和加法运算，同时 TPU 的速度非常快、能耗非常小且物理空间占用也更小。
- 其主要助因是对冯诺依曼瓶颈的大幅度简化。因为该处理器的主要任务是矩阵处理，TPU 的硬件设计者知道该运算过程的每个步骤。因此他们放置了成千上万的乘法器和加法器并将它们直接连接起来，以构建那些运算符的物理矩阵。这被称作脉动阵列（Systolic Array）架构。在 Cloud TPU v2 的例子中，有两个 128X128 的脉动阵列，在单个处理器中集成了 32768 个 ALU 的 16 位浮点值。
- 我们来看看一个脉动阵列如何执行神经网络计算。首先，TPU 从内存加载参数到乘法器和加法器的矩阵中。


# GPU环境准备

## Colab实现

- 【2019-06-05】[colab笔记代码](https://colab.research.google.com/drive/1uqhBy1zNF7uU7EdrQqwlMSoykgxvubyg)
- [gpu使用方法](https://www.jianshu.com/p/26ac409dfb38)

```python
import tensorflow as tf
import os

#GPU:计算密集型任务, CPU: I/O, 逻辑运算
if tf.test.is_gpu_available(): # 检测GPU是否可用
    print 'GPU可用: %s'%(tf.test.gpu_device_name())
# 环境变量设置，默认有限选择GPU，占用所有GPU的所有显存
config = tf.ConfigProto()
os.environ['TF_CPP_MIN_LOG_LEVEL']='5'#日志级别
#----自定义使用哪些GPU----
os.environ["CUDA_VISIBLE_DEVICES"] = "2"#只使用第三块GPU，默认选用id最小的gpu
os.environ["CUDA_VISIBLE_DEVICES"] = "0,1,2"#只使用第2-3块GPU
#----自定义显存分配----
config.gpu_options.allow_growth = True#刚开始分配较少内存，逐步按需分配，注意：系统不自动释放内存，以免可能导致更严重的内存碎片情况
config.gpu_options.per_process_gpu_memory_fraction = 0.4#按比例分配,40%，方便多任务并行

a = tf.constant([1.0, 2.0, 3.0], shape=[3], name='a')
b = tf.constant([1.0, 2.0, 3.0], shape=[3], name='b')
c = a + b
#通过tf.device指定运行设备。不管CPU多少个，一律标记为/cpu0,而GPU不同，分别是/gpu:1-n
#注意：不宜包含过多设备约束，不同设备实现方式不同，多了会限制移植能力
with tf.device('/gpu:0'):
    #将tf.Variable强制放在GPU上会报错,GPU只在部分数据类型上支持tf.Variable操作（variable_ops.cc）
    #除非启动自动更改设备的参数allow_soft_placement
    a_gpu = tf.Variable(0, name="a_gpu")
# 通过log_device_placement参数来输出运行每一个运算的设备。
sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))
# 通过allow_soft_placement参数自动将无法放在GPU上的操作放回CPU上。
sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True, log_device_placement=True))
#sess.run(tf.initialize_all_variables())
sess.run(tf.global_variables_initializer())
print(sess.run(c))
#jupyter中显示不出来
```

没GPU时的结果：

```shell
2018-03-26 19:29:38.606679: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
Device mapping: no known devices.
2018-03-26 19:29:38.606972: I tensorflow/core/common_runtime/direct_session.cc:299] Device mapping:
add: (Add): /job:localhost/replica:0/task:0/device:CPU:0
2018-03-26 19:29:38.610106: I tensorflow/core/common_runtime/placer.cc:874] add: (Add)/job:localhost/replica:0/task:0/device:CPU:0
b: (Const): /job:localhost/replica:0/task:0/device:CPU:0
2018-03-26 19:29:38.610137: I tensorflow/core/common_runtime/placer.cc:874] b: (Const)/job:localhost/replica:0/task:0/device:CPU:0
a: (Const): /job:localhost/replica:0/task:0/device:CPU:0
2018-03-26 19:29:38.610142: I tensorflow/core/common_runtime/placer.cc:874] a: (Const)/job:localhost/replica:0/task:0/device:CPU:0
[ 2.  4.  6.]
```

有GPU时的结果：

```
2018-03-26 19:27:46.864593: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 0 with properties: 
name: Tesla P40 major: 6 minor: 1 memoryClockRate(GHz): 1.531
pciBusID: 0000:02:00.0
totalMemory: 22.38GiB freeMemory: 745.94MiB
2018-03-26 19:27:46.864673: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla P40, pci bus id: 0000:02:00.0, compute capability: 6.1)
Device mapping:
/job:localhost/replica:0/task:0/device:XLA_GPU:0 -> device: XLA_GPU device
/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device
/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: Tesla P40, pci bus id: 0000:02:00.0, compute capability: 6.1
2018-03-26 19:27:47.193611: I tensorflow/core/common_runtime/direct_session.cc:297] Device mapping:
/job:localhost/replica:0/task:0/device:XLA_GPU:0 -> device: XLA_GPU device
/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device
/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: Tesla P40, pci bus id: 0000:02:00.0, compute capability: 6.1
add: (Add): /job:localhost/replica:0/task:0/device:GPU:0
2018-03-26 19:27:47.194462: I tensorflow/core/common_runtime/placer.cc:874] add: (Add)/job:localhost/replica:0/task:0/device:GPU:0
b: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-03-26 19:27:47.194498: I tensorflow/core/common_runtime/placer.cc:874] b: (Const)/job:localhost/replica:0/task:0/device:GPU:0
a: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-03-26 19:27:47.194526: I tensorflow/core/common_runtime/placer.cc:874] a: (Const)/job:localhost/replica:0/task:0/device:GPU:0
[2. 4. 6.]
```

# Tensorflow分布式实现

- 黄文坚的[Tensorflow分布式实战](https://blog.csdn.net/CodeMaster_/article/details/76223835)

## 单机单卡

- 单机单卡是最普通的情况，当然也是最简单的，示例代码如下：

```python
#coding=utf-8
#单机单卡，对于单机单卡，可以把参数和计算都定义再gpu上，不过如果参数模型比较大，显存不足等情况，就得放在cpu上
import  tensorflow as tf
with tf.device('/cpu:0'):#也可以放在gpu上
    w=tf.get_variable('w',(2,2),tf.float32,initializer=tf.constant_initializer(2))
    b=tf.get_variable('b',(2,2),tf.float32,initializer=tf.constant_initializer(5))
with tf.device('/gpu:0'):
    addwb=w+b
    mutwb=w*b
init=tf.initialize_all_variables()
with tf.Session() as sess:
    sess.run(init)
    np1,np2=sess.run([addwb,mutwb])
    print np1,np2
```

## 单机多卡

- 单机多卡，只要用device直接指定设备，就可以进行训练，SGD采用各个卡的平均值
- 问题：除了取均值，还有别的方式吗？

```python
#coding=utf-8
#单机多卡：一般采用共享操作定义在cpu上，然后并行操作定义在各自的gpu上，比如对于深度学习来说，我们一般把参数定义、参数梯度更新统一放在cpu上，各个gpu通过各自计算各自batch数据的梯度值，然后统一传到cpu上，由cpu计算求取平均值，cpu更新参数。具体的深度学习多卡训练代码，请参考：https://github.com/tensorflow/models/blob/master/inception/inception/inception_train.py
import  tensorflow as tf
  
with tf.device('/cpu:0'):
    w=tf.get_variable('w',(2,2),tf.float32,initializer=tf.constant_initializer(2))
    b=tf.get_variable('b',(2,2),tf.float32,initializer=tf.constant_initializer(5))
with tf.device('/gpu:0'):
    addwb=w+b
with tf.device('/gpu:1'):
    mutwb=w*b
  
ini=tf.initialize_all_variables()
with tf.Session() as sess:
    sess.run(ini)
    while 1:
        print sess.run([addwb,mutwb])
```
- 多个 GPU 上运行 TensorFlow，则可以采用多塔式方式构建模型，其中每个塔都会分配给不同 GPU。例如：

```python
# Creates a graph.
c = []
for d in ['/device:GPU:2', '/device:GPU:3']:
  with tf.device(d):
    a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3])
    b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2])
    c.append(tf.matmul(a, b))
with tf.device('/cpu:0'):
  sum = tf.add_n(c)
# Creates a session with log_device_placement set to True.
sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))
# Runs the op.
print(sess.run(sum))
```

- 【2020-5-20】每个gpu的梯度要累加起来，单独计算

```python
        # train op def
        tower_grads = []
        for i in xrange(FLAGS.num_gpus):
            with tf.device('/gpu:{}'.format(i)):
                with tf.name_scope('tower_{}'.format(i)):
                    next_batch = dhs.get_next_batch()
                    cnn.inference(
                        next_batch[0], next_batch[1], next_batch[2],
                        dropout_keep_prob=FLAGS.dropout_keep_prob,
                        input_dropout_keep_prob=FLAGS.input_dropout_keep_prob,
                        phase_train=True)
                    grads = optimizer.compute_gradients(cnn.loss)
                    tower_grads.append(grads)
        grads = average_gradients(tower_grads)
        train_op = optimizer.apply_gradients(grads, global_step=global_step)

def average_gradients(tower_grads):
    """
    Calculate the average gradient for each shared variable across all towers.
    Note that this function provides a synchronization point across all towers.
    NOTE: This function is copied from cifar codes in tensorflow tutorial with minor
    modification.
    Args:
        tower_grads: List of lists of (gradient, variable) tuples. The outer list
            is over individual gradients. The inner list is over the gradient
            calculation for each tower.
    Returns:
       List of pairs of (gradient, variable) where the gradient has been averaged
       across all towers.
    """
    average_grads = []
    for grad_and_vars in zip(*tower_grads):
        # Note that each grad_and_vars looks like the following:
        #   ((grad0_gpu0, var0_gpu0), ... , (grad0_gpuN, var0_gpuN))
        grads = []
        for g, _ in grad_and_vars:
            # Add 0 dimension to the gradients to represent the tower.
            # NOTE: if batch norm applied, the grad of conv-maxpool-n/b will be
            #       None
            if g is None:
                continue
            expanded_g = tf.expand_dims(g, 0)

            # Append on a 'tower' dimension which we will average over below.
            grads.append(expanded_g)

        # Average over the 'tower' dimension.
        if grads:
            grad = tf.concat(axis=0, values=grads)
            grad = tf.reduce_mean(grad, 0)
        else:
            grad = None

        # Keep in mind that the Variables are redundant because they are shared
        # across towers. So .. we will just return the first tower's pointer to
        # the Variable.
        v = grad_and_vars[0][1]
        grad_and_var = (grad, v)
        average_grads.append(grad_and_var)
    return average_grads
```

- 参考官网：[TensorFlow with multiple GPUs](https://jhui.github.io/2017/03/07/TensorFlow-GPU/)



## 多机多卡

- 一、基本概念
    - Cluster、Job、task概念：三者可以简单的看成是层次关系
    - task相当于每台机器上的一个进程，多个task组成job；
    - job又有两种：ps参数服务、worker计算服务，组成cluster。
- 二、同步SGD与异步SGD
    - 1、同步更新：各个用于并行计算的电脑，计算完各自的batch 后，求取梯度值，把梯度值统一送到ps服务机器中，由ps服务机器求取梯度平均值，更新ps服务器上的参数。
        - 如下图所示，可以看成有四台电脑，第一台电脑用于存储参数、共享参数、共享计算，可以简单的理解成内存、计算共享专用的区域，也就是ps job；另外三台电脑用于并行计算的，也就是worker task。
        - 这种计算方法存在的缺陷是：每一轮的梯度更新，都要等到A、B、C三台电脑都计算完毕后，才能更新参数，也就是迭代更新速度取决与A、B、C三台中，最慢的那一台电脑，所以采用同步更新的方法，建议A、B、C三台的计算能力都不想。
    - 2、异步更新：ps服务器收到只要收到一台机器的梯度值，就直接进行参数更新，无需等待其它机器。这种迭代方法比较不稳定，收敛曲线震动比较厉害，因为当A机器计算完更新了ps中的参数，可能B机器还是在用上一次迭代的旧版参数值。

代码编写
- 1、定义集群
    - 比如假设上面的图所示，我们有四台电脑，名字假设为：A、B、C、D，那么集群可以定义如下
```python
#coding=utf-8
#多台机器，每台机器有一个显卡、或者多个显卡，这种训练叫做分布式训练
import  tensorflow as tf
#现在假设我们有A、B、C、D四台机器，首先需要在各台机器上写一份代码，并跑起来，各机器上的代码内容大部分相同
# ，除了开始定义的时候，需要各自指定该台机器的task之外。以机器A为例子，A机器上的代码如下：
cluster=tf.train.ClusterSpec({
    "worker": [
        "A_IP:2222",#格式 IP地址：端口号，第一台机器A的IP地址 ,在代码中需要用这台机器计算的时候，就要定义：/job:worker/task:0
        "B_IP:1234"#第二台机器的IP地址 /job:worker/task:1
        "C_IP:2222"#第三台机器的IP地址 /job:worker/task:2
    ],
    "ps": [
        "D_IP:2222",#第四台机器的IP地址 对应到代码块：/job:ps/task:0
    ]})
```
然后需要写四分代码，这四分代码文件大部分相同，但是有几行代码是各不相同的。

- 2、在各台机器上，定义server
    - 比如A机器上的代码server要定义如下：
```python
server=tf.train.Server(cluster,job_name='worker',task_index=0)#找到‘worker’名字下的，task0，也就是机器A
```

- 3、在代码中，指定device
```python
with tf.device('/job:ps/task:0'):#参数定义在机器D上
    w=tf.get_variable('w',(2,2),tf.float32,initializer=tf.constant_initializer(2))
    b=tf.get_variable('b',(2,2),tf.float32,initializer=tf.constant_initializer(5))
with tf.device('/job:worker/task:0/cpu:0'):#在机器A cpu上运行
    addwb=w+b
with tf.device('/job:worker/task:1/cpu:0'):#在机器B cpu上运行
    mutwb=w*b
with tf.device('/job:worker/task:2/cpu:0'):#在机器C cpu上运行
    divwb=w/b
```

在深度学习训练中，一般图的计算，对于每个worker task来说，都是相同的，所以我们会把所有图计算、变量定义等代码，都写到下面这个语句下：
```python
with tf.device(tf.train.replica_device_setter(worker_device='/job:worker/task:indexi',cluster=cluster))
```
函数replica_deviec_setter会自动把变量参数定义部分定义到ps服务中(如果ps有多个任务，那么自动分配)。下面举个例子，假设现在有两台机器A、B，A用于计算服务，B用于参数服务，那么代码如下：
```python
#coding=utf-8
#上面是因为worker计算内容各不相同，不过再深度学习中，一般每个worker的计算内容是一样的，
# 以为都是计算神经网络的每个batch 前向传导，所以一般代码是重用的
import  tensorflow as tf
#现在假设我们有A、B台机器，首先需要在各台机器上写一份代码，并跑起来，各机器上的代码内容大部分相同
# ，除了开始定义的时候，需要各自指定该台机器的task之外。以机器A为例子，A机器上的代码如下：
cluster=tf.train.ClusterSpec({
    "worker": [
        "192.168.11.105:1234",#格式 IP地址：端口号，第一台机器A的IP地址 ,在代码中需要用这台机器计算的时候，就要定义：/job:worker/task:0
    ],
    "ps": [
        "192.168.11.130:2223"#第四台机器的IP地址 对应到代码块：/job:ps/task:0
    ]})
  
#不同的机器，下面这一行代码各不相同，server可以根据job_name、task_index两个参数，查找到集群cluster中对应的机器
  
isps=False
if isps:
    server=tf.train.Server(cluster,job_name='ps',task_index=0)#找到‘worker’名字下的，task0，也就是机器A
    server.join()
else:
    server=tf.train.Server(cluster,job_name='worker',task_index=0)#找到‘worker’名字下的，task0，也就是机器A
    with tf.device(tf.train.replica_device_setter(worker_device='/job:worker/task:0',cluster=cluster)):
        w=tf.get_variable('w',(2,2),tf.float32,initializer=tf.constant_initializer(2))
        b=tf.get_variable('b',(2,2),tf.float32,initializer=tf.constant_initializer(5))
        addwb=w+b
        mutwb=w*b
        divwb=w/b

saver = tf.train.Saver()
summary_op = tf.merge_all_summaries()
init_op = tf.initialize_all_variables()
sv = tf.train.Supervisor(init_op=init_op, summary_op=summary_op, saver=saver)
with sv.managed_session(server.target) as sess:
    while 1:
        print sess.run([addwb,mutwb,divwb])
```

把该代码在机器A上运行，你会发现，程序会进入等候状态，等候用于ps参数服务的机器启动，才会运行。因此接着我们在机器B上运行如下代码：
```python
#coding=utf-8
#上面是因为worker计算内容各不相同，不过再深度学习中，一般每个worker的计算内容是一样的，
# 以为都是计算神经网络的每个batch 前向传导，所以一般代码是重用的
#coding=utf-8
#多台机器，每台机器有一个显卡、或者多个显卡，这种训练叫做分布式训练
import  tensorflow as tf
#现在假设我们有A、B、C、D四台机器，首先需要在各台机器上写一份代码，并跑起来，各机器上的代码内容大部分相同
# ，除了开始定义的时候，需要各自指定该台机器的task之外。以机器A为例子，A机器上的代码如下：
cluster=tf.train.ClusterSpec({
    "worker": [
        "192.168.11.105:1234",#格式 IP地址：端口号，第一台机器A的IP地址 ,在代码中需要用这台机器计算的时候，就要定义：/job:worker/task:0
    ],
    "ps": [
        "192.168.11.130:2223"#第四台机器的IP地址 对应到代码块：/job:ps/task:0
    ]})
  
#不同的机器，下面这一行代码各不相同，server可以根据job_name、task_index两个参数，查找到集群cluster中对应的机器
  
isps=True
if isps:
    server=tf.train.Server(cluster,job_name='ps',task_index=0)#找到‘worker’名字下的，task0，也就是机器A
    server.join()
else:
    server=tf.train.Server(cluster,job_name='worker',task_index=0)#找到‘worker’名字下的，task0，也就是机器A
    with tf.device(tf.train.replica_device_setter(worker_device='/job:worker/task:0',cluster=cluster)):
        w=tf.get_variable('w',(2,2),tf.float32,initializer=tf.constant_initializer(2))
        b=tf.get_variable('b',(2,2),tf.float32,initializer=tf.constant_initializer(5))
        addwb=w+b
        mutwb=w*b
        divwb=w/b
  
saver = tf.train.Saver()
summary_op = tf.merge_all_summaries()
init_op = tf.initialize_all_variables()
sv = tf.train.Supervisor(init_op=init_op, summary_op=summary_op, saver=saver)
with sv.managed_session(server.target) as sess:
  
    while 1:
        print sess.run([addwb,mutwb,divwb])
```
- [Tensorflow官方指南](https://www.tensorflow.org/versions/master/how_tos/distributed/index.html)
- 分布式训练需要熟悉的函数：
    - tf.train.Server
    - tf.train.Supervisor
    - tf.train.SessionManager
    - tf.train.ClusterSpec
    - tf.train.replica_device_setter
    - tf.train.MonitoredTrainingSession
    - tf.train.MonitoredSession
    - tf.train.SingularMonitoredSession
    - tf.train.Scaffold
    - tf.train.SessionCreator
    - tf.train.ChiefSessionCreator
    - tf.train.WorkerSessionCreator


# 结束


