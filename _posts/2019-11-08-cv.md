---
layout: post
title:  "图像识别-Image-Recognizer"
date:   2019-11-08 16:52:00
categories: 计算机视觉
tags: 深度学习 计算机视觉 GAN 扫地机器人 自动驾驶 何恺明 CVPR 论文 sota OCR 硕士论文 ocr
excerpt: 图像风格迁移是什么原理？具体如何实施？可以迁移到文本吗？
mathjax: true
---

* content
{:toc}


# 说明

- 【2022-8-25】生成人脸：[this-person-does-not-exist.com](https://this-person-does-not-exist.com/en)
- 【2022-8-23】[国产AI作画神器火了，更懂中文，竟然还能做周边](https://mp.weixin.qq.com/s/xh6Q0Pnv9OfP8Je3lDiyZg), “一句话生成画作”这个圈子里，又一个AI工具悄然火起来了,不是你以为的Disco Diffusion、DALL·E，再或者Imagen……而是全圈子都在讲中国话的那种, 文心·一格
  - 操作界面上，Disco Diffusion开放的接口不能说很复杂，但确实有点门槛。它直接在谷歌Colab上运行，需要申请账号后使用（图片生成后保存在云盘），图像分辨率、尺寸需要手动输入，此外还有一些模型上的设置。好处是可更改的参数更多，对于高端玩家来说可操作性更强，只是比较适合专门研究AI算法的人群;相比之下，文心·一格的操作只需三个步骤：输入文字，鼠标选择风格&尺寸，点击生成。
  - 提示词，Disco Diffusion的设置还要更麻烦一些。除了描述画面的内容以外，包括画作类别和参考的艺术家风格也都得用提示词来设置，通常大伙儿会在其他文档中编辑好，再直接粘过来。相比之下文心·一格倒是没有格式要求，输入150字的句子或词组都可以
  - 性能要求上，Disco Diffusion是有GPU使用限制的，每天只能免费跑3小时。抱抱脸（HuggingFace）上部分AI文生图算法的Demo虽然操作简单些，但一旦网速不行，就容易加载不出来; 文心·一格除了使用高峰期以外，基本上都是2分钟就能生成，对使用设备也没有要求。
  - 总体来看，同样是文字生成图片AI，实际相比文心·一格的“真·一句话生成图片”，DALL·E和Disco Diffusion的生成过程都不太轻松。
- 【2022-7-11】AI工具：[解你描述的东西还可以画出来的AI，真就全靠想象](https://www.ixigua.com/7117201327523892488?wid_try=1)
  - 一、[DALL•E2](https://openai.com/dall-e-2/)：openai出品，理解并画出你描述的所有东西，如：一直粉红色大象在撒哈拉沙漠玩扑克
  - 二、[AI智能图片放大](https://bigjpg.com/zh)：还原马赛克图片，就连表情包也能高清重置
  - 三、线稿自动上色：[Style2paints](http://paintstransfer.com/), [GitHub 链接](https://github.com/lllyasviel/style2paints)
    - 用 AI 技术为黑白线稿快速自动上色。在最近推出的 2.0 版中，研究人员使用了完全无监督的生成对抗网络（GAN）训练方法大幅提高了上色的准确性。Style2paints 的作者表示，该工具在精细度、漫画风格转换等方面超越了目前其他所有工具。
- 【2021-11-22】image2text的反向：text2image，NVIDIA Demo; 用深度学习模型(GauGAN)可以将文本转换成图片, demo，[GauGAN AI Art Demo](http://gaugan.org/gaugan2)，只要输入一句简短的描述就可以生成图片了。下图是他们演示的“海浪击打岩石”的效果。
  - [The absurd beauty of hacking Nvidia's GauGAN 2 AI image machine](https://www.zdnet.com/article/the-absurd-beauty-of-hacking-nvidias-gaugan-2-ai-image-machine/)
  - ![](https://p6.toutiaoimg.com/img/tos-cn-i-qvj2lq49k0/50832d2763094d32ab1c2e974d7a625a~tplv-obj:480:272.gif)
- 汇总计算机视觉的应用案例
- 【2021-3-26】[视频大脑：视频内容理解的技术详解和应用](https://www.infoq.cn/article/vhIXoD0CAqmojPHeKP5f/)，[极客时间视频](https://time.geekbang.org/dailylesson/detail/100022917),黄君实 奇虎360 人工智能研究院资深研发科学家
  - ![](https://static001.infoq.cn/resource/image/46/6a/46ccd482ad8a09752bca0a184aaca56a.png)

# sota

【2021-6-22】[CVPR 2021大奖公布！何恺明获最佳论文提名，代码已开源](https://mp.weixin.qq.com/s/sdboE0KmvCV-Zc2R6hs0Tg)

推特上，有学者打趣说，CV论文可以分为这几类：
- 「只想混文凭」
- 「教电脑生成更多猫的照片」
- 「ImageNet上实验结果提升0.1%！」
- 「手握超酷数据集但并不打算公开」
- 「3年过去了，代码仍在赶来的路上」
- 「实验证明还是老baseline性能更牛」
- 「我们的数据集更大！」
- 「研究范围超广，无他，我们有钱」
- 「花钱多，结果好」......

何恺明和Xinlei Chen的论文Exploring Simple Siamese Representation Learning（探索简单的连体表征学习）获得了最佳论文提名。

「连体网络」（Siamese networks）已经成为最近各种无监督视觉表征学习模型中的一种常见结构。这些模型最大限度地提高了一个图像的两个增量之间的相似性，但必须符合某些条件以避免collapse的解决方案。在本文中，我们报告了令人惊讶的经验结果，即简单的连体网络即使不使用以下任何一种情况也能学习有意义的表征。(i) 负样本对，(ii) 大batch，(iii) 动量编码器。我们的实验表明，对于损失和结构来说，collapse的解决方案确实存在，但stop-gradient操作在防止collapse方面发挥了重要作用。我们提供了一个关于stop-gradient含义的假设，并进一步展示了验证该假设的概念验证实验。我们的 「SimSiam 」方法在ImageNet和下游任务中取得了有竞争力的结果。我们希望这个简单的基线能促使人们重新思考连体结构在无监督表征学习中的作用。

代码已开源 https://github.com/facebookresearch/simsiam


## 何恺明编年史

![](https://pic1.zhimg.com/v2-5e022845e2440e673f98a11f99ac6dac_1440w.jpg?source=172ae18b)
- [何恺明：从高考状元到CV领域年轻翘楚，靠“去雾算法”成为“CVPR最佳论文”首位华人得主](https://zhuanlan.zhihu.com/p/55621213)

何恺明履历
- 出生于广州的何恺明是家中独子，父母均在企业里从事管理工作，从小就接触到优良的教学环境。实际上，能从众多学子中脱颖而出，除了教学环境之外，更多的是靠自己的努力。
- 何恺明年少时就被送到少年宫学习绘画，有时一待就是大半天，这也不断使他练就出沉稳的性格。同绘画一样，他对于文化课的钻研也十分耐得住性子，学习成绩优秀而且稳定。在老师的心目中，他是一个“性格比较内向”但是“目标明确”的学生，“从小就立志上清华”。
- 高中时，全国物理竞赛一等奖被保送进清华大学机械工程及其自动化专业，不去，偏要考，结果成了2003年广东理科状元；
- 大学期间，何恺明继续着自己沉稳而优秀的表现，不仅连续3年获得清华奖学金，2007年，还未毕业的他就进入了微软亚洲研究院（MSRA）实习。
- 本科毕业后，他进入香港中文大学攻读研究生，师从AI名人汤晓鸥；
- 2009年，第一篇论文“Single ImageHaze Removalusing Dark Channel Prior”被计算机视觉领域顶级会议CVPR接收并被评为年度最佳论文，CVPR创办25年来华人学者第一次获此殊荣，也使何恺明在CV领域声名鹊起
- 2011年，博士毕业的何恺明正式加入MSRA计算机视觉和深度学习的研究工作。
- 2015年的ImageNet图像识别大赛中，何恺明和他的团队凭借152层深度残差网络ResNet-152，击败谷歌、英特尔、高通等业界团队，荣获第一。目前ResNets也已经成为计算机视觉领域的流行架构，同时也被用于机器翻译、语音合成、语音识别和AlphaGo的研发上。
- 2016年，何恺明凭借ResNets论文再次获得CVPR最佳论文奖，也是目前少有的一人两次获得CVPR最佳论文奖的学者。
- 后来，何恺明和孙剑相继离开MSRA。与孙剑的选择不同，何凯明走得还是那条学院路。他选择了去Facebook，担任其人工智能实验室研究科学家，选择了进一步走学术之路。
- 2017年3月，何恺明和同事公布了其最新的研究Mask R-CNN，提出了一个概念上简单、灵活和通用的用于目标实例分割（object instance segmentation）框架，能够有效地检测图像中的目标，同时还能为每个实例生成一个高质量的分割掩码。同年，凭借《利用焦点损失提升物体检测效果》这篇论文，他一举夺下了另一个计算机视觉顶级会议ICCV最佳论文奖。
- 2018年，何恺明在美国盐湖城召开的CVPR上，获得了PAMI青年研究者奖。几个月前，何恺明等人发表论文称，ImageNet预训练却并非必须。何恺明和其同事使用随机初始化的模型，不借助外部数据就取得了不逊于COCO 2017冠军的结果，再次引发业内关注。


【2022-1-12】[何恺明编年史](https://zhuanlan.zhihu.com/p/415353143)

别人的荣誉都是在某某大厂工作，拿过什么大奖，而何恺明的荣誉是best，best，best ...... kaiming科研嗅觉顶级，每次都能精准的踩在最关键的问题上，提出的方法简洁明了，同时又蕴含着深刻的思考，文章赏心悦目，实验详尽扎实，工作质量说明一切。

何恺明的研究兴趣大致分成这么几个阶段：
- 传统视觉时代：Haze Removal(3篇)、Image Completion(2篇)、Image Warping(3篇)、Binary Encoding(6篇)
- 深度学习时代：Neural Architecture(11篇)、Object Detection(7篇)、Semantic Segmentation(11篇)、Video Understanding(4篇)、Self-Supervised(8篇)

代表作
- 2009 CVPR best paper Single Image Haze Removal Using Dark Channel Prior
  - 利用实验观察到的暗通道先验，巧妙的构造了图像**去雾算法**。现在主流的图像去雾算法还是在Dark Channel Prior的基础上做的改进。
- 2016 CVPR best paper Deep Residual Learning for Image Recognition
  - 通过**残差连接**，可以训练非常深的卷积神经网络。不管是之前的CNN，还是最近的ViT、MLP-Mixer架构，仍然摆脱不了残差连接的影响。
- 2017 ICCV best paper **Mask R-CNN**
  - 在Faster R-CNN的基础上，增加一个实例分割分支，并且将RoI Pooling替换成了RoI Align，使得实例分割精度大幅度提升。虽然最新的实例分割算法层出不穷，但是精度上依然难以超越Mask R-CNN。
  - ![](https://pic1.zhimg.com/80/v2-55b2b7227b553659dd7deea52082bef4_720w.jpg)
- 2017 ICCV best student paper Focal Loss for Dense Object Detection
  - 构建了一个**One-Stage**检测器RetinaNet，同时提出Focal Loss来处理One-Stage的类别不均衡问题，在目标检测任务上首次One-Stage检测器的速度和精度都优于Two-Stage检测器。近些年的One-Stage检测器(如FCOS、ATSS)，仍然以RetinaNet为基础进行改进。
  - ![](https://pic3.zhimg.com/80/v2-7628af32f42bc07197bdc27bc02f9d52_720w.jpg)
- 2020 CVPR Best Paper Nominee Momentum Contrast for Unsupervised Visual Representation Learning
  - 19年末，NLP领域的Transformer进一步应用于Unsupervised representation learning，产生后来影响深远的BERT和GPT系列模型，反观CV领域，ImageNet刷到饱和，似乎遇到了怎么也跨不过的屏障。就在CV领域停滞不前的时候，Kaiming He带着**MoCo**横空出世，横扫了包括PASCAL VOC和COCO在内的7大数据集，至此，CV拉开了Self-Supervised研究新篇章。

# 数字图像处理

## 硕士论文

【2022-01-24】
- 硕士毕业论文：[图像分割配准技术在小鼠舌头三维重建中的应用](https://www.docin.com/p-1966389845.html)
- 计算机工程学报：[基于形态学的小鼠舌头切片图像分割与实现](https://jz.docin.com/p-533278994.html)

## 图像分割
- 【2020-7-17】图像分割（Image Segmentation）是计算机视觉领域中的一项重要基础技术，是图像理解中的重要一环。图像分割是将数字图像细分为多个图像子区域的过程，通过简化或改变图像的表示形式，让图像能够更加容易被理解。
   - 图像分割技术自 60 年代数字图像处理诞生开始便有了研究，随着近年来深度学习研究的逐步深入，图像分割技术也随之有了巨大的发展。
   - 早期的图像分割算法不能很好地分割一些具有抽象语义的目标，比如文字、动物、行人、车辆。这是因为早期的图像分割算法基于简单的像素值或一些低层的特征，如边缘、纹理等，人工设计的一些描述很难准确描述这些语义，这一经典问题被称之为“语义鸿沟”。
   - 第三代图像分割很好地避免了人工设计特征带来的“语义鸿沟”，从最初只能基于像素值以及低层特征进行分割，到现在能够完成一些根据高层语义的分割需求。
   - 参考：[基于深度学习的图像分割在高德的实践](https://yqh.aliyun.com/detail/15920?utm_content=g_1000154176)
   - ![](https://p1-tt-ipv6.byteimg.com/img/pgc-image/9811c9fff31a4fe282dbce591f7642b8~tplv-obj:745:306.image)


## 素描风格化

【2022-1-25】[5个方便好用的Python自动化脚本](https://www.toutiao.com/i7056585992664269344)

自动生成素描草图
- 这个脚本可以把彩色图片转化为铅笔素描草图，对人像、景色都有很好的效果。而且只需几行代码就可以一键生成，适合批量操作，非常的快捷。

第三方库：
- Opencv - 计算机视觉工具，可以实现多元化的图像视频处理，有Python接口


```python
""" Photo Sketching Using Python """
import cv2

img = cv2.imread("elon.jpg")
## Image to Gray Image
gray_image = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
## Gray Image to Inverted Gray Image
inverted_gray_image = 255-gray_image
## Blurring The Inverted Gray Image
blurred_inverted_gray_image = cv2.GaussianBlur(inverted_gray_image, (19,19),0)
## Inverting the blurred image
inverted_blurred_image = 255-blurred_inverted_gray_image
### Preparing Photo sketching
sketck = cv2.divide(gray_image, inverted_blurred_image,scale= 256.0)
cv2.imshow("Original Image",img)
cv2.imshow("Pencil Sketch", sketck)
cv2.waitKey(0)
```

素描草图：马斯克
- ![](https://p26.toutiaoimg.com/origin/tos-cn-i-qvj2lq49k0/8c5a6fdc6274477ca9e3b85990f6942f?from=pc)

## 图像动态化

[Live2D](https://www.live2d.com/en/download/cubism/)

Live2D是一种应用于电子游戏的绘图渲染技术，通过一系列的连续图像和人物建模来生成一种类似二维图像的三维模型。对于以动画风格为主的冒险游戏来说非常有用。该技术由日本Guyzware公司开发，Live2D的前身为TORA系统，衍生技术是OIU系统。
- 知乎：[如何看待live2D这项技术？](https://www.zhihu.com/question/28130936)

<video width="620" height="440" controls="controls" autoplay="autoplay">
  <source src="https://vdn.vzuu.com/SD/fc42fe58-2322-11eb-a20b-9a794694b530.mp4" type="video/mp4" />
</video>


## 图像3D化

- [2017-9-21]自拍照三维重建[3D Face Reconstruction from a Single Image](http://www.cs.nott.ac.uk/~psxasj/3dme/index.php)
- ![demo](https://cdn.vox-cdn.com/thumbor/fXbE0rbXW6WlcmtB1cKBiTsV1b0=/0x0:482x334/1820x1213/filters:focal(203x129:279x205):no_upscale()/cdn.vox-cdn.com/uploads/chorus_image/image/56734861/3d_mark_take_2.0.gif)
- 【2020-7-23】2D照片转3D的效果，代码：[3d-photo-inpainting](https://github.com/vt-vl-lab/3d-photo-inpainting)
- ![](https://p1-tt-ipv6.byteimg.com/img/pgc-image/54a7f500dc92415f91e0766e2f74c45a~tplv-obj:340:424.image?from=post)

- 【2020-11-18】端到端面部表情合成 Speech-Driven Animation [Github代码](https://github.com/DinoMan/speech-driven-animation)
   - ![](https://github.com/DinoMan/speech-driven-animation/raw/master/example.gif)
- 【2021-3-10】面部表情迁移：吴京+甄子丹 [微博示例](https://video.weibo.com/show?fid=1034:4609199536013325)
- 【2020-12-29】[单张图片三维重建](https://blog.csdn.net/zouxy09/article/details/8083553),Andrew Ng介绍他的两个学生用单幅图像去重构这个场景的三维模型。
   - [斯坦福大学](http://ai.stanford.edu/~asaxena/reconstruction3d/)
      - ![](http://ai.stanford.edu/~asaxena/reconstruction3d/Results/mountain_mesh_small.jpg)
   - [康奈尔大学](http://www.cs.cornell.edu/~asaxena/learningdepth/)


# OCR

- `光学字符识别`(`OCR`,Optical Character Recognition)是指对文本资料进行扫描，然后对图像文件进行分析处理，获取文字及版面信息的过程。OCR技术非常专业，一般多是印刷、打印行业的从业人员使用，可以快速的将纸质资料转换为电子资料。

## OCR工具


### 简易工具

- 【2021-7-26】免费在线OCR工具 [ocrmaker](http://ocrmaker.com/)
- [UU Tool](https://uutool.cn/ocr/)：截图黏贴图片到网站，提取文本；text转语音
- [城华OCR](https://zhcn.109876543210.com/)，将图片转成各种文档格式，限制次数
- [白描](https://web.baimiaoapp.com/image-to-excel)：提取图片中的文字、数学公式等


### 对比总结

【2022-1-25】kaggle笔记：各类OCR方法对比：[Keras-OCR vs EasyOCR vs PYTESSERACT](https://www.kaggle.com/odins0n/keras-ocr-vs-easyocr-vs-pytesseract)
- ![](https://www.kaggleusercontent.com/kf/72864633/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..bibMciKL4OvFX6946nkcFw.H2h7vNPLD0EzD2z6onWrw4R9VV561rtI-O7dAgl4zRkrjH216E96Cg8_ZO6-4Xny6XZL45qeH7xqBdHs7DWKpxJwl6PSES-c3wCDkH1pZifDsjEiIhboIFocwMjIEWDWNFlTY-gafig2CIc9OMmr8Kj2HyhJ_Xmg88Lbsa25dpCF2XkWG6DDLww1eL9wmXE66SzF7sM1_rsUxLvmAplprAQVNPOo2dVSKaGtD5Q1FOD8NvkeRPVeA-MiFGHe8bCtu0paeoX7aPC1z6WzunEsbpGjAeHOWrHXDtYZMPde_Qc77FVe2Qc91b6W_aAYgFoWuehxHKhOgp-jtcSA8cr_UocTj3chqBQgJKkwFodQdZInVeknz7L1HA9IGJgpWEy8DPZcNjhNuXgoWqpjqJLslljIJa-8N3Dy3qqu5p8Ku54YnzDSak2rMgdn_ThhC5AtDM3_7aB_s6vI4LoeVFxYTJ4JLVyw3v_YqIOe1BG7qD-QN2bqZixhJvtajOYzllcLP21NqMesuo7dHa-favmNVYo6o9zirwLvyYFrW4z0BpdBGkf_nQ_6n452u6GMiaRwmpNgNpD3zVv1BRCNbvMrJyzm5Mb7iqmedml2Yi6NFMxEgyOvb6rclteSyWU8_CMhP0bl3KGxEgeqNSD9Z02teSGWd9Gl8Nb6F9SByo90TtEZPJy7kIpa9Y9VPHwV7JAD.PtUtOX_gh2gJUMvxM9Wyvw/__results___files/__results___14_1.png)

CONCLUSIONS 对比结论
* **Keras-OCR** is image specific OCR tool. If text is inside the image and their fonts and colors are unorganized, Keras-ocr consumes time if used on CPU 
* **EasyOCR** is lightweight model which is giving a good performance for receipt or PDF conversion. It is giving more accurate results with organized texts like pdf files, receipts, bills. EasyOCR also performs well on noisy images 适合发票、pdf格式、噪声图片
* **Pytesseract** is performing well for **high-resolution** images. Certain morphological operations such as dilation, erosion, OTSU binarization can help increase pytesseract performance. It also provides better results on handwritten text as compared to EasyOCR 适合高分辨率图、手写字体
* All these results can be further improved by performing specific image operations.



### Tesseract

- Tesseract 的OCR引擎最先由HP实验室于1985年开始研发，至1995年时已经成为OCR业内最准确的三款识别引擎之一。
- Tesseract 目前已作为开源项目发布在Google Project，其最新版本3.0已经支持中文OCR，并提供了一个命令行工具。

安装：
- pip install pytesseract

调用代码

```python
import cv2                        # OpenCV: Computer vision and image manipulation package
import pytesseract                # python Tesseract: OCR in python
import matplotlib.pyplot as plt   # plotting
import numpy as np                # Numpy for arrays
from PIL import Image             # Pillow: helps us read remote images
import requests                   # Requests: to fetch remote URLs
from io import BytesIO            # Helps read remote images

def get_image(url):
  response = requests.get(url)
  img = Image.open(BytesIO(response.content))
  return img

img = get_image('https://github.com/jalammar/jalammar.github.io/raw/master/notebooks/cv/label.png')
# OCR结果
print(pytesseract.image_to_string(img))
```

### EasyOCR

- 【2020-8-7】[一个超好用的开源OCR](https://www.toutiao.com/i6858234401206043140/?tt_from=mobile_qq&utm_campaign=client_share&timestamp=1596809559&app=news_article&utm_source=mobile_qq&utm_medium=toutiao_android&use_new_style=1&req_id=20200807221239010147083076216022E3&group_id=6858234401206043140)：[EasyOCR](https://github.com/JaidedAI/EasyOCR)，目前能够支持58种语言，其中有中文(简体和繁体)、日语、泰语、韩语等
   - EasyOCR的模型主要分为两个，基于CRAFT的文字检测模型和基于ResNet+LSTM+CTC的识别模型
   - ![](http://p6-tt.byteimg.com/large/pgc-image/2402e44dff954e4985f6762de5b07ce6?from=pc)
   - 第三方基于easyOCR提供了几个demo地址，大家可以试试自己的数据看看效果：
      - [Demo1](https://colab.fan/easyocr)
      - [Demo2](https://hub.docker.com/r/challisa/easyocr)
      - [Demo3](https://easyocrgpu-wook-2.endpoint.ainize.ai/)
      - ![](http://p3-tt.byteimg.com/large/pgc-image/a56400ef928d419c8ef29c64abede5da?from=pc)

### 中文OCR比赛第一

【2022-1-25】[第一次比赛，拿了世界人工智能大赛 Top1 ！](https://blog.csdn.net/Datawhale/article/details/122613233)，“世界人工智能创新大赛”——手写体 OCR 识别竞赛（任务一），取得了Top1的成绩
- [赛题地址](http://ailab.aiwin.org.cn/competitions/65)
- 背景：银行日常业务中涉及到各类凭证的识别录入，例如身份证录入、支票录入、对账单录入等。以往的录入方式主要是以人工录入为主，效率较低，人力成本较高。近几年来，OCR相关技术以其自动执行、人为干预较少等特点正逐步替代传统的人工录入方式。但OCR技术在实际应用中也存在一些问题，在各类凭证字段的识别中，手写体由于其字体差异性大、字数不固定、语义关联性较低、凭证背景干扰等原因，导致OCR识别率准确率不高，需要大量人工校正，对日常的银行录入业务造成了一定的影响
- 数据集：原始手写体图像共分为三类，分别涉及银行名称、年月日、金额三大类，分别示意如下：
  - ![](https://img-blog.csdnimg.cn/img_convert/4cfda26453767dec3b2d436540d3c6b8.png)
- 相应图片切片中可能混杂有一定量的干扰信息
  - ![](https://img-blog.csdnimg.cn/img_convert/cd77146fdad3c8b41f455b2992a6b784.png)

OCR比赛最常用的模型是 CRNN + CTC，选择代码：Attention_ocr.pytorch-master.zip

模型改进：crnn的卷积部分类似VGG，我对模型的改进主要有一下几个方面：
- 1、加入激活函数Swish。
- 2、加入BatchNorm。
- 3、加入SE注意力机制。
- 4、适当加深模型。

```python
self.cnn = nn.Sequential(
   nn.Conv2d(nc, 64, 3, 1, 1), Swish(), nn.BatchNorm2d(64),
   nn.MaxPool2d(2, 2),  # 64x16x50
   nn.Conv2d(64, 128, 3, 1, 1), Swish(), nn.BatchNorm2d(128),
   nn.MaxPool2d(2, 2),  # 128x8x25
   nn.Conv2d(128, 256, 3, 1, 1), nn.BatchNorm2d(256), Swish(),  # 256x8x25
   nn.Conv2d(256, 256, 3, 1, 1), nn.BatchNorm2d(256), Swish(),  # 256x8x25
   SELayer(256, 16),
   nn.MaxPool2d((2, 2), (2, 1), (0, 1)),  # 256x4x25
   nn.Conv2d(256, 512, 3, 1, 1), nn.BatchNorm2d(512), Swish(),  # 512x4x25
   nn.Conv2d(512, 512, 1), nn.BatchNorm2d(512), Swish(),
   nn.Conv2d(512, 512, 3, 1, 1), nn.BatchNorm2d(512), Swish(),  # 512x4x25
   SELayer(512, 16),
   nn.MaxPool2d((2, 2), (2, 1), (0, 1)),  # 512x2x25
   nn.Conv2d(512, 512, 2, 1, 0), nn.BatchNorm2d(512), Swish()
)  # 512x1x25
# SE和Swish
class SELayer(nn.Module):
    def __init__(self, channel, reduction=16):
        super(SELayer, self).__init__()
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.fc = nn.Sequential(
            nn.Linear(channel, channel // reduction, bias=True),
            nn.LeakyReLU(inplace=True),
            nn.Linear(channel // reduction, channel, bias=True),
            nn.Sigmoid()
        )

    def forward(self, x):
        b, c, _, _ = x.size()
        y = self.avg_pool(x).view(b, c)
        y = self.fc(y).view(b, c, 1, 1)
        return x * y.expand_as(x)

class Swish(nn.Module):
    def forward(self, x):
        return x * torch.sigmoid(x)
```

## 验证码识别

- 验证码是一种区分用户是计算机还是人的公共全自动程序。可以防止：恶意破解密码、刷票、论坛灌水，有效防止某个黑客对某一个特定注册用户用特定程序暴力破解方式进行不断的登陆尝试，实际上用验证码是现在很多网站通行的方式。由于验证码可以由计算机生成并评判，但是必须只有人类才能解答，所以回答出问题的用户就可以被认为是人类。
- 目前的验证码通常的种类及特点如下：
   - （1）最基础的英文验证码：纯粹的英文与数字组合，白色背景，这是最容易实现OCR识别的验证码。
   - （2）字体变形的英文验证码：可以通过简单的机器学习实现对英文与数字的识别，准确率较高。
   - （3）加上扰乱背景线条的验证码：可以通过程序去除干扰线，准确率较高。
   - （4）中文验证码：中文由于字体多样，形状多变，数量组合众多，实现起来难度较大，准确率不高。
   - （5）中文字体变形验证码：准确率更低。
   - （6）中英文混合验证码：非常考验OCR程序的判断能力，基本上识别起来非常有难度。
   - （7）提问式验证码：这是需要OCR结合人工智能才能实现，目前基本上无法识别。
   - （8）GIF动态图验证码：由于GIF图片是动态图，无法定位哪一帧是验证码，所以难度很大。
   - （9）划动式验证码：虽然程序可以模拟人的操作，但是具体拖动到哪个位置很难处理。
   - （10）视频验证码：目前OCR识别还未实现。
   - （11）手机验证码：手机验证码实现自动化是很容易的，但是手机号码不那么容易获得。
   - （12）印象验证码：目前无解。

![](https://pic1.zhimg.com/80/v2-2b9748a5ca5498ba1955eec9a5b79db4_720w.jpg)

- 附录：
   - [利用Tesseract-OCR实现验证码识别](https://zhuanlan.zhihu.com/p/34530032)

- 「Happy Captcha」，一款易于使用的 Java 验证码软件包，旨在花最短的时间，最少的代码量，实现 Web 站点的验证码功能。
   - Captcha缩写含义：Completely Automated Public Turing test to tell Computers and Humans Apart
- 效果图
   - ![](https://pic3.zhimg.com/v2-971f594800cdd101950f916f92cb7b1e_b.webp)

## GAN方法

- 【2018-12-14】[基于GAN的验证码识别工具，0.5秒宣告验证码死刑！](https://baijiahao.baidu.com/s?id=1619803729564462538)
   - 中英两国研究人员联合开发了一套基于GAN的验证码AI识别系统，能在0.5秒之内识别出验证码，从 实际测试结果看，可以说宣布了对验证码的“死刑判决”。
      - ![](https://ss2.baidu.com/6ONYsjip0QIZ8tyhnq/it/u=280512761,907748494&fm=173&app=49&f=JPEG?w=640&h=273&s=0D30E51281D85DC04A55B0CB0000D0B3)
      - [论文地址](http://www.lancaster.ac.uk/staff/wangz3/publications/ccs18.pdf)，博文介绍：[An A.I. cracks the internet’s squiggly letter bot test in 0.5 seconds](https://www.digitaltrends.com/cool-tech/ai-cracks-captcha-05-seconds/)
   - 该系统已在不同的33个验证码系统中进行了成功测试，其中11个来自世界上最受欢迎的一些网站，包括eBay和维基百科等。
   - 这种方法的新颖之处在于：使用生成对抗网络（GAN）来创建训练数据。不需要收集和标记数以百万计的验证码文本数据，只需要500组数据就可以成功学习。而且可以使用这些数据，来生成数百万甚至数十亿的合成训练数据，建立高性能的图像分类器。
   - 结果显示，<font color='red'>该系统比迄今为止所见的任何验证码识别器系统的识别精度都高。</font>
   - ![](https://ss1.baidu.com/6ONXsjip0QIZ8tyhnq/it/u=1299396691,4195542946&fm=173&app=49&f=JPEG?w=640&h=416&s=A498E633795644CA4A6580DA0000C0B3)


# 图像风格化

[图像风格迁移(Neural Style)简史](https://zhuanlan.zhihu.com/p/26746283)

图像风格迁移科技树
- ![](https://pic4.zhimg.com/80/v2-526f16430324d3fbd8c07ff3d1c05c0b_hd.jpg)

## Demo

- 【2021-5-11】[案例解析：用Tensorflow和Pytorch计算骰子值](https://www.toutiao.com/i6622845628902801933/)
  - ![](https://p6-tt.byteimg.com/origin/pgc-image/557db089145144b5a7ae5195cf6d4aef?from=pc)，[github](https://github.com/sugi-chan/2-stage-dice-pipeline)
- 【2020-12-04】AI姿势传递模型，[论文地址](https://arxiv.org/pdf/2012.01158.pdf)，不愿意出节目的码农的年会神器？[将舞蹈化为己用-视频](https://weibo.com/tv/show/1034:4578074625245199?from=old_pc_videoshow)
- 【2020-12-02】【MaskDetection：滴滴开源的口罩检测模型】 by DiDi [GitHub](https://github.com/didi/maskdetection)
- 【2020-12-04】[孪生网络用于图片搜索](https://www.pyimagesearch.com/2020/11/30/siamese-networks-with-keras-tensorflow-and-deep-learning/)《Siamese networks with Keras, TensorFlow, and Deep Learning - PyImageSearch》by Adrian Rosebrock
   - ![](https://www.pyimagesearch.com/wp-content/uploads/2020/11/keras_siamese_networks_header.png)

### fast-style

- 图像风格迁移，[深度学习之风格迁移简介](http://melonteam.com/posts/shen_du_xue_xi_zhi_feng_ge_qian_yi_jian_jie/)
- ![](http://melonteam.com/image/shen_du_xue_xi_zhi_feng_ge_qian_yi_jian_jie/5489df3b2f1d117cbd275724697edda9ccadb0e92ba6d7c40dfb19c465378e01)
- Fast style transfer check [demo](https://wqw547243068.github.io/demo/fast-style/)

![alt text](https://raw.githubusercontent.com/zaidalyafeai/zaidalyafeai.github.io/master/images/fast-style.PNG)

### 风格化案例
- 【2019-07-19】[图像风格迁移示例](https://reiinakano.github.io/arbitrary-image-stylization-tfjs)

<iframe src="https://reiinakano.github.io/arbitrary-image-stylization-tfjs" scrolling="yes" border="0" frameborder="no" framespacing="0" allowfullscreen="true" width='800' height='600'> </iframe>

### Real Time style transfer 实时风格迁移

Real Time style transfer check [demo](https://wqw547243068.github.io/demo/RST/)

![alt text](https://raw.githubusercontent.com/zaidalyafeai/zaidalyafeai.github.io/master/images/rst.png)


## 扩散模型

【2022-8-31】苏剑林的[生成扩散模型漫谈](https://kexue.fm/archives/9119)
- 生成模型中，VAE、GAN“如雷贯耳”，还有一些比较小众的选择，如flow模型、VQ-VAE等，颇有人气，尤其是VQ-VAE及其变体VQ-GAN，近期已经逐渐发展到“图像的Tokenizer”的地位，用来直接调用NLP的各种预训练方法。
- 除此之外，还有一个本来更小众的选择——`扩散模型`（Diffusion Models）——正在生成模型领域“异军突起”，当前最先进的两个文本生成图像—— OpenAI 的 `DALL·E 2` 和 Google的`Imagen`，都是基于`扩散模型`来完成的。

生成扩散模型的大火，始于2020年所提出的[DDPM](https://arxiv.org/abs/2006.11239)（Denoising Diffusion Probabilistic Model），虽然也用了“**扩散模型**”这个名字，但事实上除了采样过程的形式有一定的相似之外，DDPM与传统基于`朗之万`方程采样的扩散模型完全不一样，一个新的起点、新的篇章。

### 扩散模型原理

标准的`扩散模型`（diffusion models）涉及到**图像变换**（添加高斯噪声）和**图像反转**。但是扩散模型的生成并不强烈依赖于图像降解的选择。通过实验证明了基于完全确定性的降解（例如模糊、masking 等），也可以轻松训练一个扩散生成模型。
- [项目地址](https://github.com/arpitbansal297/cold-diffusion-models)
- [论文地址](https://arxiv.org/abs/2208.09392)
这个工作成功地质疑了社区对扩散模型的理解：它并非依赖于**梯度郎之万动力学**（gradient Langevin dynamics）或**变分推理**（variational inference）。


准确来说，`DDPM`叫“**渐变模型**”更为准确一些，扩散模型这一名字反而容易造成理解上的误解，传统扩散模型的**能量模型**、**得分匹配**、`朗之万`方程等概念，其实跟DDPM及其后续变体都没什么关系。
- DDPM的数学框架其实在ICML2015的论文《Deep Unsupervised Learning using Nonequilibrium Thermodynamics》就已经完成了，但DDPM是首次将它在高分辨率图像生成上调试出来了，从而引导出了后面的火热。由此可见，一个模型的诞生和流行，往往还需要时间和机遇


## 应用案例

### 杂货店货架识别

- 【2021-1-23】[Grocery-Product-Detection](https://github.com/sayakpaul/Grocery-Product-Detection)
  - This repository builds a product detection model to recognize products from grocery shelf images. The dataset comes from [here](https://github.com/gulvarol/grocerydataset). Everything from data preparation to model training is done using [Colab Notebooks](https://colab.research.google.com/) so that no setup is required locally. All the relevant commentaries have been included inside the Colab Notebooks.

### 文本生成图片

【2022-8-16】[TikTok 乱拳打死老师傅：硅谷大厂还在发论文，它产品已经上线了](https://www.sohu.com/a/577300364_114819)
- 不少家互联网大厂都在试图测试、开发 AI 文字转图片技术，结果没想到，TikTok 却率先将这项技术应用到了产品里，在 AI 创作潮流中异军突起。
- TikTok 的特效菜单下，最近增加了一个名叫“AI 绿幕” (AI Greenscreen) 的新选项。
- 点击这个选项，然后在屏幕中间的对话框里输入一段文字描述，只用不到5秒的时间，TikTok 就可以根据文字描述生成一张竖版画作，用作短视频的背景：
- ![](https://p3.itc.cn/q_70/images03/20220816/5fdb55b70e054099a88ea6bc5bfeca09.png)
- 生成结果具有非常强的水彩/油画感觉，风格迁移 (style transfer) 的痕迹明显，而且用的颜色也都鲜亮明快，给人一种耳目一新的感受。
- ![](https://p4.itc.cn/q_70/images03/20220816/902b3d3e8e8d45e0aba3be1bdf1694e6.png)

#### Disco Diffusion


#### DALL·E


#### 文心-一格

- 【2022-8-23】[国产AI作画神器火了，更懂中文，竟然还能做周边](https://mp.weixin.qq.com/s/xh6Q0Pnv9OfP8Je3lDiyZg), “一句话生成画作”这个圈子里，又一个AI工具悄然火起来了,不是你以为的Disco Diffusion、DALL·E，再或者Imagen……而是全圈子都在讲中国话的那种, [文心·一格](https://yige.baidu.com/#/)
  - 操作界面上，Disco Diffusion开放的接口不能说很复杂，但确实有点门槛。它直接在谷歌Colab上运行，需要申请账号后使用（图片生成后保存在云盘），图像分辨率、尺寸需要手动输入，此外还有一些模型上的设置。好处是可更改的参数更多，对于高端玩家来说可操作性更强，只是比较适合专门研究AI算法的人群;相比之下，文心·一格的操作只需三个步骤：输入文字，鼠标选择风格&尺寸，点击生成。
  - 提示词，Disco Diffusion的设置还要更麻烦一些。除了描述画面的内容以外，包括画作类别和参考的艺术家风格也都得用提示词来设置，通常大伙儿会在其他文档中编辑好，再直接粘过来。相比之下文心·一格倒是没有格式要求，输入150字的句子或词组都可以
  - 性能要求上，Disco Diffusion是有GPU使用限制的，每天只能免费跑3小时。抱抱脸（HuggingFace）上部分AI文生图算法的Demo虽然操作简单些，但一旦网速不行，就容易加载不出来; 文心·一格除了使用高峰期以外，基本上都是2分钟就能生成，对使用设备也没有要求。
  - 总体来看，同样是文字生成图片AI，实际相比文心·一格的“真·一句话生成图片”，DALL·E和Disco Diffusion的生成过程都不太轻松。

看似“一句话生成图片”不难，其实对AI语义理解和图像生成能力提出了进一步要求。
- 为了能更好地理解文本、提升输出效果，文心·一格还在百度文心的图文生成跨模态模型ERNIE-VilG的基础上，进行了更详细的优化。
- 为了提升图文理解能力，在知识增强的基础上，引入跨模态多视角对比学习；
- 为了降低输入要求同时提升效果，采用基于知识的文本联想能力，让模型学会自己扩展提示词的细节和风格；
- 为了提升图像生成能力，采用渐进式扩散模型训练算法，让模型来选择效果最好的生成网络。

#### 自定义图片的text2image

【2022-9-7】[An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion](https://textual-inversion.github.io/)
- [github-textual_inversion](https://github.com/rinongal/textual_inversion)
- 基于潜在扩散模型（Latent Diffusion Models, LDM），允许用户使用自然语言指导 AI 生成包含特定独特概念的图像。
- 例如我想将心爱的宠物猫咪变成一幅独特的画作——抽象派猫猫，只需要提供3-5张照片，然后通过控制自然语言输入，来得到一个我家猫咪的抽象画作。
- 简单介绍下过程：首先，模型会通过学习这些图片，使用一些单词去表示图片。其次，这些单词可以组合成自然语言句子，通过 prompt 形式指导模型进行个性化创作。好处在于，图像的自然语言表示对用户非常友好。用户可以自由修改 prompt 内容以获取他们想要的风格、主题和独一无二的结果。
- We learn to generate **specific concepts**, like personal objects or artistic styles, by describing them using new "words" in the embedding space of pre-trained **text-to-image** models. These can be used in new sentences, just like any other word.
- Our work builds on the publicly available [Latent Diffusion Models](https://github.com/CompVis/latent-diffusion)
- ![](https://textual-inversion.github.io/static/images/editing/teaser.JPG)
- ![](https://textual-inversion.github.io/static/images/training/training.JPG)

#### 视频风格化

【2022-9-7】通过将预训练的语言图像模型（pretrained language-image models）调整为视频识别，以此将对比语言图像预训练方法（contrastive language-image pretraining）扩展到视频领域；
- 为了捕捉视频中帧沿时间维度的远程依赖性，提出了一个跨帧的注意力机制，明确了跨帧的信息交换。此外该模块非常轻量化，可以无缝插入预训练的语言图像模型。
- [项目地址](https://github.com/microsoft/videox)
- [论文地址](https://arxiv.org/abs/2208.02816)


## 资料

- 更多[Demo地址](http://wqw547243068.github.io/demo)

# 风格迁移简介

- [深度学习之风格迁移简介](http://melonteam.com/posts/shen_du_xue_xi_zhi_feng_ge_qian_yi_jian_jie/)

`风格迁移`（style transfer）最近两年非常火，可谓是深度学习领域很有创意的研究成果。它主要是通过神经网络，将一幅艺术风格画（style image）和一张普通的照片（content image）巧妙地融合，形成一张非常有意思的图片。

![](http://melonteam.com/image/shen_du_xue_xi_zhi_feng_ge_qian_yi_jian_jie/4a0dbd0ba7031a2b9e0f97d222d5050799764b92b7b135ffba3edfda4fd2feea)

因为新颖而有趣，自然成为了大家研究的焦点。目前已经有许多基于风格迁移的应用诞生了，如移动端风格画应用Prisma，手Q中也集成了不少的风格画滤镜：

![](http://melonteam.com/image/shen_du_xue_xi_zhi_feng_ge_qian_yi_jian_jie/de7624d2c5163daeb833b4a4a4d4bbbf340fbc2a8289763833f8de2608f89b9c)

本文将对风格迁移[1]的实现原理进行下简单介绍，然后介绍下它的快速版，即fast-style- transfer[2]。

## 1. 风格迁移开山之作

2015年，Gatys等人发表了文章[1]《A Neural Algorithm of Artistic Style》，首次使用深度学习进行艺术画风格学习。把风格图像Xs的绘画风格融入到内容图像Xc，得到一幅新的图像Xn。则新的图像Xn：即要保持内容图像Xc的原始图像内容（内容画是一部汽车，融合后应仍是一部汽车，不能变成摩托车），又要保持风格图像Xs的特有风格（比如纹理、色调、笔触等）。

### 1.1 内容损失（Content Loss）
在CNN网络中，一般认为较低层的特征描述了图像的具体视觉特征（即纹理、颜色等），较高层的特征则是较为抽象的图像内容描述。 所以要比较两幅图像的内容相似性，可以比较两幅图像在CNN网络中高层特征的相似性（欧式距离）。

![](http://melonteam.com/image/shen_du_xue_xi_zhi_feng_ge_qian_yi_jian_jie/880c6f8c21936bb4c29a2e6952ce357f7e844e7328c86f2a730f500768e66802)

1.2 风格损失（Style Loss）
而要比较两幅图像的风格相似性，则可以比较它们在CNN网络中较低层特征的相似性。不过值得注意的是，不能像内容相似性计算一样，简单的采用欧式距离度量，因为低层特征包含较多的图像局部特征（即空间信息过于显著），比如两幅风格相似但内容完全不同的图像，若直接计算它们的欧式距离，则可能会产生较大的误差，认为它们风格不相似。论文中使用了Gram矩阵，用于计算不同响应层之间的联系，即在保留低层特征的同时去除图像内容的影响，只比较风格的相似性。

![](http://melonteam.com/image/shen_du_xue_xi_zhi_feng_ge_qian_yi_jian_jie/bc72f7cd6be684f73c7c7e3649dbba4b030bb2607c66370104e043c71b2ac31c)

那么风格的相似性计算可以用如下公式表示：

![](http://melonteam.com/image/shen_du_xue_xi_zhi_feng_ge_qian_yi_jian_jie/1a7ce05010b913ae2c5f58ef362aa76638199c79293f493856feb80d99703476)

![](http://melonteam.com/image/shen_du_xue_xi_zhi_feng_ge_qian_yi_jian_jie/eff0adf1fd4b183cb872d79e2a5a70ca66d6d21845a59bbf6faf31012532be3a)

### 1.3 总损失（Total Loss）
这样对两幅图像进行“内容+风格”的相似度评价，可以采用如下的损失函数：

![](http://melonteam.com/image/shen_du_xue_xi_zhi_feng_ge_qian_yi_jian_jie/3d679c5b0a174e93a56eba66383e6abd57431c24e76805f0fdcf8d7caa3d89ef)

### 1.4 训练过程
文章使用了著名的VGG19网络[3]来进行训练（包含16个卷积层和5个池化层，但实际训练中未使用任何全连接层，并使用平均池化average- pooling替代最大池化max-pooling）。

![](http://melonteam.com/image/shen_du_xue_xi_zhi_feng_ge_qian_yi_jian_jie/3f981ef8277f3d56dbc0dfb8cb9fb38bbcf6c35914f8bc3e53fda29ac2eed6f6)

内容层和风格层的选择：将`内容图像`和`风格图像`分别输入到VGG19网络中，并将网络各个层的特征图（feature map）进行可视化（重构）。

内容重构五组对比实验：
- 1. conv1_1 (a)
- 2. conv2_1 (b)
- 3. conv3_1 (c)
- 4. conv4_1 (d)
- 5. conv5_1 (e)
风格重构五组对比实验：
- 1. conv1_1 (a)
- 2. conv1_1 and conv2_1 (b) 
- 3. conv1_1, conv2_1 and conv3_1 (c)
- 4. conv1_1, conv2_1, conv3_1 and conv4_1 (d)
- 5. conv1_1, conv2_1, conv3_1, conv4_1 and conv5_1 (e)
通过实验发现：对于内容重构，(d)和(e)较好地保留了图像的高阶内容（high-level content）而丢弃了过于细节的像素信息；对于风格重构，(e)则较好地描述了艺术画的风格。如下图红色方框标记：

![](http://melonteam.com/image/shen_du_xue_xi_zhi_feng_ge_qian_yi_jian_jie/44a2b217d7d007c6110d5248c90ccf0f66c6296f320954668d73c1af6b0d5fa7)

在实际实验中，内容层和风格层选择如下：
- 内容层：conv4_2
- 风格层：conv11, conv2_1, conv3__1_, _conv4_1, conv5_1
- 训练过程：以白噪声图像作为输入(x)到VGG19网络，conv4_2层的响应与原始内容图像计算出内容损失（Content Loss），“conv1_1, conv2_1, conv3_1, conv4_1, conv5_1”这5层的响应分别与风格图像计算出风格损失，然后它们相加得到总的风格损失（Style Loss），最后Content Loss + Style Loss = Total Loss得到总的损失。采用梯度下降的优化方法求解Total Loss函数的最小值，不断更新x，最终得到一幅“合成画”。

### 1.5 总结
每次训练迭代，更新的参数并非VGG19网络本身，而是随机初始化的输入x；
由于输入x是随机初始化的，最终得到的“合成画”会有差异；
每生成一幅“合成画”，都要重新训练一次，速度较慢，难以做到实时。
## 2. 快速风格迁移
2016年Johnson等人提出了一种更为快速的风格迁移方法[2]《[Perceptual losses for real-time style transfer and super- resolution](http://cs.stanford.edu/people/jcjohns/papers/eccv16/JohnsonECCV16.pdf)》。

### 2.1 网络结构
它们设计了一个变换网络（Image Transform Net），并用VGG16网络作为损失网络（Loss Net）。输入图像经由变换网络后，会得到一个输出，此输出与风格图像、内容图像分别输入到VGG16损失网络，类似于[1]的思路，使用VGG16不同层的响应结果计算出内容损失和风格损失，最终求得总损失。然后使用梯度下降的优化方法不断更新变换网络的参数。 
- 内容层：relu3_3
- 风格层：relu12, relu2_2, relu3_3, _relu4_3
其中变换网络（Image Transform Net）的具体结构如下图所示： 

![](http://melonteam.com/image/shen_du_xue_xi_zhi_feng_ge_qian_yi_jian_jie/bea3e1a19df5198b9a31f7d241361cb129f13e0d0d5f4f4e0d14439d6d0b8126)

### 2.2 跑个实验
Johnson等人将论文的代码实现在[github](https://github.com/jcjohnson/fast-neural-style)上进行了开源，包括了论文的复现版本，以及将“Batch-Normalization ”改进为“Instance Normalization”[[4](https://arxiv.org/pdf/1607.08022.pdf)]的版本。咱们可以按照他的说明，训练一个自己的风格化网络。我这里训练了一个“中国风”网络，运行效果如下： 

![](http://melonteam.com/image/shen_du_xue_xi_zhi_feng_ge_qian_yi_jian_jie/5489df3b2f1d117cbd275724697edda9ccadb0e92ba6d7c40dfb19c465378e01)
![](http://melonteam.com/image/shen_du_xue_xi_zhi_feng_ge_qian_yi_jian_jie/c628d678344dddaef81c122d33fcff1cd00d1d7f2b90834410492ae03bf005d4)

### 2.3 总结

网络训练一次即可，不像Gatys等人[1]的方法需要每次重新训练网络；
可以实现实时的风格化滤镜：在Titan X GPU上处理一张512x512的图片可以达到20FPS。下图为fast-style-transfer与Gatys等人[1]方法的运行速度比较，包括了不同的图像大小，以及Gatys方法不同的迭代次数。

![](http://melonteam.com/image/shen_du_xue_xi_zhi_feng_ge_qian_yi_jian_jie/66b64458ff003281762ca3b3da2a7ad3e769b6274259431e2dbd82f9fd5543dd)

3. 参考资料

- Gatys L A, Ecker A S, Bethge M. A neural algorithm of artistic style[J]. arXiv preprint arXiv:1508.06576, 2015.
- Johnson J, Alahi A, Fei-Fei L. Perceptual losses for real-time style transfer and super-resolution[C]//European Conference on Computer Vision. Springer International Publishing, 2016: 694-711.
- Simonyan K, Zisserman A. Very deep convolutional networks for large-scale image recognition[J]. arXiv preprint arXiv:1409.1556, 2014.
- Ulyanov D, Vedaldi A, Lempitsky V. Instance normalization: The missing ingredient for fast stylization[J]. arXiv preprint arXiv:1607.08022, 2016.
- [Fast Style Transfer(快速风格转移)](http://closure11.com/fast-style-transfer%E5%BF%AB%E9%80%9F%E9%A3%8E%E6%A0%BC%E8%BD%AC%E7%A7%BB/)
- [图像风格迁移(Neural Style)简史](https://zhuanlan.zhihu.com/p/26746283)

