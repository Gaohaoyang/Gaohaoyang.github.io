---
layout: post
title:  "论文阅读-Paper-Club"
date:   2020-06-11 10:48:00
categories: 论文
tags: 深度学习 自然语言处理 sota
excerpt: 最新技术咨询梳理，sota(the state of the art), 论文资料汇总
author: 鹤啸九天
mathjax: true
---

* content
{:toc}

# 最新技术

## 论文检索

- 【2020-6-11】阿里员工开发的[论文知识图谱](https://www.connectedpapers.com/)
  - ![](http://p1.pstatp.com/large/pgc-image/37054b2db9b64394a73feecfa9ad024d)
- ak47开发的[arxiv最新文章跟进工具](http://www.arxiv-sanity.com/)
- [ResearchGate](https://www.researchgate.net/)（论文引用高效关联）

## Sota

- [StateOfTheArt.ai](http://StateOfTheArt.ai)
- [Paperwithcode](https://paperswithcode.com)，[Browse State-of-the-Art](https://paperswithcode.com/sota)
- 【2020-7-5】快速找到最新AI技术的模型/代码/API】“[CatalyzeX: machine intelligence to catalyze your projects](https://www.catalyzex.com/)”

## NLP论文

- [NLP Progress](http://nlpprogress.com/english/dialogue.html)
- [nlp最新论文及代码](https://paperswithcode.com/search?q_meta=&q=generation+text)
- [100 Must-Read NLP Papers](http://masatohagiwara.net/100-nlp-papers/)
- 【按主题分类的自然语言处理文献大列表】[NLP Paper - natural language processing paper list](http://t.cn/A6Aia1D0)
- 清华NLP组文本生成方向：[Text Generation Reading List](https://github.com/THUNLP-MT/TG-Reading-List)
- [文本生成框架](https://tobiaslee.top/2019/08/31/TG-framework-notes/)：AllenNLP、FairSeq、OpenNMT、Texar、HuggingFace
- [中文公开聊天语料库](https://github.com/codemayq/chinese_chatbot_corpus)
- [Chatbot and Related Research Paper Notes with Images](https://github.com/ricsinaruto/Seq2seqChatbots/wiki/Chatbot-and-Related-Research-Paper-Notes-with-Images)

## 技术社区

- [AIMner学术头条](https://www.aminer.cn/)
- [PaperWeekly](https://www.paperweekly.site/home)（论文分享社区）


# 顶会信息

## 资讯

- [AI Conference Deadlines顶会截止时间](https://aideadlin.es/?sub=ML,NLP,RO,SP,DM)
- [Conference Ranks](http://www.conferenceranks.com/#)


## 录用论文

- 【2020-7-18】[2020 IJCAI](https://www.ijcai.org/Proceedings/2020/)，[ICML 2020](https://proceedings.icml.cc/book/2020)

# 研究

## 研究思路

### [好的想法从哪里来](https://www.aminer.cn/research_report/5de5cd5caf66005a44823119)

- 做过一些研究的同学会有感受，仅阅读自己研究方向的文献，新想法还是不会特别多。这是因为读到的都是该研究问题已经完成时的想法，它们本身无法启发新的想法。
- 如何产生新的想法呢？有三种可行的基本途径：
  - `实践法`。即在研究任务上实现已有最好的算法，通过分析实验结果，例如发现这些算法计算复杂度特别高、训练收敛特别慢，或者发现该算法的错误样例呈现明显的规律，都可以启发你改进已有算法的思路。现在很多自然语言处理任务的Leaderboard上的最新算法，就是通过分析错误样例来有针对性改进算法的 [1]。
  - `类比法`（迁移）。即将研究问题与其他任务建立类比联系，调研其他相似任务上最新的有效思想、算法或工具，通过合理的转换迁移，运用到当前的研究问题上来。例如，当初注意力机制在神经网络机器翻译中大获成功，当时主要是在词级别建立注意力，后来我们课题组的林衍凯和沈世奇提出建立句子级别的注意力解决关系抽取的远程监督训练数据的标注噪音问题 [2]，这就是一种类比的做法。
  - `组合法`。即将新的研究问题分解为若干已被较好解决的子问题，通过有机地组合这些子问题上的最好做法，建立对新的研究问题的解决方案。例如，我们提出的融合知识图谱的预训练语言模型，就是将BERT和TransE等已有算法融合起来建立的新模型 [3]。

- 与阅读论文、撰写论文、设计实验等环节相比，如何产生好的研究想法，是一个不太有章可循的环节，很难总结出固定的范式可供遵循。像小马过河，需要通过大量训练实践，来积累自己的研究经验。不过，对于初学者而言，仍然有几个简单可行的原则可以参考。
  - （1）一篇论文的可发表价值，取决于它**与已有最直接相关工作间的Delta**。我们大部分研究工作都是站在前人工作的基础上推进的。牛顿说：**如果说我看得比别人更远些，那是因为我站在巨人的肩膀上**。在我看来，评判一篇论文研究想法的价值，就是看它站在了哪个或哪些巨人的肩膀上，以及在此基础上又向上走了多远。反过来，在准备开始一项研究工作之前，在形成研究想法的时候，也许要首先明确准备站在哪个巨人的肩膀上，以及计划通过什么方式走得更远。与已有最直接相关工作之间的Delta，决定了这个研究想法的价值有多大。
  - （2）**兼顾摘果子和啃骨头**。人们一般把比较容易想到的研究想法，叫做Low Hanging Fruit（**低垂果实**）。低垂果实容易摘，但同时摘的人也多，选择摘果子就容易受到想法撞车的困扰。例如，2018年以BERT为首的预训练语言模型取得重大突破，2019年中就出现大量改进工作，其中以跨模态预训练模型为例，短短几个月里 [arxiv](http://arxiv.org)上挂出了超过六个来自不同团队的图像与文本融合的预训练模型。设身处地去想，进行跨模态预训练模型研究，就是一个比较容易想到的方向，你一定需要有预判能力，知道世界上肯定会有很多团队也同时开展这方面研究，这时你如果选择入场，就一定要做得更深入更有特色，有自己独特的贡献才行。相对而言，那些困难的问题，愿意碰的人就少，潜下心来啃硬骨头，也是不错的选择，当然同时就会面临做不出来的风险，或者做出来也得不到太多关注的风险。同学需要根据自身特点、经验和需求，兼顾摘果子和啃骨头两种类型的研究想法。
  - （3）注意多项研究工作的**主题连贯性**。同学的研究训练往往持续数年，需要注意前后多项研究工作的主题连贯性，保证内在逻辑统一。需要考虑，在个人简历上，在出国申请Personal Statement中，或者在各类评奖展示中，能够将这些研究成果汇总在一起，讲出自己开展这些研究工作的总目标、总设想。客观上讲，人工智能领域研究节奏很快，技术更新换代快，所以成果发表也倾向于小型化、短平快。我有商学院、社科的朋友，他们一项研究工作往往需要持续一年甚至数年以上；高性能计算、计算机网络方向的研究周期也相对较长。人工智能这种小步快跑的特点，决定了很多同学即使本科毕业时，也会有多篇论文发表，更不用说硕士生、博士生。在这种情况下，就格外需要在研究选题时，注意前后工作的连贯性和照应关系。几项研究工作放在一起，到底是互相割裂说不上话，还是在为一个统一的大目标而努力，格外反映研究的大局意识和布局能力。例如，下图是我们课题组涂存超博士2018年毕业时博士论文《面向社会计算的网络表示学习》的章节设置，整体来看就比《社会计算的若干重要问题研究》等没有内在关联的写法要更让人信服一些。当然，对于初学者而言，一开始就想清楚五年的研究计划，根本不可能。但想，还是不去想，结果还是不同的。
  - （4）注意**总结和把握研究动态和趋势，因时而动**。
    - 2019年在知乎上有这样一个问题：“<font color='blue'>2019年在NLP领域，资源有限的个人/团队能做哪些有价值有希望的工作？</font>” 
    - 我当时的回答如下：
      - 我感觉，产业界开始集团化搞的问题，说明其中主要的开放性难题已经被解决得差不多了，如语言识别、人脸识别等，在过去20年里面都陆续被广泛商业应用。看最近的BERT、GPT-2，我理解更多的是将深度学习对大规模数据拟合的能力发挥到极致，在深度学习技术路线基本成熟的前提下，大公司有强大计算能力支持，自然可以数据用得更多，模型做得更大，效果拟合更好。
      - 成熟高新技术进入商用竞争，就大致会符合摩尔定律的发展规律。现在BERT等训练看似遥不可及，但随着计算能力等因素的发展普及，说不定再过几年，人人都能轻易训练BERT和GPT-2，大家又会在同一个起跑线上，把目光转移到下一个挑战性难题上。
      - 所以不如提前考虑，**哪些问题是纯数据驱动技术无法解决的**。NLP和AI中的困难任务，如<font color='green'>常识和知识推理，复杂语境和跨模态理解，可解释智能</font>，都还没有可行的解决方案，我个人也不看好数据驱动方法能够彻底解决。更高层次的联想、创造、顿悟等认知能力，更是连边还没碰到。这些正是有远见的研究者们应该开始关注的方向。
  - 需要看到，不同时期的研究动态和趋势不同。把握这些动态和趋势，就能够做出研究社区感兴趣的成果。不然的话，即使研究成果没有变化，只是简单早几年或晚几年投稿，结果也会大不相同。例如，2013年word2vec发表，在2014-2016年之间开展词表示学习研究，就相对比较容易得到ACL、EMNLP等会议的录用；但到了2017-2018年，ACL等会议上的词表示学习的相关工作就比较少见了。

- 周志华：[做研究与写论文](https://zhuanlan.zhihu.com/p/98747105)

# 写作

- 【2020-7-4】【论文写作相关资源大列表】’Paper Writing' by wangdongdut [GitHub](https://github.com/wangdongdut/PaperWriting)


# 资料

- 更多[Demo地址](http://wqw547243068.github.io/demo)


# 结束


