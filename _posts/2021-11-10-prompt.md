---
layout: post
title:  提示学习 Prompt learning
date:   2021-11-10 19:58:00
categories: 深度学习 自然语言处理
tags: 自监督 prompt 思维链
excerpt: NLP新范式：Prompt（提示学习）
mathjax: true
permalink: /prompt
---

* content
{:toc}

# NLP新范式：Prompt


## 资料

- 【2023-2-16】清华开源的 [OpenPrompt](https://github.com/thunlp/OpenPrompt)：An Open-Source Framework for Prompt-learning. 包含prompt各个算法、模块的代码及示例
  - [PromptPapers](https://github.com/thunlp/PromptPapers)
  - CMU的Neubig老师的[Advanced NLP课程](http://www.phontron.com/class/anlp2021/schedule/prompting.html)
- 【2021-8-3】[Fine-tune之后的NLP新范式：Prompt越来越火，CMU华人博士后出了篇综述文章](https://blog.csdn.net/xixiaoyaoww/article/details/119363189)，CMU 博士后研究员刘鹏飞：近代自然语言处理技术发展的第四范式可能是预训练语言模型加持下的 Prompt Learning。
- 从 BERT 开始，**预训练+finetune** 已经成为了整个领域的常规范式。但是从 GPT-3 开始，一种新范式开始引起大家的关注并越来越流行：**prompting**。
- [论文地址](https://arxiv.org/pdf/2107.13586.pdf)，更多研究见：清华大学开源的论文列表 [thunlp/PromptPapers](https://github.com/thunlp/PromptPapers)
- ![img](https://img-blog.csdnimg.cn/img_convert/e538ffef7d05deaf84a5a66225c7f4bc.png)

prompt讲解
- [CMU](https://blender.cs.illinois.edu/course/fall22/lecture9.pdf)

<object type="application/pdf" data="https://blender.cs.illinois.edu/course/fall22/lecture9.pdf"
           id="review" style="width:100%;  height:800px; margin-top:0px;  margin-left:0px" >
</object>




## NLP 范式

全监督学习在 NLP 领域也非常重要。但是全监督的数据集对于学习高质量的模型来说是不充足的，<span style='color:red'>早期的 NLP 模型严重依赖特征工程</span>。
- 随着用于 NLP 任务的神经网络出现，使得特征学习与模型训练相结合，研究者将研究重点转向了**架构工程**，即通过设计一个网络架构能够学习数据特征。

各种模式的对比如下：

|模式paradigm|工程重心engineering|示例|任务关系task relation|
|---|---|---|---|
|①全监督（非神经网络）|特征|如单词，词性，句子长度等|分类、序列标注、语言模型（无监督）、生成|
|②全监督（神经网络）|结构|如卷积、循环、自注意力|同上|
|③pre-train与fine-tune|目标|掩码语言模型、NSP下一句预测|以语言模型为中心，含无监督训练|
|④pre-train、prompt与predict|提示|完形填空、前缀|语言模型为中心，含文本提示|

- ![img](https://img-blog.csdnimg.cn/img_convert/e7be4976c76f47cf54a95a7dcd2150b9.png)
- 传统的监督学习（不需要神经网络）
- 神经网络-监督学习：不同NLP任务需要单独训练
- pre-train + fine-tune：目前流行的范式，可以适应不同的场景任务
- pre-train + prompt + predict：模板prompt范式，可以适应不同的场景任务

【2023-2-12】[【NLP】Prompt Learning 超强入门教程](https://zhuanlan.zhihu.com/p/442486331), 刘鹏飞在[北京智源大会](https://event.baai.ac.cn/activities/172)上关于 Prompt 的分享

【2023-2-9】
- [【NLP】Prompt Learning 超强入门教程](https://zhuanlan.zhihu.com/p/442486331)
- [Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing](https://arxiv.org/pdf/2107.13586.pdf)

很长一段时间内，NLP任务采用的都是 `Pretrain` + `Fine-tuning`（Model Tuning）的解决方案，需要对于每个任务都重新 fine-tune 一个新的模型，且**不能共用**。
- 但是对于一个预训练的大语言模型来说，这就仿佛对于每个任务都进行了**定制化**，十分不高效。
- 是否存在一种方式，可以将预训练语言模型作为**电源**，不同的任务当作**电器**，仅需要根据不同的电器（任务），选择不同的**插座**，对于模型来说，即插入不同的任务特定的参数，就可以使得模型适配该下游任务。
- `Prompt Learning` 就是这个**适配器**，它能高效得进行预训练语言模型的使用。

这种方式大大地提升了预训练模型的**使用效率**，如下图：
- ![](https://pic2.zhimg.com/80/v2-ffa9e652a07961216d1ed260dfdea95d_1440w.webp)
*   左边是传统的 `Model Tuning` 的范式：对于不同的任务，都需要将整个预训练语言模型进行**精调**，每个任务都有自己的一整套参数。
*   右边是`Prompt Tuning`，对于不同任务，仅需要插入不同的 prompt参数，每个任务都单独训练 Prompt 参数，不训练预训练语言模型，这样子可以大大缩短训练时间，也极大的提升了模型的使用率。


### NLP四大范式

NLP四大范式
- 第一范式：**非神经网络时代的完全监督学习**（`特征工程`）。该阶段需要大量任务相关的训练数据，通过特征工程和算法，比较有代表的算法是朴素贝叶斯Naïve Bayes、支持向量机SVM、逻辑回归LR等；
- 第二范式：**基于神经网络的完全监督学习**（`架构工程`）。该阶段也需要大量任务相关的训练数据，通过深度学习方法，自动获取特征(表示学习)进行端到端分类学习；
- 第三范式：**预训练，精调范式**（`目标工程`）：该阶段是当前使用比较多的预训练+微调范式，通过预训练的方式(比如掩码语言模型Masked Language Model)来学习海量的语言学知识，然后下游使用少量的任务相关的数据对预训练模型进行微调即可完成相关任务；
- 第四范式：**预训练，提示，预测范式**（`Prompt工程`）：当前进入了Prompt Learning提示学习的新范式，使用Few shot或者Zero shot即可完成下游任务。
- ![img](https://pic3.zhimg.com/80/v2-c4227aa430fb3be39172a08274f0c5da_1440w.webp)

预训练+微调和Prompt Learning流程对比
- ![](https://pic1.zhimg.com/80/v2-bc80e4117e9cb014decdde51f5685180_1440w.webp)

### NLP 范式发展历史

| 年份	| NLP模型范式变化 |
|---|---|
| 2017年以前 |	传统机器学习模型、神经网络 |
| 2017-2019	| 预训练 + 微调（pre-train + fine-tune） |
| 2019-至今	| ”预训练，prompt和预测“（pre-train，prompt and predict）范式 |

NLP发展历史上的三种范式（时间顺序）
*   很久以前发展起来的`全监督学习` Fully Supervised Learning
  - Fully Supervised Learning，即仅在目标任务的输入输出样本数据集上训练特定任务模型，长期以来在许多机器学习任务中发挥着核心作用，同样的，全监督学习在 NLP 领域也非常重要。
  - ![基于神经网络的监督学习](https://pic4.zhimg.com/80/v2-82767e969a98bc63c647374a049f6f17_1440w.webp)
  - 但是全监督数据集对于学习高质量的模型来说是不充足的，**早期的 NLP 模型严重依赖特征工程**。随着用于 NLP 任务的神经网络出现，使得特征学习与模型训练相结合，研究者将研究重点转向了架构工程，即通过设计一个网络架构能够学习数据特征。
  - ![基于神经网络的监督学习](https://pic4.zhimg.com/80/v2-82767e969a98bc63c647374a049f6f17_1440w.webp)
*   前三年火爆的`预训练+微调`  Pre-train, Fine-tune
  - 然而，从 2017-2019 年开始，NLP 模型发生了翻天覆地的变化，这种`全监督范式`发挥的作用越来越小。具体而言，研究重点开始转向预训练、Fine-tuning范式。一个具有固定架构的模型通过**预训练**作为`语言模型`（LM），用来预测观测到的文本数据的概率。
  - ![预训练+微调](https://pic1.zhimg.com/80/v2-9b02ff7acb3ac010d683c76dd53a4c14_1440w.webp)
*   最新的 `预训练+提示+预测 `  Pre-train, Prompt, Predict  
  - 当前正处于第二次巨变中，「预训练、Fine-tuning」过程被「预训练、prompt 和预测」过程所取代。
  - 在这种范式中，不是通过目标工程使预训练的语言模型（LM）适应下游任务，而是**重新形式化**（Reformulate）下游任务，使其看起来更像是在文本 prompt 的帮助下在原始 LM 训练期间解决的任务。
  - 通过这种方式，选择适当的 prompt，该方法可以操纵模型的行为，以便预训练的 LM 本身可以用于预测所需的输出，有时甚至无需任何额外的特定任务训练。
  - 优点是给定一组合适的 prompt，以**完全无监督**方式训练的单个 LM 就能够用于解决大量任务。
  - 然而该方法也存在一个问题 —— 这种方法引入了 **prompt 挖掘工程**的必要性，即需要找出最合适的 prompt 来让 LM 解决面临的问题

Prompt刚刚出现时，还没有被叫做Prompt，是研究者们为了下游任务设计出来的一种输入**形式**或**模板**，它能够帮助PLM“回忆”起自己在预训练时“学习”到的东西，因此后来慢慢地被叫做Prompt了。
- ![prompt](https://pic1.zhimg.com/80/v2-f152c022e31126bd8520e453899aaadc_1440w.webp)

不同于fine-tuning方法，prompt 范式需要给出一个定义好的模板，这个模板可以是离散的或者是连续的，来提醒模型在预训练的时候学习的知识。这是因为预训练的任务和下游任务往往差别较大，模型可能会存在特定性遗忘。

## prompt 介绍

什么是 `Prompt`, Prompt 就是 **提示**：  
- 有人忘记了某个事情，给予特定提示，他就可以想起来
- **白日依山尽**, 大家自然而然地会想起来下一句诗：**黄河入海流**。
- 搜索引擎，可以根据输入，进行输出提示：
  - ![](https://pic3.zhimg.com/80/v2-81d4c4b774e7910935d97fcde34b3842_1440w.webp)
 
文本情感分类：
- “I missed the bus today.” 句子后紧跟着给出这样一个prompt：“I felt so _____”
- “English: I missed the bus today. French: ______”

PLM将自动补充单词sad，或使用法语来进行填空。

### prompt演变过程

【2023-2-16】[一个小白如何学好prompt tuning?](https://www.zhihu.com/question/509079916/answer/2757278140)

任务：
- 对描述的商品类型进行分类
- 第一句需要被分类到「水果」类别中；
- 第二句则需要分类到「电脑」类别中。

标注数据集：

```sh
什么苹果啊，都没有苹果味，怪怪的味道，而且一点都不甜，超级难吃！  1
这破笔记本速度太慢了，卡的不要不要的。    0
```

#### （1）早期分类方法

直觉上分类方式：
- 将该问题建模成一个**传统文本分类**任务，通过人工标注，为每一个类别设置一个id，例如：

```json
{
    '电脑': 0,
    '水果': 1,
    ....
}
```

这种方法可行，但需要「较多的标注数据」才能取得不错的效果。

#### （2）BERT 预训练+分类

由于大多数**预训练模型**（如BRET）在 pretrain 的时候都使用了 \[MASK\] token 做 MLM 任务，而实际下游任务中不会使用到 \[MASK\] 这个 token，这就意味着今天训练下游任务时需要较多的数据集去抹平上下游任务不一致的 gap。

#### （3）Prompt

如果没有足够多的训练数据呢？
- prompt learning 就是为了解决这一问题，它将 \[MASK\] 的 token 引入到了下游任务中，将下游任务构造成和 MLM 类似的任务。

上述评论改写为：

```sh
这是一条[MASK][MASK]评论：这破笔记本速度太慢了，卡的不要不要的。
```

然后让模型去预测两个 \[MASK\] token 的真实值是什么，那模型根据**上下文**能推测出被掩码住的词应该为「电脑」。由于下游任务中也使用了和预训练任务中同样的 MLM 任务，这样就可以使用更少的训练数据来进行微调了。

#### （4）p-tuning

但，这还不是 `P-tuning`。

可见，构建句子最关键的部分是在于 **prompt生成**，即：

```sh
「这是一条[MASK][MASK]评论：」(prompt) + 这破笔记本速度太慢了，卡的不要不要的。(content)
```

被括号括起来的前缀（prompt）的生成非常重要，不同 prompt 会极大影响模型对 \[MASK\] 预测的正确率。

那么这个 prompt 怎么生成呢？
- 当然可以通过**人工去设计**很多不同类型的前缀 prompt，即 `prompt pattern`，例如：

```sh
这是一条[MASK][MASK]评论：
下面是一条描述[MASK][MASK]的评论：
[MASK][MASK]：
...
```

但是人工列这种 `prompt pattern` 非常麻烦，不同数据集所需要的 prompt pattern 也不同，可复用性很低。

那么，我们能不能通过机器自己去学习 prompt pattern 呢？可以，`P-tuning`。

作者：[何枝](https://www.zhihu.com/question/509079916/answer/2757278140)

### 什么是 prompt？

NLP中 `Prompt` 代表的是什么呢？
*   prompt 就是给 预训练语言模型 的一个**线索**/**提示**，更好的理解 人类的问题。

下图的 BERT/BART/ERNIE 均为预训练语言模型，对于人类提出的问题以及线索，预训练语言模型可以给出正确的答案。
- ![](https://pic4.zhimg.com/80/v2-f09af919f520b65363e38d8340ac4e4f_1440w.webp)
*   根据提示，BERT能回答，JDK 是 _Oracle_ 研发的
*   根据 `TL;DR:` 的提示，BART知道人类想要问的是文章的摘要
*   根据提示，ERNIE 知道人类想要问鸟类的能力--飞行

Prompt 更严谨的定义如下：
> - Prompt is the technique of making better use of the knowledge from the pre-trained model by adding additional texts to the input.  
> - Prompt 是一种为了更好的使用预训练语言模型的知识，采用在输入段添加**额外文本**的技术。
 
*   目的：更好挖掘预训练语言模型的能力
*   手段：在输入端添加文本，即重新定义任务（task reformulation）

### Prompt 概要

该综述研究试图通过提供 prompting 方法的概述和形式化定义，以及使用这些 prompt 的预训练语言模型的概述，来梳理这一迅速发展领域的当前知识状态。然后该论文对 prompt 方法进行了深入的讨论，包括 **prompt工程**、**answer工程**等基础和**多prompt学习**方法、**prompt相关的训练方法**等更高级的概念。

然后，该研究列出了已有的基于 prompt 学习方法的多种应用，并探讨了不同应用场景中如何选择合适的训练方法。最后，该研究尝试在研究生态系统中定位 prompt 方法的当前状态，并与其他研究领域建立联系。此外，该研究提出一些可能适合进一步研究的挑战性问题，并针对当前研究趋势进行了分析。

基于 Prompt 的学习方法试图通过**学习LM**来规避这一问题，该 LM 对文本 x 本身的概率 P(x;θ) 进行建模并使用该概率来预测 y，从而减少或消除了训练模型对大型监督数据集的需求。

最基本的 Prompt 形式的数学描述，包含许多有关 Prompt 的工作，并且可以扩展到其他内容。

基础 Prompt 分三步预测得分最高的 ^y，即：**prompt 添加**、**answer 搜索**和 **answer 映射**。prompting 方法的术语和符号。
- ![img](https://img-blog.csdnimg.cn/img_convert/c441c670a31e4b2669da835842b9f171.png)
不同任务的输入、模板和 answer 示例：
- ![img](https://img-blog.csdnimg.cn/img_convert/90e84a4ad6cf465e3e306fc376581bcf.png)

### Prompt设计思路

Prompting 设计考虑：
- 预训练模型选择：有许多预训练 LM 可以用来计算 P(x; θ)。在第 3 章中，研究者对预训练 LM 进行了初步的介绍；
- **Prompt 工程**：如果 prompt 指定了任务，那么选择正确的 prompt 不仅对准确率影响很大，而且对模型首先执行的任务也有很大影响。在第 4 章中，研究者讨论了应该选择哪个 prompt 作为 f_prompt(x) 方法；
- **Answer 工程**：根据任务的不同，会有不同的方式设计 Z (Answer)，可能会和映射函数一起使用。在第 5 章中，详细介绍了不同的设计方式；
- **扩展范式**：如上所述， 上面的公式仅仅代表了各种底层框架中最简单的一种，这些框架已经被提议用于执行各种 prompting。在 第 6 章中，研究者讨论了扩展这种基本范式以进一步提高结果或适用性的方法；
- 基于 Prompt 的**训练策略**：在第 7 章中，研究者总结了不同的训练策略并详细说明它们的相对优势
- ![img](https://img-blog.csdnimg.cn/img_convert/eae05d97522535d2a976353daae592c2.png)

## Prompt 工作流

清华：[OpenPrompt](https://github.com/thunlp/OpenPrompt)
- ![prompt](https://camo.githubusercontent.com/d26f498b1bc63fe804fe21df9d1f91fa55673ee3c45242fd3f49439718376db7/68747470733a2f2f7a332e617831782e636f6d2f323032312f31312f30332f4941645433442e706e67)

Prompt 的工作流包含以下4部分：
1.  Prompt **模版**（Template）构造*   
2.  Prompt **答案空间映射**（Verbalizer）的构造    
3.  文本代入template，并且使用预训练语言模型进行**预测**    
4.  将预测结果**映射回**label。
 
具体的步骤如下图，拆解分析。
-  ![workflow](https://pic4.zhimg.com/80/v2-65b25d4895d4d7b81d747282cdb4c7f3_1440w.webp)
 
### Step 1: prompt construction【Template】

首先构建一个模版Template，作用是将输入和输出进行**重新构造**，变成一个新的带有mask slots的文本，具体如下：
*   定义一个模版，包含了2处代填入的slots：\[x\] 和 \[z\]
*   将\[x\] 用输入文本代入

例如：
*   输入：<span style='color:green'>x = 我喜欢这个电影</span>。
*   模版：<span style='color:blue'>\[x\]总而言之，它是一个\[z\]电影</span>。
*   代入（prompting）：<span style='color:green'>我喜欢这个电影。总而言之，它是一个\[z\]电影</span>。
- ![](https://pic2.zhimg.com/80/v2-e6c4edefdb2498229dfa37f9fc883f15_1440w.webp)
 
### Step 2: answer construction【Verbalizer】
 
对于构造的prompt，要知道预测词和label 之间的关系，并且不可能运行z(任意词)，就需要一个`映射函数`（mapping function）将输出的词与label进行映射。
- 输出的label 有两个，一个是 😄，一个是 😭，可以限定，如果预测词是`fantastic` 则对应 😄，如果是 `boring` 则对应 😭.
- ![](https://pic3.zhimg.com/80/v2-6c3ab4435a08d559c69d2b46b18a5d1e_1440w.webp)
- ![](https://pic4.zhimg.com/80/v2-4708199326266b548a6b3e0361b1bb47_1440w.webp)
 
### Step 3: answer prediction【Prediction】
 
只需要选择[合适的预训练语言模型](https://huggingface.co/docs/transformers/model_summary)，然后进行 mask slots \[z\] 的预测。例如下图，得到了结果 `fantastic`, 将其代入\[z\] 中。
- ![](https://pic2.zhimg.com/80/v2-c12486224648f205c3c8199101a06b75_1440w.webp)

### Step 4: answer-label mapping【Mapping】
 
第四步骤，对于得到的 `answer`，用 `Verbalizer` 将其映射回原本的label。
- 例如：fantastic 映射回 label：
- ![](https://pic2.zhimg.com/80/v2-b9a60cb83f1c6772490053801d13885d_1440w.webp)
 
### 总结

- ![](https://pic1.zhimg.com/80/v2-45666504e1714ef274be6ed35a86d388_1440w.webp)
 
## Prompt-based 方法的工程选择问题

在知乎中有个提问：
> 现代的deep learning 就是为了规避 feature engineering，可是prompt 这边选择了 template 和 answer 不还是 feature engineering吗？

确实, 如果使用 BERT 的 fine-tuning 范式（下图左），不需要使用任何的人工特征构造，而使用 prompt-based 的方法的话，需要人工参与的部分包含了以下部分：
*   template 构造
*   answer 构造
*   预训练模型选择
*   prompt 的组合问题选择
*   以及训练策略的选择等
- ![](https://pic3.zhimg.com/80/v2-011fccce5f1d7367c243def5d3da4e32_1440w.webp)
 
会先进行每个需要人工engineering 的部分进行详细讲解，然后再分析为什么我们还需要prompt 这种范式。
 
### Prompt Template Engineering（Prompt模版工程）
 
如何构造合适的Prompt 模版？对于同一个任务，不同的人可能构造不同的Template。
- ![](https://pic3.zhimg.com/80/v2-6b1eef9478acd01687934bcaa07a3d0a_1440w.webp)

且每个模版都具有合理性。Tempalte的选择，对于Prompt任务起到了很重大的作用，就算一个word的区别，也坑导致10几个点的效果差别，论文[GPT Understands, Too](https://arxiv.org/abs/2103.10385) 给出了如下的结果：
- ![](https://pic1.zhimg.com/80/v2-b98e6f18abfad252f96b04a549cc9898_1440w.webp)
 
对于不同的template，可以从以下两种角度进行区分：
1.  根据**slot 的形状/位置**区分
  *   1.1 `完形填空`（Cloze）的模式，即未知的slot在template的中间等不定的位置
  *   1.2 `前缀模式`（Prefix），未知的slot在template的开头
1.  根据是否**由人指定**的来区分
  *   2.1 `人工指定` template
  *   2.2 `自动搜索` template
  *   2.3 `Discrete` 离散Template，即搜索的空间是离散的，为预训练语言模型的字典里的字符。
  *   2.4 `Continuous` 连续Template，即搜索的空间是连续的，因为所有新增的这些prompt的参数主要是为了让机器更好地服务于任务，所以其参数的取值空间不需要限定在特定的取值范围内，可以是连续的空间。

具体的思维导图如下：
- ![](https://pic3.zhimg.com/80/v2-c96d89a06ca2ec58e31a7cd2b4f30e7e_1440w.webp)

#### P-Tuning 自动生成模板

[详解当前大火的提示学习prompt learning](https://zhuanlan.zhihu.com/p/594767132)

自动学习模板可以分为`离散`（Discrete Prompts）和`连续`（Continuous Prompts）两大类。
- `离散`主要包括 Prompt Mining, Prompt Paraphrasing, Gradient-based Search, Prompt Generation 和 Prompt Scoring；
- `连续`则主要包括Prefix Tuning, Tuning Initialized with Discrete Prompts 和 Hard-Soft Prompt Hybrid Tuning

作者：[何枝](https://www.zhihu.com/question/509079916/answer/2757278140)

人工构建模板对人类来讲是合理的，但是在机器眼中，prompt pattern 长成什么样真的关键吗？
- 机器对自然语言的理解和人类对理解大不相同，曾经有做一个 model attention 和人类对语言重要性的理解的[对比实验](https://www.zhihu.com/zvideo/1569770114018127872)，发现机器对语言的理解和人类是存在一定的偏差的。

那是不是不用特意为模型去设定一堆我们觉得「合理」的 prompt pattern，而是让模型自己去找「合理」的prompt pattern 呢？

p-tuning
- 论文：[GPT Understands, Too](https://arxiv.org/pdf/2103.10385.pdf)
- 代码：[p-tuning](https://github.com/HarderThenHarder/transformers_tasks/tree/main/prompt_tasks/p-tuning)

P-Tuning 的训练一共分为：prompt token(s) 生成、mask label 生成、mlm loss 计算 三个步骤。
- （1）prompt token(s) 生成
- （2）mask label 生成
- （3）mlm loss 计算

（1）prompt token(s) 生成

随便生成一个模板扔给模型
- 选用中文 BERT 作为 backbone 模型，选用 vocab.txt 中的 \[unused\] token 作为构成 prompt 模板的**元素**。
- \[unused\] 是 BERT 词表里预留出来的未使用的 token，本身没有什么含义，随意组合也不会产生很大的语义影响，这也是使用它来构建 prompt 模板的原因。

那么，构建出来的 prompt pattern 就长这样：

```sh
[unused1][unused2][unused3][unused4][unused5][unused6]
```

（2）mask label 生成

完成 prompt 模板构建后，还需要把 mask label 给加到句子中，好让模型完成标签预测任务。

设定 label 的长度为2（'水果'、'电脑'，都是 2 个字的长度），并将 label 塞到句子的开头位置：

```sh
[CLS][MASK][MASK]这破笔记本速度太慢了，卡的不要不要的。[SEP]
```

其中 \[MASK\] token 就是需要模型预测的标签 token，现在把两个部分拼起来：

```
[unused1][unused2][unused3][unused4][unused5][unused6][CLS][MASK][MASK]这破笔记本速度太慢了，卡的不要不要的。[SEP]
```

这就是最终输入给模型的样本。


（3）mlm loss 计算

开始进行模型微调，喂给模型这样的数据：

```
[unused1][unused2][unused3][unused4][unused5][unused6][CLS][MASK][MASK]这破笔记本速度太慢了，卡的不要不要的。[SEP]
```

并获得模型预测 \[MASK\] token 的预测结果，并计算和真实标签之间的 CrossEntropy Loss。

P-Tuning 中标签数据长这样：

```
水果    什么苹果啊，都没有苹果味，怪怪的味道，而且一点都不甜，超级难吃！
电脑    这破笔记本速度太慢了，卡的不要不要的。
...
```

也就是说，需要计算的是模型对 \[MASK\] token 的输出与「电脑」这两个标签 token 之间的 CrossEntropy Loss，以教会模型在这样的上下文中，被 \[MASK\] 住的标签应该被还原成「物品类别」。

实验

选用 63 条评论（8 个类别）的评论作为训练数据，在 417 条评论上作分类测试，模型 F1 能收敛在 76%。
- 通过实验结果我们可以看到，基于 prompt 的方式即使在训练样本数较小的情况下模型也能取得较为不错的效果。
- 相比于传统的分类方式，P-Tuning 能够更好的缓解模型在小样本数据下的过拟合，从而拥有更好的鲁棒性。


### Answer Engineering（答案工程）
 
在给定一个任务或者Prompt，如何对 label 空间 和 answer 空间进行映射？
- ![](https://pic2.zhimg.com/80/v2-3df9747e3a96385804fce424e5c7b619_1440w.webp)
 
在上图，label 空间 Y 是: Positive, Negative, 答案空间 Z 可以是表示positive或者negative 的词，例如: Interesting/Fantastic/Happy/Boring/1-Star/Bad，具体的答案空间 Z 的选择范围可以由我们指定。可以指定一个 y 对应1-N个字符/词。
- ![](https://pic4.zhimg.com/80/v2-acf52745920aae345e6c5ac9ee20e92b_1440w.webp)
 
具体的答案空间的选择可以有以下三个分类标注：
1.  根据**形状**
  *   1.1 Token 类型
  *   1.2 Span 类型
  *   1.3 Sentence 类型
1.  是否**有界**
  *   2.1 有界
  *   2.2 无界
1.  是否**人工选择**
  *   3.1 人工选择
  *   3.2 自动搜素
  *   3.2.1 离散空间
  *   3.2.2 连续空间

具体的思维导图如下：
- ![](https://pic4.zhimg.com/80/v2-9e58677317b0576537d58fa669c6fc5b_1440w.webp)
 
### Pre-trained Model Choice（预训练模型选择）

在定义完模版以及答案空间后，选择合适的预训练语言模型对 prompt 进行预测，如何选择一个合适的预训练语言模型需要人工经验判别的。

具体的预训练语言模型可以分为如下5类，具体参考：[Huggingface Summary of the models](https://huggingface.co/docs/transformers/model_summary)
*   autoregressive-models: `自回归模型`，主要代表有 `GPT`，主要用于生成任务
*   autoencoding-models: `自编码模型`，主要代表有 `BERT`，主要用于NLU任务
*   seq-to-seq-models：`序列到序列任务`，包含了an encoder 和 a decoder，主要代表有 `BART`，主要用于基于条件的生成任务，例如翻译，summary等
*   multimodal-models：`多模态模型`
*   retrieval-based-models：`基于召回的模型`，主要用于开放域问答

基于此，例如下图想要做summary 任务，我们可以选择更合适的 BART 模型。
- ![](https://pic3.zhimg.com/80/v2-f7cf71aaa125547394fc53faadc0e7e6_1440w.webp)
 
其他分类标准也可参考：
- ![](https://pic3.zhimg.com/80/v2-654bc36f0f684b17b97f1966fe5904aa_1440w.webp)
- ![](https://pic2.zhimg.com/80/v2-f8e5c095bf30cd97176098cd725974e1_1440w.webp)
 
### Expanding the Paradigm（范式拓展）

如何对已有的 Prompt 进行任务增强以及拓展，具体可以从以下几个方面进行探讨：
*   Prompt Ensemble：Prompt 集成，采用多种方式询问同一个问题
  - ![](https://pic2.zhimg.com/80/v2-0143f03d078e4c78231b987dd041752d_1440w.webp)
*   Prompt Augmentation：Prompt 增强，采用类似的 prompt 提示进行增强
  - ![](https://pic1.zhimg.com/80/v2-04b7abb8bac6d642808f95c3eaeef088_1440w.webp)
*   Prompt Composition：Prompt 组合，例如将一个任务，拆成多个任务的组合，比如判别两个实体之间是否是父子关系，首先对于每个实体，先用Prompt 判别是人物，再进行实体关系的预测。
  - ![](https://pic3.zhimg.com/80/v2-f3fd7d887bcec01d18e410efe74c36aa_1440w.webp)
*   Prompt Decomposition：  
  - 将一个prompt 拆分成多个prompt
  - ![](https://pic1.zhimg.com/80/v2-a13127a5cd6927bcbff1f278b6c32c28_1440w.webp)
 
具体的思维导图如下：
- ![](https://pic1.zhimg.com/80/v2-62bb4b0d499ab4ba4eb57fc27304ec00_1440w.webp)
 
## Prompt-based Training Strategies（训练策略选择）

Prompt-based 模型在训练中，有多种训练策略，可以选择哪些模型部分训练，哪些不训练。
 
可以根据训练数据的多少分为：
*   `Zero-shot`: 对于下游任务，没有任何训练数据
*   `Few-shot`: 对于下游任务只有很少的训练数据，例如100条
*   `Full-data`: 有很多的训练数据，例如1万多条数据
 
也可以根据不同的参数更新的部分，对于prompt-based 的模型，主要分为两大块  
- 一个是预训练模型，一个是 Prompts 参数。  

这两个部分，都可以独立选择参数训练选择。  

对于
*   预训练语言模型，可以选择精调，或者不训练
*   对于prompts：
*   可以是没有prompts
*   固定的离散字符 prompts。（无参数）
*   使用训练好的 prompts参数，不再训练。
*   继续训练 prompts参数
- ![](https://pic3.zhimg.com/80/v2-edcf6508177774d990a83a9867bc96de_1440w.webp)
 
这些训练策略均可以两两组合，下面举例说明：
 
### 策略分类

*   **Promptless** Fine-tuning
  - 如果只有预训练语言模型，没有prompts，然后fine-tuning，即是bert 的常规使用。
  - ![](https://pic3.zhimg.com/80/v2-eb301f6533d25e15a4b980581d4c736e_1440w.webp)
*   **Fixed-Prompt** Tuning  
  - 如果使用精调预训练语言模型+离散的固定prompts，就是 BERT + Discrete Prompt for text classification
  - ![](https://pic1.zhimg.com/80/v2-f9deac3450d2f4af12cb96fc42784fc8_1440w.webp)
  - 如果使用精调预训练语言模型+连续训练好的固定prompts，就是 BERT + Transferred Continuous Prompt for text classification
-  ![](https://pic1.zhimg.com/80/v2-ae562fd4a0b05f4a3d87247f64b68e88_1440w.webp)
*   **Prompt+LM** Fine-tuning
  - 如果使用精调预训练语言模型+可训练的prompts，就是 BERT + Continuous Prompt for text classification
  - ![](https://pic3.zhimg.com/80/v2-0e47867fb32772bbe94974f4b2a37ff6_1440w.webp)
*   **Adapter** Tuning
  - 如果使用固定预训练语言模型无prompt，只是插入task-specific模块到预训练语言模型中，就是BERT + Adapter for text classification
  - ![](https://pic2.zhimg.com/80/v2-bf82c965a4d2abf020356ff70401ee85_1440w.webp)
*   **Tuning-free** Prompting
  - 如果使用固定预训练语言模型和离散固定的prompt，就是GPT3 + Discrete Prompts for Machine Translation
  - ![](https://pic4.zhimg.com/80/v2-c72a6a803262e3ed3e4ec442a4382ebf_1440w.webp)
  - 如果使用固定预训练语言模型和连续固定的prompt，就是 GPT3 + Continuous Prompts for Machine Translation
  - ![](https://pic1.zhimg.com/80/v2-5bfc03ab8699075d28f0f64ca60142cc_1440w.webp)
*   **Fixed-LM** Prompt Tuning
  - 如果使用固定预训练语言模型和可训练的prompt，就是 BART + Continuous Prompts for Machine Translation
  - ![](https://pic2.zhimg.com/80/v2-d19c5a6e03987aea08bfd7896fc24c51_1440w.webp)
 
### 策略选择
 
对于不同的策略，需要进行不同的选择，我们往往需要考虑以下两点：
*   我们的数据量级是多少
*   我们的是否有个超大的 Left-to-right 的语言模型  

通常如果只有很少的数据的时候，希望不要去 fine-tune 预训练语言模型，而是使用LM的超强能力，只是去调prompt 参数。而让数据量足够多的时候，我们可以精调语言模型。

而只有像GPT-3 这种超大的语言模型的时候，我们才能直接使用，不需要任何的fine-tuning.
- ![](https://pic4.zhimg.com/80/v2-6a96f5cf3c0524bce4f9328746c6bff7_1440w.webp)
 
## Prompt 的优势是什么
 
Prompt Learning 的优势有哪些呢？从四个角度进行分析。
*   Level 1. Prompt Learning 角度
*   Level 2. Prompt Learning 和 Fine-tuning 的区别
*   Level 3. 现代 NLP 历史
*   Level 4. 超越NLP
- ![](https://pic2.zhimg.com/80/v2-b9363e52298acdc1997182b72b16bfe9_1440w.webp)
 
### Level 1. Prompt Learning 使得所有的NLP任务成为一个语言模型的问题

*   Prompt Learning 可以将所有的任务归一化预训练语言模型的任务
*   避免了预训练和fine-tuning 之间的gap，几乎所有 NLP 任务都可以直接使用，不需要训练数据。
*   在少样本的数据集上，能取得超过fine-tuning的效果。
*   使得所有的任务在方法上变得一致
- ![](https://pic2.zhimg.com/80/v2-e344a7b5f4364cb2ab6aa3e38681ebbd_1440w.webp)
 
### Level 2. Prompt Learning 和 Fine-tuning 的范式区别
 
*   Fine-tuning 是使得预训练语言模型适配下游任务
*   Prompting 是将下游任务进行任务重定义，使得其利用预训练语言模型的能力，即适配语言模型
- ![](https://pic3.zhimg.com/80/v2-e89d427849c1270e7caffc4990d5d5e6_1440w.webp)
 
### Level 3. 现代 NLP 第四范式
 
Prompting 方法是现在NLP的第四范式。其中现在NLP的发展史包含
1.  Feature Engineering：即使用文本特征，例如词性，长度等，在使用机器学习的方法进行模型训练。（无预训练语言模型）
2.  Architecture Engineering：在W2V基础上，利用深度模型，加上固定的embedding。（有固定预训练embedding，但与下游任务无直接关系）
3.  Objective Engineering：在bert 的基础上，使用动态的embedding，在加上fine-tuning。（有预训练语言模型，但与下游任务有gap）
4.  Prompt Engineering：直接利用与训练语言模型辅以特定的prompt。（有预训练语言模型，但与下游任务无gap）

在四个范式中，预训练语言模型，和下游任务之间的距离，变得越来越近，直到最后Prompt Learning是直接完全利用LM的能力。
- ![](https://pic4.zhimg.com/80/v2-041acfa2e38bb9c616e9ba1835f17b1f_1440w.webp)
 
### Level 4. 超越NLP的角度
 
Prompt 可以作为连接多模态的一个契机，例如 CLIP 模型，连接了文本和图片。相信在未来，可以连接声音和视频，这是一个广大的待探索的领域。
- ![](https://pic3.zhimg.com/80/v2-9fa54eb5b1b4b5ebc88843a6647c6c42_1440w.webp)

## 提升效果

提升 prompting 效果的方法
- prompt ensembling
  - 把多个prompt通过某种加权方法组合到一起
- prompt augmentation
  - 启发式学习
- prompt composition
  - 将复合的prompt句子，拆解成多个小段prompt，最后再组合在一起训练
- prompt decomposition
  - 由于一些任务的mask工作使用句子数量有限（比如词性标注任务），于是就只能通过decomposition将一个句子拆分成多个部分后，再对每个部分做prompt单独训练

## prompt 应用

prompt的应用
- 知识探索（事实探索和语言学探索）
- 分类任务（文本分类和自然语言推理）
- 信息提取（关系提取、语义分析和命名实体识别）
- NLP 中的推理（常识推理和数学推理）
- 问答
- 文本生成
- 文本生成的自动评估
- 多模态学习
- 元应用（域自适应、除偏和数据集创建）

![img](https://pic3.zhimg.com/80/v2-934ee6d5289bcf81b6a32f3e85c88712_1440w.webp)


## prompt 实践

### 清华 OpenPrompt

- 【2023-2-16】清华开源的 [OpenPrompt](https://github.com/thunlp/OpenPrompt)：An Open-Source Framework for Prompt-learning. 包含prompt各个算法、模块的代码及示例
- ![](https://camo.githubusercontent.com/d26f498b1bc63fe804fe21df9d1f91fa55673ee3c45242fd3f49439718376db7/68747470733a2f2f7a332e617831782e636f6d2f323032312f31312f30332f4941645433442e706e67)

Define a task

```py
from openprompt.data_utils import InputExample
classes = [ # There are two classes in Sentiment Analysis, one for negative and one for positive
    "negative",
    "positive"
]
dataset = [ # For simplicity, there's only two examples
    # text_a is the input text of the data, some other datasets may have multiple input sentences in one example.
    InputExample(
        guid = 0,
        text_a = "Albert Einstein was one of the greatest intellects of his time.",
    ),
    InputExample(
        guid = 1,
        text_a = "The film was badly made.",
    ),
]
```

Define a Pre-trained Language Models (PLMs) as backbone.

```py
from openprompt.plms import load_plm
plm, tokenizer, model_config, WrapperClass = load_plm("bert", "bert-base-cased")
```

定义模板

```py
from openprompt.prompts import ManualTemplate
promptTemplate = ManualTemplate(
    text = '{"placeholder":"text_a"} It was {"mask"}',
    tokenizer = tokenizer,
)
```

Define a Verbalizer

```py
from openprompt.prompts import ManualVerbalizer
promptVerbalizer = ManualVerbalizer(
    classes = classes,
    label_words = {
        "negative": ["bad"],
        "positive": ["good", "wonderful", "great"],
    },
    tokenizer = tokenizer,
)
```

Combine them into a PromptModel

```py
from openprompt import PromptForClassification
promptModel = PromptForClassification(
    template = promptTemplate,
    plm = plm,
    verbalizer = promptVerbalizer,
)
```

Define a DataLoader

```py
from openprompt import PromptDataLoader
data_loader = PromptDataLoader(
    dataset = dataset,
    tokenizer = tokenizer,
    template = promptTemplate,
    tokenizer_wrapper_class=WrapperClass,
)
```

Train and inference

```py
import torch

# making zero-shot inference using pretrained MLM with prompt
promptModel.eval()
with torch.no_grad():
    for batch in data_loader:
        logits = promptModel(batch)
        preds = torch.argmax(logits, dim = -1)
        print(classes[preds])
# predictions would be 1, 0 for classes 'positive', 'negative'
```


等等组件


# 思维链（CoT：Chain-of-thoughts）

【2023-2-19】[思维链（Chain-of-thoughts）作为提示](https://zhuanlan.zhihu.com/p/493533589)

## CoT 诞生之初

2021年，`提示学习`（prompt learning）浪潮兴起，以`离散式提示学习`（提示词的组合）为**起点**，`连续化提示学习`（冻结大模型权重+微调较小参数达到等价性能）为**复兴**，几乎是在年末达到了研究的一个巅峰。

2022年开始，逐渐有很多人意识到: `连续化提示学习`其中的一些好处伴随的一些**局限性**，比如**伪资源节约**，**不稳定**等等。很多研究者拒绝陪玩，虽认同提示学习将会带来下一代NLP界的革命，但是认为拒绝做他人大模型的附庸，开始探索大模型的训练技术，并且训练自己的大模型；而手头暂时没有掌握资源的研究者开始再次将研究重心从`连续化学习`转移到`离散式提示学习`上去，将研究聚焦于特定的`大模型GPT3`上。

此时，距离175B的GPT3模型被发布和上下文学习被发现过去了不到2年，热度经历了高潮与低谷，经历了深度学习流派关于`连接学派`和`符号学派`的辩论和是否具有**意识**和**推理能力**的讨论，一些基础的玩法在被开发之后就被搁置了一段时间直到提示学习的兴起。

2022年1月，OpenAI通过`强化学习`调试模型，使用强化学习调试更新了他们的模型到了第二代，LLM肉眼可见地变得更好提示，很多任务的性能也显著提升，尤其是一些之前没有办法很好进行的任务被显著地提高了起来。

思维链系列工作就是在这样一个大环境下产生的。

## CoT 介绍

2022年5月，谷歌年度开发者大会（Google I/O）上对CoT技术进行了宣传，还有谷歌的540B系数大模型PaLM和Pixel系列手机手表等等。
- ![](https://pic2.zhimg.com/80/v2-50bd83f418275a6db5afeb32c1be86a9_1440w.webp)

### CoT 提出

22年1月, 谷歌大脑研究员[Jason Wei](https://twitter.com/_jasonwei)放到arxiv上面的文章，提出了思维链这个概念。
- 论文：[Chain-of-Thought Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/abs/2201.11903)
- 2023年2月15日，一作[Jason Wei](https://www.jasonwei.net/),[twitter](https://twitter.com/_jasonwei),[github](https://github.com/jasonwei20), [personal page](https://jasonwei20.github.io/personal/), 离开Google Brain，加入OpenAI
- 2020年，达特茅斯本科毕业的ABC，I graduated with an AB from Dartmouth College in 2020.
- <img src="https://images.squarespace-cdn.com/content/v1/633a6c0dd119b45af0d898c2/1226b6e8-1232-4d1f-aeb2-7c4a61657610/taverna.jpeg?format=500w" weigth=30%>

> We explore how generating a **chain of thought** (CoT) -- a series of intermediate reasoning steps -- significantly improves the ability of **large language models** (LLM) to perform complex reasoning (复杂推理). 
>- In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called **chain of thought prompting**, where a few chain of thought demonstrations are provided as exemplars in prompting. 
>- Experiments on three large language models show that **chain of thought prompting** improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.

`思维链`是一种`离散式提示学习`
- 大模型下的上下文学习（即不进行训练，将例子添加到当前样本输入的前面，让模型一次输入这些文本进行输出完成任务），相比于之前传统的上下文学习，即通过 x1,y1,x2,y2,....x_test 作为输入来让大模型补全输出 y_test，思维链多了中间的一些闲言碎语絮絮叨叨，以下面这张图为例子
- ![CoT](https://pic3.zhimg.com/80/v2-e1b5adff46170f633e8ed635e7a57646_1440w.webp)

例子选择自一个数据集叫`GSM8K`，每一个样例大概就是一个小学一二年级的看几句话（基本都是三句）写算式然后算答案的难度，但是`GPT-3`通过刚刚说的最简单的提示方法曾经只能在这个数据集上做到6%左右的准确度。
- 由此可见，直接预测y不太可行。

思维链的絮絮叨叨，即不直接预测y，而是将y的“思维过程”r（学术上统称为relationale）也要预测出来。当然最后不需要这些“思维过程”，这些只是用来**提示**，获得更好的答案，只选择最后的答案即可。作者对不同的数据集的原本用于上下文学习的提示标注了这些思维链然后跑了实验，发现这么做能够**显著**提升性能（左图），且这种性能的提升是具有类似于**井喷性质**（右图）的（后来称这种性质叫`涌现性`）。
- ![](https://pic1.zhimg.com/80/v2-a5e20815867c458a74b5e30a13f3b53c_1440w.webp)


### CoT 改进

- 多数投票显著提高CoT性能
  - 2023年，[Self-Consistency Improves Chain of Thought Reasoning in Language Models](https://arxiv.org/abs/2203.11171)，[Jason Wei](https://www.jasonwei.net/) 二作
- 提出了一种boost方法，让中小模型也可以通过训练具有思维链能力
  - STaR: Self-Taught Reasoner Bootstrapping Reasoning With Reasoning
- Zero-Shot 推理
  - Large Language Models are Zero-Shot Reasoners
  - “Let's do it step by step“
- 其它文章
  - Least-to-Most Prompting Enables Complex Reasoning in Large Language Models
  - On the Advance of Making Language Models Better Reasoners
  - Rationale-Augmented Ensembles in Language Models

#### 多数投票显著提高CoT性能

2022年3月在arxiv上放出来
- 论文：[Self-Consistency Improves Chain of Thought Reasoning in Language Models](https://arxiv.org/abs/2203.11171)
- 改进思维链解码效率：新的解码策略（自一致性）替换原生的贪心解码

>**Chain-of-thought prompting** combined with pre-trained large language models has achieved encouraging results on **complex reasoning** tasks. In this paper, we propose a new **decoding strategy**, self-consistency, to replace the **naive greedy decoding** used in chain-of-thought prompting. 
>- It first samples a diverse set of **reasoning paths** instead of only taking the greedy one, and then selects the most consistent answer by marginalizing out the sampled reasoning paths. 
>- Self-consistency leverages the intuition that a complex reasoning problem typically admits multiple different ways of thinking leading to its unique correct answer.
>- Our extensive empirical evaluation shows that **self-consistency** boosts the performance of chain-of-thought prompting with a striking margin on a range of popular arithmetic and commonsense reasoning benchmarks, including GSM8K (+17.9%), SVAMP (+11.0%), AQuA (+12.2%), StrategyQA (+6.4%) and ARC-challenge (+3.9%).

思维链初代文章很快的一个跟进工作，是思维链系列文章版图的重要一步。
- 这篇文章几乎用的和初代思维链文章完全一样的数据集和设置，主要改进是使用了对答案进行了**多数投票**（majority vote），并且发现其可以显著地提高思维链方法的性能。
- 将greedy search变成了sample+vote
- ![](https://pic1.zhimg.com/80/v2-82f1234d53f8c0623d783eccdff272e4_1440w.webp)
- 一开始不如cot（因为temperature），后面大幅度超过
- ![](https://pic3.zhimg.com/80/v2-bc59df588bc49b11c82a46dab77379de_1440w.webp)
- 将`贪婪搜索`（greedy search），即将GPT模型的 temperature 从0设置为某个数值，比如说0.4，然后sample多个按照y进行投票，会显著地提升性能


#### Zero-Shot 推理

cot原本使用few shot来让llm说出来cot然后得到答案，但是他这个是分成两部分：
- 首先, 加一个“Lets think step by step”，得到一个cot的murmur
- 然后, 后面接上一句“so the anwser is”然后得到答案。

性能远超原来的zero shot逼近fee shot和cot的few shot性能。


# 结束