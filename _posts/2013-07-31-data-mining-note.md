---
layout: post
title:  "数据挖掘经验总结-data-mining-note"
date:   2013-07-31 23:02:00
categories: 数据挖掘
tags: 数据挖掘 机器学习 数据分析 陈皓 大数据
excerpt: 数据挖掘知识点、经验总结
author: 鹤啸九天
mathjax: true
---

* content
{:toc}

> 数据挖掘方向知识点、经验总结

# 相关性分析

- 注意：相关关系不等于因果关系
    - ![](http://bluewhale.cc/wp-content/uploads/2016/06/54b9822f9402b0.92166338.jpg)
- 相关分析的方法很多
    - 初级方法：快速发现数据之间的关系，如正相关，负相关或不相关。
    - 中级方法：对数据间关系的强弱进行度量，如完全相关，不完全相关等。
    - 高级方法：将数据间的关系转化为模型，并通过模型对未来的业务发展进行预测。

Read more: http://bluewhale.cc/2016-06-30/analysis-of-correlation.html#ixzz6fwTnlB2v


- （1）协方差及协方差矩阵
    - 协方差用来衡量两个变量的总体误差，如果两个变量的变化趋势一致，协方差就是正值，说明两个变量正相关。如果两个变量的变化趋势相反，协方差就是负值，说明两个变量负相关。如果两个变量相互独立，那么协方差就是0，说明两个变量不相关。
        - ![](http://bluewhale.cc/wp-content/uploads/2016/06/%E5%8D%8F%E6%96%B9%E5%B7%AE%E5%85%AC%E5%BC%8F.jpg)
    - 协方差只能对两组数据进行相关性分析，当有两组以上数据时就需要使用协方差矩阵。
        - ![](http://bluewhale.cc/wp-content/uploads/2016/06/%E5%8D%8F%E6%96%B9%E5%B7%AE%E7%9F%A9%E9%98%B5%E5%85%AC%E5%BC%8F.jpg)
    - 协方差通过数字衡量变量间的相关性，正值表示正相关，负值表示负相关。但无法对相关的密切程度进行度量。当我们面对多个变量时，无法通过协方差来说明那两组数据的相关性最高。要衡量和对比相关性的密切程度，就需要使用下一个方法：**相关系数**。
- （2）相关系数
    - 相关系数(Correlation coefficient)是反应变量之间关系密切程度的统计指标，相关系数的取值区间在1到-1之间。1表示两个变量完全线性相关，-1表示两个变量完全负相关，0表示两个变量不相关。数据越趋近于0表示相关关系越弱。
        - ![](http://bluewhale.cc/wp-content/uploads/2016/06/%E7%9B%B8%E5%85%B3%E7%B3%BB%E6%95%B0%E5%85%AC%E5%BC%8F.jpg)
        - ![](http://bluewhale.cc/wp-content/uploads/2016/06/Sxy%E5%85%AC%E5%BC%8F.jpg)
        - ![](http://bluewhale.cc/wp-content/uploads/2016/06/Sx%E5%85%AC%E5%BC%8F.jpg)
    - 缺点是无法利用这种关系对数据进行预测，简单的说就是没有对变量间的关系进行提炼和固化，形成模型。要利用变量间的关系进行预测，需要使用到下一种相关分析方法，回归分析。
    - **pearson**(皮尔逊), **kendall**（肯德尔）和**spearman**（斯伯曼/斯皮尔曼）三种相关分析方法有什么异同
    - （2.1）**pearson**(皮尔逊) **线性**相关系数
        - r = Cov(x,y) / σᵪσ 
    - 两个连续变量间呈线性相关时
        - 满足**积差**相关分析的适用条件时，使用Pearson积差相关系数
        - 不满足**积差**相关分析的适用条件时，使用Spearman**秩**相关系数来描述.
    - （2.2）Spearman（斯皮尔曼）相关系数又称秩相关系数，是利用两变量的**秩次大小**作线性相关分析，对原始变量的分布不作要求，属于**非参数**统计方法，适用范围要广些。对于服从Pearson相关系数的数据亦可计算Spearman相关系数，但统计效能要低一些。Pearson相关系数的计算公式可以完全套用Spearman相关系数计算公式，但公式中的x和y用相应的秩次代替即可。
        - 1.连续数据，正态分布，线性关系，用pearson相关系数是最恰当，当然用spearman相关系数也可以， 就是效率没有pearson相关系数高。
        - 2.上述任一条件不满足，就用spearman相关系数，不能用pearson相关系数。
        - 3.两个定序数据之间也用spearman相关系数，不能用pearson相关系数。
            - 定序数据是指仅仅反映观测对象等级、顺序关系的数据，是由定序尺度计量形成的，表现为类别，可以进行排序，属于品质数据。
            - 斯皮尔曼相关系数的适用条件比皮尔逊相关系数要广，只要数据满足单调关系（例如线性函数、指数函数、对数函数等）就能够使用。
    - （2.3）Kendall’s tau-b**等级**相关系数：用于反映分类变量相关性的指标，适用于两个分类变量均为有序分类的情况。对相关的有序变量进行非参数相关检验；取值范围在-1-1之间，此检验适合于正方形表格；
        - 计算积距pearson相关系数，连续性变量才可采用;
        - 计算Spearman秩相关系数，适合于定序变量或不满足正态分布假设的等间隔数据; 
        - 计算Kendall秩相关系数，适合于定序变量或不满足正态分布假设的等间隔数据。
    - 计算相关系数：当资料不服从**双变量正态分布**或**总体分布未知**，或原始数据用等级表示时，宜用 spearman或kendall相关
        - Pearson 相关复选项 积差相关计算连续变量或是等间距测度的变量间的相关分析
        - Kendall 复选项 等级相关 计算分类变量间的秩相关，适用于合并等级资料
        - Spearman 复选项 等级相关计算斯皮尔曼相关，适用于连续等级资料
    - 注：
        - 1 若非等间距测度的连续变量 因为分布不明-可用等级相关/也可用Pearson 相关，对于完全等级离散变量必用等级相关
        - 2 当资料不服从双变量正态分布或总体分布型未知或原始数据是用等级表示时,宜用 Spearman 或 Kendall相关。
        - 3 若不恰当用了Kendall 等级相关分析则可能得出相关系数偏小的结论。则若不恰当使用，可能得相关系数偏小或偏大结论而考察不到不同变量间存在的密切关系。对一般情况默认数据服从正态分布的，故用Pearson分析方法。
- （3）一元回归及多元回归
    - 回归分析（regression analysis)是确定两组或两组以上变量间关系的统计方法。两个变量使用一元回归，两个以上变量使用多元回归。
    - ![](http://bluewhale.cc/wp-content/uploads/2016/06/b0%E5%85%AC%E5%BC%8F-1024x72.jpg)
- （4）信息熵及互信息
    - 影响最终效果的因素可能有很多，并且不一定都是数值形式
    - 互信息可以发现哪一类特征与最终的结果关系密切.
        - 互信息是用来衡量信息之间相关性的指标。当两个信息完全相关时，互信息为1，不相关时为0。
        - 信息之间是有相关性的，互信息是度量相关性的尺子。互信息越高，相关性也越高。
    - 相关系数 vs 互信息：
        - 线性相关系数，从统计学出发度量信息A、B的关系，范围在-1到1，即有正相关和负相关。0表示相关
        - 互信息，从联合概率的角度计算，可以理解为A出现的时候B出现的概率，概率范围是从0到1，即完全不确定到完全确定
    - 计算方法见：[决策树分类和预测算法的原理及实现](http://bluewhale.cc/2016-03-20/decision-tree.html#ixzz6fwV8sDfo)
        - ![](http://bluewhale.cc/wp-content/uploads/2016/03/%E4%BA%92%E4%BF%A1%E6%81%AF1.jpg)
        - 离散
            - ![](https://imgconvert.csdnimg.cn/aHR0cDovL3MyLnNpbmFpbWcuY24vYm1pZGRsZS82MjU1ZDIwZGc3NTUyOGMzMTJlMDEmNjkw)
        - 连续
            - ![](https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy85MDcwNjM5LWY1ZjI4Y2Q4ZjdiOWFiZGQucG5nP2ltYWdlTW9ncjIvYXV0by1vcmllbnQvc3RyaXAlN0NpbWFnZVZpZXcyLzIvdy8zODUvZm9ybWF0L3dlYnA)
- 代码

```python
# 方法1:
#sklearn 计算
from sklearn import metrics as mr
if __name__ == '__main__':
    A = [1, 1, 1, 2, 3, 3]
    B = [1, 2, 3, 1, 2, 3]
    result_NMI=metrics.normalized_mutual_info_score(A, B)
    print("result_NMI:",result_NMI)
 
#备注：计算A和B的互信息，A,B为list或array。
 
#方法2:
from scipy.stats import chi2_contingency
 
def calc_MI(x, y, bins):
    c_xy = np.histogram2d(x, y, bins)[0]
    g, p, dof, expected = chi2_contingency(c_xy, lambda_="log-likelihood")
    mi = 0.5 * g / c_xy.sum()

# 备注：唯一区别就是这样实现使用自然对数而不是基2对数(因此它以“nats”而不是“bits”表示信息)。如果你喜欢bit，只需把mi除以log(2)

#方法3：
import numpy as np
 
def calc_MI(X,Y,bins):
 
   c_XY = np.histogram2d(X,Y,bins)[0]
   c_X = np.histogram(X,bins)[0]
   c_Y = np.histogram(Y,bins)[0]
 
   H_X = shan_entropy(c_X)
   H_Y = shan_entropy(c_Y)
   H_XY = shan_entropy(c_XY)
 
   MI = H_X + H_Y - H_XY
   return MI
 
def shan_entropy(c):
    c_normalized = c / float(np.sum(c))
    c_normalized = c_normalized[np.nonzero(c_normalized)]
    H = -sum(c_normalized* np.log2(c_normalized))  
    return H
 
A = np.array([[ 2.0,  140.0,  128.23, -150.5, -5.4  ],
              [ 2.4,  153.11, 130.34, -130.1, -9.5  ],
              [ 1.2,  156.9,  120.11, -110.45,-1.12 ]])
 
bins = 5 # ?
n = A.shape[1]
matMI = np.zeros((n, n))
 
for ix in np.arange(n):
    for jx in np.arange(ix+1,n):
        matMI[ix,jx] = calc_MI(A[:,ix], A[:,jx], bins)

```



- 【2020-12-07】
- `Sperman`或`kendall` **等级**相关分析
- `Person`相关（样本点的个数比较多）// 一般常用皮尔逊相关
- `Copula`相关（比较难，金融数学，概率密度）
- 一般这样认为：
    - 0.8-1.0 极强相关
    - 0.6-0.8 强相关
    - 0.4-0.6 中等程度相关
    - 0.2-0.4 弱相关
    - 0.0-0.2 极弱相关或无相关

- 参考
    - [五种常用的相关性分析方法](http://bluewhale.cc/2016-06-30/analysis-of-correlation.html)
    - [Pearson，Kendall和Spearman三种相关分析方法的异同](https://blog.csdn.net/sadfasdgaaaasdfa/article/details/46965827)
    - [皮尔森Pearson相关系数 VS 斯皮尔曼Spearman相关系数](https://blog.csdn.net/lambsnow/article/details/79972145)
    - [数学建模--相关性分析及Python实现](https://blog.csdn.net/ddjhpxs/article/details/105767589)
    - [大数据风控---互信息及Python实现](https://blog.csdn.net/qq_32123787/article/details/96371544)


## Pearson

- 皮尔逊相关系数( Pearson correlation coefficient），又称**皮尔逊**积矩相关系数（Pearson product-moment correlation coefficient，简称 **PPMCC**或**PCCs**）。用于衡量两个变量X和Y之间的线性相关相关关系，值域在-1与1之间。
- 给定两个连续变量x和y，皮尔森相关系数被定义为
    - ![](https://img-blog.csdn.net/20180417114819536)
    - ![](https://img-blog.csdnimg.cn/20190529103112626.png)

- 代码

```python
# （1）numpy库
pccs = np.corrcoef(x, y)
# （2）scipy库
from scipy.stats import pearsonr
pccs = pearsonr(x, y)
# （3）直接计算
def cal_pccs(x, y, n):
    """
    warning: data format must be narray
    :param x: Variable 1
    :param y: The variable 2
    :param n: The number of elements in x
    :return: pccs
    """
    sum_xy = np.sum(np.sum(x*y))
    sum_x = np.sum(np.sum(x))
    sum_y = np.sum(np.sum(y))
    sum_x2 = np.sum(np.sum(x*x))
    sum_y2 = np.sum(np.sum(y*y))
    pcc = (n*sum_xy-sum_x*sum_y)/np.sqrt((n*sum_x2-sum_x*sum_x)*(n*sum_y2-sum_y*sum_y))
    return pcc

```



## Spearman

- 由于原则上无法准确定义顺序变量各类别之间的距离，导致计算出来的相关系数不是变量间的关联性的真实表示。因此，建议对顺序变量使用斯皮尔曼相关系数。
- 斯皮尔曼相关系数的计算采用的是取值的等级，而不是取值本身。例如，给定三个值：33，21，44，它们的等级就分别是2，1，3.  计算斯皮尔曼相关系数的公式与计算皮尔森相关系数的类似，但用等级代替了各自的取值。
- ![](https://img-blog.csdn.net/20180417113553969)
- 相对于皮尔森相关系数，斯皮尔曼相关系数对于数据错误和极端值的反应不敏感。
- 斯皮尔曼相关系数的另一种公式表示如下：
    - ![](https://img-blog.csdn.net/20180417114927879)

## Kendall


# 数据分析

- 【2020-9-29】数据分析知识点：[定性分析和定量分析的根本区别在于](https://www.zhihu.com/question/19603466)
    - ① 画图前想想是定性分析还是定量分析
        - 定性分析描述性质，侧重结论，如占比，一般用饼图、四分位图等；
            - 分析1:郭小明胃口很大
            - 分析2:郭晓明胃口很小
        - 定量分析描述量级变化，侧重数字，一般用折线图、曲线图等。
            - 分析1:郭小明一顿可以吃3个Schweinshaxe
            - 分析2:郭小明在吃了3个Schweinshaxe之后还可以吃5个蛋挞
        - ![](https://pic3.zhimg.com/80/v2-d30a8823aa5039fb7d83183fbeaf4ac2_720w.jpg)
        - ![](https://pic3.zhimg.com/80/v2-1fe00ae8f51c1a31fa884a009f302ac6_720w.jpg)
        - ![](https://picb.zhimg.com/80/v2-08a5f625113fc11cac12b7901e933a02_720w.jpg)
    - ② 数据分析三大类：趋势、比较、细分。参考：[数据分析的三个常用方法：数据趋势、对比和细分分析](http://www.woshipm.com/data-analysis/676038.html)
        - 趋势
        - 比较
        - 细分


# [数据的游戏：冰与火](https://coolshell.cn/articles/10192.html)

- 【2013-7-31】陈皓（左耳朵耗子）
![](https://coolshell.cn/wp-content/uploads/2013/07/game-of-thrones-300x206.jpg)

我对数据挖掘和机器学习是新手，从去年7月份在Amazon才开始接触，而且还是因为工作需要被动接触的，以前都没有接触过，做的是需求预测机器学习相关的。后来，到了淘宝后，自己凭兴趣主动地做了几个月的和用户地址相关数据挖掘上的工作，有一些浅薄的心得。下面这篇文章主要是我做为一个新人仅从事数据方面技术不到10个月的一些心得，也许对你有用，也许很傻，不管怎么样，欢迎指教和讨论。
 
另外，注明一下，这篇文章的标题模仿了一个美剧《[权力的游戏：冰与火之歌](http://movie.douban.com/subject/3016187/)》。在数据的世界里，我们看到了很多很牛，很强大也很有趣的案例。但是，**数据就像一个王座一样，像征着一种权力和征服，但登上去的路途一样令人胆颤**。
 
## 数据挖掘中的三种角色
 
在Amazon里从事机器学习的工作时，我注意到了Amazon玩数据的三种角色。
*  **Data Analyzer：数据分析员**。这类人的人主要是分析数据的，从数据中找到一些规则，并且为了数据模型的找不同场景的Training Data。另外，这些人也是把一些脏数据洗干净的的人。
*   **Research Scientist：研究科学家**。这种角色主要是根据不同的需求来建立数据模型的。他们把自己戏称为不近人间烟火的奇异性物种，就像《生活大爆炸》里的 那个Sheldon一样。这些人基本上玩的是数据上的科学
*   **Software Developer ：软件开发工程师**。主要是把 Scientist 建立的数据模型给实现出来，交给Data Analyzer去玩。这些人通常更懂的各种机器学习的算法。

我相信其它公司的做数据挖掘或是机器学习的也就这三种工作，或者说这三种人，对于我来说，
*   **最有技术含量的是 Scientist**，因为数据建模和抽取最有意义的向量，以及选取不同的方法都是这类人来决定的。这类人，我觉得在国内是找不到的。
*   **最苦逼，也最累，但也最重要的是Data Analyzer**，他们的活也是这三个角色中最最最重要的（注意：我用了三个最）。因为，无论你的模型你的算法再怎么牛，在一堆烂数据上也只能干出一堆垃圾的活来。正所谓：*Garbage In, Garbage Out ！*但是这个活是最脏最累的活，也是让人最容易退缩的活。
*   **最没技术含量的是Software Developer**。现在国内很多玩数据的都以为算法最重要，并且，很多技术人员都在研究机器学习的算法。错了，最重要的是上面两个人，一个是苦逼地洗数据的Data Analyzer，另一个是真正懂得数据建模的Scientist！而像什么[K-Means](https://coolshell.cn/articles/7779.html "K-Means 算法")，[K Nearest Neighbor](https://coolshell.cn/articles/8052.html "K Nearest Neighbor 算法")，或是别的什么贝叶斯、回归、决策树、随机森林等这些玩法，都很成熟了，而且又不是人工智能，说白了，这些算法在机器学习和数据挖掘中，似乎就像Quick Sort之类的算法在软件设计中基本没什么技术含量。当然，我不是说算法不重要，我只想说这些算法在整个数据处理中是最不重要的。
    
 
## 数据的质量
 
- 目前所流行的Buzz Word——`大数据`是相当误导人的。在我眼中，<font color='red'>数据不分大小，只分好坏</font>。
- 在处理数据的过程中，我第一个感受最大的就是**数据质量**。下面我分几个案例来说明：
 
### 案例一：数据的标准
 
在Amazon里，所有的商品都有一个唯一的ID，叫ASIN——Amazon Single Identify Number，这个ID是用来标识商品的唯一性的（来自于条形码）。也就是说，无论是你把商品描述成什么样，只要ASIN一样，这就是完完全全一模一样的商品。
 
这样，就不像淘宝一样，当你搜索一个iPhone，会出现一堆各种各样的iPhone，有的叫“超值iPhone”，有的叫“苹果iPhone”，有的叫“智能手机iPhone”，有的叫“iPhone 白色/黑色”……，这些同一个商品不同的描述是商家为了吸引用户。但是带来的问题有两点：
- 1）**用户体验不好**。以商品为中心的业务模型，对于消费者来说，体验明显好于以商家为中心的业务模型。
- 2）**只要你不能正确读懂（识别）数据，你后面的什么算法，什么模型统统没用**。
 
所以，只要你玩数据，你就会发现，**如果数据的标准没有建立起来，干什么都没用。数据标准是数据质量的第一道关卡**，没这个玩意，你就什么也别玩了。所谓数据的标准，为数据做唯一标识只是其中最最基础的一步，数据的标准还单单只是这个，**更重要的是把数据的标准抽象成数学向量，没有数学向量，后面也无法挖掘**。
 
所以，你会看到，**洗数据的大量的工作就是在把杂乱无章的数据归并聚合**，这就是在建立数据标准。这里面绝对少不了人肉的工作。无非就是：
*   聪明的人在数据产生之前就定义好标准，并在数据产生之时就在干数据清洗的工作。
*   一般的人是在数据产生并大量堆积之后，才来干这个事。
 
另外，说一下Amazon的ASIN，这个事从十多年前就开始了，我在Amazon的内网里看到的资料并没有说为什么搞了个这样一个ID，我倒觉得这并不是因为Amazon因为玩数据发现必需建议个商品ID，也许因为Amazon的业务模型就是设计成以“商品为中心”的。今天，这个ASIN依然有很多很多的问题，ASIN一样不能完全保证商品就是一样的，ASIN不一样也不代表商品不一样，不过90%以上的商品是保证的。Amazon有专门的团队Category Team，里面有很多业务人员天天都在拼命地在对ASIN的数据进行更正。
 
### 案例二：数据的准确
 
用户地址是我从事过数据分析的另一个事情。我还记得当时看到那数以亿计的用户地址的数据的那种兴奋。但是随后我就兴奋不起来了。因为地址是用户自己填写的，这里面有很多的坑，都不是很容易做的。

第一个是假/错地址，因为有的商家作弊或是用户做测试。所以地址是错的，
*   比如，直接就输入“该地址不存在”，“13243234asdfasdi”之类的。这类的地址是可以被我的程序识别出来的。
*   还有很难被我的程序所识别出来的。比如：“宇宙路地球小区”之类的。但这类地址可以被人识别出来。
*   还有连人都识别不出来的，比如：“北京市东四环中路23号南航大厦5楼540室”，这个地址根本不存在。

第二个是真地址，但是因为用户写的不标准，所以很难处理，比如：
*   缩写：“建国门外大街” 和 “建外大街”，“中国工商银行”和“工行”……
*   错别字：“潮阳门”，“通慧河”……
*   颠倒：“东四环中路朝阳公园” 和 “朝阳公园 （靠东四环）” ……
*   别名：有的人写的是开发商的小区名“东恒国际”，有的则是写行政的地名“八里庄东里”……

这样的例子多得不能再多了。可见数据如果不准确，会增加你处理的难度。有个比喻非常好，**玩数据的就像是在挖金矿一样，如果含金量高，那么，挖掘的难度就小，也就容易出效果，如果含金量低，那么挖掘的难度就大，效果就差**。
 
上面，我给了两个案例，旨在说明——
- 1）**数据没有大小之分，只有含金量大的数据和垃圾量大的数据之分**。
- 2）**数据清洗是一件多么重要的工作，这也是一件人肉工作量很大的工作。**
 
所以，这个工作最好是在数据产生的时候就一点一滴的完成。
 
有一个观点：
>- 如果数据准确度在60%的时候，你干出来的事，一定会被用户骂！
>- 如果数据准确度在80%左右，那么用户会说，还不错！
>- 只有数据准确度到了90%的时候，用户才会觉得真牛B。

- 但是从数据准确度从80%到90%要付出的成本要比60% 到 80%的付出大得多得多**。
- 大多数据的数据挖掘团队都会止步于70%这个地方。
- 因为，再往后，这就是一件相当累的活。
 
## 数据的业务场景
 
我不知道有多少数据挖掘团队真正意识到了业务场景和数据挖掘的重要关系？我们需要知道，**根本不可能做出能够满足所有业务的数据挖掘和分析模型**。
 
推荐音乐视频，和电子商务中的推荐商品的场景完全不一样。电商中，只要你买了一个东西没有退货，那么，有很大的概率我可以相信你是喜欢这个东西的，然后，对于音乐和视频，你完全不能通过用户听了这首歌或是看了这个视频就武断地觉得用户是喜欢这首歌和这个视频的，所以，我们可以看到，推荐算法在不同的业务场景下的实现难度也完全不一样。
 
说到推荐算法，你是不是和我一样，有时候会对推荐有一种感觉——**推荐就是一种按不同维度的排序的算法**。我个人以为，就提一下推荐这个东西在某些业务场景下是比较Tricky的，比如，推荐有两种（不是按用户关系和按物品关系这两种），
*   一种是`共性化推荐`，结果就是推荐了流行的东西，这也许是好的，但这也许会是用户已知的东西，比如，到了北京，我想找个饭馆，你总是给我推荐烤鸭，我想去个地方，你总是给我推荐天安门故宫天坛（因为大多数人来北京就是吃烤鸭，就是去天安门的），这些我不都知道了嘛，还要你来推荐？另外，共性化的东西通常是可以被水军刷的。
*   另一种是一种是`个性化推荐`，这个需要分析用户的个体喜好，好的就是总是给我我喜欢的，不好的就是也许我的口味会随我的年龄和环境所改变，而且，总是推荐符合用户口味的，不能帮用户发掘新鲜点。比如，我喜欢吃辣的，你总是给我推荐川菜和湘菜，时间长了我也会觉得烦的。

**推荐有时并不是民主投票，而是专业用户或资深玩家的建议；推荐有时并不是推荐流行的，而是推荐新鲜而我不知道的**。你可以看到，不同的业务场景，不同的产品形态下的玩法可能完全不一样，
 
另外，就算是对于同一个电子商务来说，书、手机 和服装的业务形态完全不一样。我之前在Amazon做Demand Forecasting（用户需求预测）——通过历史数据来预测用户未来的需求。
*   对于书、手机、家电这些东西，在Amazon里叫Hard Line的产品，你可以认为是“标品”（但也不一定），预测是比较准的，甚至可以预测到相关的产品属性的需求。
*   但是地于服装这样的叫Soft Line的产品，Amazon干了十多年都没有办法预测得很好，因为这类东西受到的干扰因素太多了，比如：用户的对颜色款式的喜好，穿上去合不合身，爱人朋友喜不喜欢…… 这类的东西太容易变了，买得人多了反而会卖不好，所以根本没法预测好，更别Stock/Vender Manager 提出来的“预测某品牌的某种颜色的衣服或鞋子”。

对于需求的预测，我发现，长期在这个行业中打拼的人的预测是最准的，什么机器学习都是浮云。机器学习只有在你要面对的是成千上万种不同商品和品类的时候才会有意义。
 
**数据挖掘不是人工智能，而且差得还太远。不要觉得数据挖掘什么事都能干，找到一个合适的业务场景和产品形态，比什么都重要**。
 
## 数据的分析结果
 
我看到很多的玩大数据的，基本上干的是数据统计的事，从多个不同的维度来统计数据的表现。最简单最常见的统计就是像网站统计这样的事。比如：PV是多少，UV是多少，来路是哪里，浏览器、操作系统、地理、搜索引擎的分布，等等，等等。
 
唠叨一句，千万不要以为，你一天有十几个T的日志就是数据了，也不要以为你会用Hadoop/MapReduce分析一下日志，这就是数据挖掘了，说得难听一点，你在做的只不过是一个统计的工作。那几个T的Raw Data，基本上来说没什么意义，只能叫日志，连数据都算不上，只有你统计出来的这些数据才是有点意义的，才能叫数据。
 
当一个用户在面对着自己网店的数据的时候，比如：每千人有5个人下单，有65%的访客是男的，18-24岁的人群有30%，等等。甚至你给出了，你打败了40%同类型商家的这样的数据。作为一个商户，面对这些数据时，大多数人的表现是完全不知道自己能干什么？是把网站改得更男性一点，还是让年轻人更喜欢一点？完全不知道所措。
 
只要你去看一看，你会发现，好些好些的数据分析出来的结果，看上去似乎不错，但是其实完全不知道下一步该干什么？
 
所以，我觉得，**数据分析的结果并不仅仅只是把数据呈现出来，而更应该关注的是通过这些数据后面可以干什么？如果看了数据分析的结果后并不知道可以干什么，那么这个数据分析是失败的。**
 
#### 总结
 
综上所述，下面是我觉得数据挖掘或机器学习最重要的东西：
- 1）**数据的质量**。分为数据的标准和数据的准确。数据中的杂音要尽量地排除掉。为了数据的质量，大量人肉的工作少不了。
- 2）**数据的业务场景**。我们不可能做所有场景下的来，所以，业务场景和产品形态很重要，我个人感觉业务场景越窄越好。
- 3）**数据的分析结果**，要让人能看得懂，知道接下来要干什么，而不是为了数据而数据。

搞数据挖掘的人很多，但成功的案例却不多（相比起大量的尝试来说），就目前而言，**我似乎觉得目前的数据挖掘的技术是一种过渡技术，还在摸索阶段**。

另外，好些数据挖掘的团队搞得业务不业务，技术不技术的，为其中的技术人员感到惋惜……
 
不好意思，我只给出了问题，没有建议，这也说明数据分析中有很多的机会……
 
最后，还要提的一个是“**数据中的个人隐私问题**”，这似乎就像那些有悖伦理的黑魔法一样，你要成功就得把自己变得黑暗。是的，**数据就像一个王座一样，像征着一种权力和征服，但登上去的路途一样令人胆颤**。

# 结束


