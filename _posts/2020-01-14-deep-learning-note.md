---
layout: post
title:  "深度学习笔记"
date:   2020-01-14 12:44:00
categories: 深度学习 机器学习
tags: 神经网络  人工智能  AI  机器学习  ML  表示学习 AI圣经 读书笔记
excerpt: 深度学习学习读书笔记汇总。
mathjax: true
---

* content
{:toc}

>编者按： 平时收集了不少神经网络知识片段，过于零散，花了大半天时间将各种细节梳理，串联起来。


总结：<font color='red'></font>

这一篇杂文，把之前收集的神经网络点点滴滴，梳理、串联起来，便于理解，如有不当，麻烦及时指出，邮箱：wqw547243068@163.com。

# 资料

## [Deep Learning模型若干年的重要进展](https://www.leiphone.com/news/201804/YYH8dZZNUHlNn6B2.html)

- 2018年，清华大学唐杰副教授及其学生丁铭结合其Aminer数据库整理出 Deep Learning 模型最近若干年的重要进展，共有4条脉络。

![](https://static.leiphone.com/uploads/new/article/740_740/201804/5acc99ae02cd7.gif)

### Track.1 CV/Tensor

- 1943 年出现雏形，1958 年研究认知的心理学家 Frank 发明了感知机，当时掀起一股热潮。后来 Marvin Minsky（人工智能大师）和 Seymour Papert 发现感知机的缺陷：不能处理异或回路、计算能力不足以处理大型神经网络。停滞！
- 1986 年 Hinton 正式地提出反向传播训练 MLP，尽管之前有人实际上这么做。
- 1979 年，Fukushima 提出 Neocognitron，有了卷积和池化的思想。
- 1998 年，以 Yann LeCun 为首的研究人员实现了一个七层的卷积神经网络 LeNet-5 以识别手写数字。

后来 SVM 兴起，这些方法没有很受重视。

- 2012 年，Hinton 组的 AlexNet 在 ImageNet 上以巨大优势夺冠，兴起深度学习的热潮。其实 Alexnet 是一个设计精巧的 CNN，加上 Relu、Dropout 等技巧，并且更大。这条思路被后人发展，出现了 VGG、GooLenet 等。
- 2016 年，青年计算机视觉科学家何恺明在层次之间加入跳跃连接，Resnet 极大增加了网络深度，效果有很大提升。一个将这个思路继续发展下去的是去年 CVPR Best Paper Densenet。CV 领域的特定任务出现了各种各样的模型（Mask-RCNN 等），这里不一一介绍。
- 2017 年，Hinton 认为反省传播和传统神经网络有缺陷，提出 Capsule Net。但是目前在 CIFAR 等数据集上效果一半，这个思路还需要继续验证和发展。

### Track.2 生成模型

传统的生成模型是要预测联合概率分布 P(x,y)。

RBM 这个模型其实是一个基于能量的模型，1986 年的时候就有，他在 2006 年的时候重新拿出来作为一个生成模型，并且将其堆叠成为 Deep Belief Network，使用逐层贪婪或者 Wake-Sleep 的方法训练，不过这个模型效果也一般现在已经没什么人提了。但是从此开始 Hinton 等人开始使用深度学习重新包装神经网络。

Auto-Encoder 也是上个世纪 80 年代 Hinton 就提出的模型，此时由于计算能力的进步也重新登上舞台。Bengio 等人又搞了 Denoise Auto-Encoder。

Max Welling 等人使用神经网络训练一个有一层隐变量的图模型，由于使用了变分推断，并且最后长得跟 Auto-encoder 有点像，被称为 Variational Auto-encoder。此模型中可以通过隐变量的分布采样，经过后面的 decoder 网络直接生成样本。

GAN 是 2014 年提出的非常火的模型，他是一个隐的生成模型，通过一个判别器和生成器的对抗训练，直接使用神经网络 G 隐式建模样本整体的概率分布，每次运行相当于从分布中采样。

DCGAN 是一个相当好的卷积神经网络实现，WGAN 是通过维尔斯特拉斯距离替换原来的 JS 散度来度量分布之间的相似性的工作，使得训练稳定。PGGAN 逐层增大网络，生成机器逼真的人脸。

### Track3 Sequence Learning

1982 年出现的 Hopfield Network 有了递归网络的思想。1997 年 Jürgen Schmidhuber 发明 LSTM，并做了一系列的工作。但是更有影响力的是 2013 年还是 Hinton 组使用 RNN 做的语音识别工作，比传统方法高出一大截。

文本方面 Bengio 在 SVM 最火的时期提出了一种基于神经网络的语言模型，后来 Google 提出的 Word2Vec 也有一些反向传播的思想。在机器翻译等任务上逐渐出现了以 RNN 为基础的 seq2seq 模型，通过一个 encoder 把一句话的语义信息压成向量再通过 decoder 输出，当然更多的要和 attention 的方法结合。

后来前几年大家发现使用以字符为单位的 CNN 模型在很多语言任务也有不俗的表现，而且时空消耗更少。self-attention 实际上就是采取一种结构去同时考虑同一序列局部和全局的信息，Google 有一篇耸人听闻的 Attention Is All You Need 的文章。

### Track.4 Deep Reinforcement Learning

这个领域最出名的是 DeepMind，这里列出的 David Silver 是一直研究 RL 的高管。

Q-Learning 是很有名的传统 RL 算法，Deep Q-Learning 将原来的 Q 值表用神经网络代替，做了一个打砖块的任务很有名。后来有测试很多游戏，发在 Nature。这个思路有一些进展 Double Dueling，主要是 Q-learning 的权重更新时序上。

DeepMind 的其他工作 DDPG、A3C 也非常有名，他们是基于 policy gradient 和神经网络结合的变种（但是我实在是没时间去研究）

一个应用是 AlphaGo 大家都知道，里面其实用了 RL 的方法也有传统的蒙特卡洛搜索技巧。Alpha Zero 是他们搞了一个用 Alphago 框架打其他棋类游戏的游戏，吊打。

雷锋网注：
>本文获唐杰副教授授权转自其微博。唐杰老师带领团队研发了研究者社会网络 ArnetMiner 系统，吸引了 220 个国家 277 万个独立 IP 的访问。AMiner 近期持续推出了 AI 与各领域结合的研究报告，可访问 AMiner 官网了解更多详情。

# 《Deep Learning》读书笔记

- 2017年8月在雷锋网上的读书分享，先后组织了五六次，面向400+初学者直播，并提供文字版材料
- 以下资料在文字版基础上稍作修改、矫正

## 目录

## 第一章 Introduction

- 参考地址：[「Deep learning」读书分享 : 第一章 前言 Introduction](https://www.leiphone.com/news/201708/LEBNjZzvm0Q3Ipp0.html)

![](https://static.leiphone.com/uploads/new/article/740_740/201708/598fd35654561.jpg?imageMogr2/format/jpg/quality/90)

「Deep learning」这本久负盛名的书。内容比较多，一共20章，600多页。是不是压力山大？别着急，这个读书分享系列就是帮大家一起入门深度学习。

先讲第一章前言，主要是深度学习的一些基本介绍、发展历史。

![](https://static.leiphone.com/uploads/new/article/740_740/201708/599048d155fdd.jpg?imageMogr2/format/jpg/quality/90)

看一下书的封面封面，一幅漂亮的风景画，纽约中央公园遍地盛开的杜鹃花，仔细看，会发现有点不太正常，城堡、树枝怎么怪怪的？对了，计算机生成的假图，确切的说，是Google DeepMind团队的杰作——`梦幻公园`。
- 附录：麻省理工的书籍的开源地址。北大张志华团队贡献的中文版。

![](https://static.leiphone.com/uploads/new/article/740_740/201708/599048e54a75a.jpg?imageMogr2/format/jpg/quality/90)

2011年硕士毕业，在BAT里工作了有六年多，一个普通的工程师，水平不高。之前一直做数据挖掘，机器学习，大数据这一块，去年接触到深度学习，比较感兴趣，转岗到阿里云，做过半年的聊天机器人。所以说，也是一个新手，讲得不好，见谅，如果错误，麻烦及时指出，万分感谢。

为什么要做这个分享？
- 第一，跟大家一起交流学习，深知自己的知识储备不够，大胆出来分享，暴露不足，共同学习。
- 第二，借这个分享给自己施加压力，因为我也是有些惰性的（这本书打印版买了大半年，啃不动）。
- 第三，这本书内容挺多，有600多页，单凭我一个人的力量还不够，所以希望有更多的人加入进来，一起分享。

另外，现在是互联网时代，信息泛滥，各种资讯浩如烟海，看起来很多，但很容易碎片化，东一榔头西一棒子，不成体系，导致消化不良。真想学好一门技术，最好还是啃砖头，这种方式看似最笨，但却最为扎实。业界有个简单的方法判断一个人的技术实力：看多少书，啃的书越多，基础就越扎实，视野就越广。

![](https://static.leiphone.com/uploads/new/article/740_740/201708/59904942d67b4.jpg?imageMogr2/format/jpg/quality/90)

这是一张珍藏已久的图，我把它放在了自己的Github主页上，时刻提醒自己，<font color='red'>注意学习方法</font>。学习一门新技术，从刚开始接触了解，到大概一周后，不同的传递方式下，知识的留存率分别是多少。
- `听讲/阅读`，也就是大家现在的状态，只听我讲（或看文章）。听完（看完）后一周，再回想下，我讲了什么，你可能只记住5%，只是几个概念“深度学习”，或几张漂亮的图片、一个好玩的gif，说了一句比较有意思的话。注意：留存率才<font color='red' size='5'>5%</font>；
- 接着`看书`、其他的视频资料、别人演示，这时候，留存率能到<font color='red' size='5'>30%</font>，大幅提升；
- `主动学习`，学完之后跟别人讨论，留存率反弹到<font color='red' size='5'>50%</font>；
- `动手`写代码，那就接着涨到<font color='red' size='5'>75%</font>。
- 最后`讲给别人听`，留存率<font color='red' size='5'>90%</font>
   - 老实说，这一步挺难，从读书，查资料，消化理解，到做ppt，平均每个章节花费我4-8小时，甚至更多时间，几个月时间没有周末。

如果真的想学深度学习，建议自己尝试着给别人讲，组建学习小组，一起分享
- 子曰：赠人玫瑰，手留余香
- 子又曰：一个人走得快，一群人走得远

![](https://static.leiphone.com/uploads/new/article/740_740/201708/5992632a07c07.png?imageMogr2/format/jpg/quality/90)

关于留存率的另一个经典概念：`艾宾浩斯遗忘曲线`告诉我们：
>接触信息后，20min、1h会急剧下降到58%，一周后25%

![](https://static.leiphone.com/uploads/new/article/740_740/201708/599049430a227.jpg?imageMogr2/format/jpg/quality/90)

这本书刚开始就给出一张图，总体的描述一下每个章节是什么内容，章节和章节之间的关系，非常好，有利于形成全局观。图里面主要分成三部分：
- 第一部分是`数学基础`，涉及一些数学基础和概念，还有机器学习基础。数学涉及线性代数、概率论、信息论、数值分析，还有少量的最优化。
- 第二部分是`深度学习基本算法`。这个算法主要是几种典型的神经网络，从DFN，也就是深度前馈网络，开始，接下来分别演化到了怎么用正则、怎么优化。还有下面一部分的CNN，它是前馈网络的一种扩展，其实，RNN也是前馈网络衍生出来的，当然CNN是绝对正宗的前馈网络。接下来是实践的方法论，就是作者平时总结出来的一些实践经验。
- 第三部分就是`进阶内容`，涉及`线性因子模型`，`编码器`。这一部分非常重要，关系到Hinton的RBF，还有多个RBF堆叠形成的鼎鼎有名的DBN。接下来有`表示学习`、`蒙特卡洛方法`、`结构概率模型`、`深度生成模型`，也就是最近老出现的GAN，`生成对抗网络`，很厉害的样子。

![](https://static.leiphone.com/uploads/new/article/740_740/201708/599049430676c.jpg?imageMogr2/format/jpg/quality/90)

右上角是这个章节的目录。实际上书上的内容不多，我会加入自己的一些解释，把书的结构做一下调整，理解起来更方便。

新结构是这样的:
- 第一，什么是深度学习；
- 第二，跟机器学习什么关系；
- 第三，神经网络的历史；
- 第四，深度学习为什么现在这么火；
- 第五，深度学习能做什么。

总体思路: 
- 只提一些感性的认识，而不会去讲具体的细节。
- 如果大家想进一步了解，请踏实看书，学知识不要偷懒，不要偷懒，不要偷懒
   - **今天偷的懒，明天会加倍偿还**，<font color='blue'>这个时代在加速惩罚不学习的人</font>（地铁里教育机构的广告语，紧张不？焦虑售卖完毕）

### 什么是深度学习？

![](https://static.leiphone.com/uploads/new/article/740_740/201708/59904943098fa.jpg?imageMogr2/format/jpg/quality/90)

书里面提到一句话
- <font color='blue'>AI系统必须具备从原始数据提取模式的能力</font>, 这个能力指的是机器学习。
- 算法的性能在很大程度上是依赖于数据表示，也就是“`表示学习`”, ML里的一大方向。

传统的机器学习是怎么做的呢？主要是依赖于人工提取的特征，比如，常用的方式有SIFT，还有HoG、Harr小波等等，都是人工经验总结出来的。这几个特征挺强大，霸占了图像处理几十年，但问题是，扩展起来很不方便。深度学习跟传统机器学习相比，最明显的区别就是，把这些人工提取的过程都自动化。

回过头来想想，什么是深度学习？这是我的理解，黄色字体的标记的部分，来源于传统的神经网络，也就是联结主义（机器学习，它有很多方法，其中有联结主义），传统神经网络就是属于联结主义，深度学习是传统神经网络方法的扩展，它是一种延伸，不同的是，用了深度结构，多个简单的概念逐层抽象，构建复杂的概念，同时自动发现、提取分布式特征，注意，有个分布式。最后学到一个好的模型。深度学习这个概念是Geoffery Hinton 2006年首次提出的。

【2017-8-21】补充：其他门派
摘自：你们天天嚷嚷神经网络, 可是知道一开始的赫布律么?。

深度学习的两个重要特点：
- 第一，特征自动提取，传统信息学习里面那一套非常复杂的特征工程都不用了；
- 第二，逐层抽象，主要通过深度结构来实现。

![](https://static.leiphone.com/uploads/new/article/740_740/201708/599167e4d1edf.jpg?imageMogr2/format/jpg/quality/90)

这个图说的是什么是表示学习，不同的表示学习会有什么样的表现。

图中就是两个分类，这个圆形的和倒三角的，分别对应两种类别。如果采用简单的线性模型，显然是分不开的。但是只要经过某种变换后，比如从笛卡尔坐标系变到极坐标系，画一条垂直的直线就轻轻松松的分开了。所以说，数据集的表示方法不同，问题的难易程度也不同。

![](https://static.leiphone.com/uploads/new/article/740_740/201708/5991680df3630.jpg?imageMogr2/format/jpg/quality/90)

这张图说的是，深度学习是怎么通过深度结构解决问题的。底层的是一些像素，这里是三个像素，只取了三种颜色。当然这张图有点误导，看起来好像就只用这三种颜色，就能完成图像分类；实际上不是的。底层对应的是整张图片，一层一层，由下往上，这是一个层级抽象的过程；第二层是根据底层像素连接起来组成一些线段，或者是复杂一点的边缘，再到上面一层，形成局部的轮廓，就是一个角或者一些轮廓线。再往上层抽象，是变成了物体的一部分、或者整体，最后，就能看到近似是一个人了。
这个过程说的是，深度学习是通过一些深度结构进行逐层抽象，变相的把问题一步一步地简化。

![](https://static.leiphone.com/uploads/new/article/740_740/201708/599168f61a72e.jpg?imageMogr2/format/jpg/quality/90)

既然是深度学习，那么首先就要问一句，到底什么是深度，怎么定义这个深度？

先看右边这张图，这是一个计算图，也就是一个DAG图，描述一个计算任务的前后经过。实际上这个任务是传统机器学习里面的一种方法，叫逻辑回归，比如说，输入了一个样本，有两个特征X1、X2，分别乘以里面的权重W1、W2，乘起来，再累加求和，然后套一个sigmoid激励函数，本质上就是一个变换。这个简单的计算过程，可以用左边的图描述出来，这只是一种描述方法；右边也是另一种描述方法。区别是，右边是逻辑概念意义上的概念，而左边是计算步骤上的描述。
左边的描述方式，网络的深度是3，注意，第一层不算。右边这个描述方式，只有1层。书里面说了，这两种概念描述都是对的，就看你喜欢哪一种。通常情况下，我们是以右边这种为基准。按照逻辑概念划分时，又可以继续细分，标准就是要不要包含输入层。

![](https://static.leiphone.com/uploads/new/article/740_740/201708/59916a4d99d56.jpg?imageMogr2/format/jpg/quality/90)

看这张图，这个是input，就是传统网络，一提到神经网络，基本都会提到这样的结构，一个输入，再加上隐含层，再加上一个输出层。隐含层可以有多层。这个输入，有的地方不算层级，有的算，通常情况下是输入是不算的，这个就算是一个两层的网络。

![]https://static.leiphone.com/uploads/new/article/740_740/201708/59916ad702a6c.jpg?imageMogr2/format/jpg/quality/90)

### 深度学习跟机器学习之间有什么关系

深度学习源于机器学习，但是高于机器学习。图里第二个是机器学习，DL是AI的一大分支，AI 还有其他的方法，这个只是其中的一个分支而已。

跟传统神经网络相比，深度学习的深度体现在复杂的网络结构上。那么怎样才叫复杂呢，一般在4到5层以上，因为传统的神经网络是在2到3层，比如BP网络，两到三层，超过的话就不好训练，也训练不出来，所以基本上只到4到5层左右，不会有太多。
这是一张韦恩图，描述的各个概念之间的一些逻辑关系。这个就看的相对直观一些，最下面的红框里是机器学习，然后里面有一个流派叫表示学习，表示学习里面又有一个深度学习，就这么个层层嵌套的关系。

![](https://static.leiphone.com/uploads/new/article/740_740/201708/59916bcb96d51.jpg?imageMogr2/format/jpg/quality/90)

然后这张图描述的是不同的学习方法，区别体现在组成结构上。最左边是一个规则系统，希望通过一些逻辑推理去模拟学习的过程，这是最早的一版，人为设计的一个程序，输入数据，根据一定的规则，得到一个输出。它比较简单，中间实际上就两个过程。
下一步传统的机器学习，有什么区别呢，在上面它有一个特征映射。

然后再到表示学习。表示学习的方法里面分成两部分，左边一个，还有右边一个。左边一个跟之前相比的话，在特征工程上会花了不少精力，特征工程非常复杂，依赖经验，人力投入大。深度学习是表示学习中的一种，在它的基础上做了一些优化，也算是一个变革，区别是在于多了这一部分：自动提取特征。
这是历史上几种不同的学习模型在基本流程上的差别。

![](https://static.leiphone.com/uploads/new/article/740_740/201708/59916dd88b1d4.jpg?imageMogr2/format/jpg/quality/90)

### 神经网络的发展历史

一言以蔽之，神经网络命途多舛，先后经历了三次浪潮，也叫三起两落（邓爷爷三起两落）。这个第三落现在还不清楚，有人可能会问，深度学习这么火，怎么可能衰落？这个真不一定，前两次浪潮之巅上，大家也是这么想的。
- 第一次是在40年到60年左右，起源于控制论，主要是基于那些规则的方法来做。
   - 控制论时代，诞生了第一个人工神经元。神经元看着挺像现在的样子，但就一个神经元，没有什么层级结构，就是多个输入得到一个输出，要求和、再sigmoid，产生一个输出，就是个神经元而已。注意，当时，神经元里面的权重是人工设置的。
   - 后来Hebb学习法则指出权重是可以学习出来的。于是感知器降临，跟之前的神经元相比，除了有多层结构，还有权重从人工设置变成了自动化。感知器的网络结构一般2到3层，也就是传统的前馈网络。
   - 不过，感知器诞生没多久，麻省理工的AI 实验室创始人Marvin Minsky就发现了这种结构的要害，专门写了一本书叫「Perceptron」，直指感知器的两个核心问题
      - 第一，连简单的非线性问题都解决不了
      - 第二，非线性问题理论上可以通过多层网络解决，但是难以训练（在当时基本不可能）。
   - 这个非线性问题就是就是数字逻辑里的异或门。他在书里提到，抑或门是神经网络的命门。果然，由于这两个问题直指感知器要害，再加上Marvin Minsky强大的影响力，许多研究者纷纷弃城而逃，放弃神经网络这个方向，直接导致了第一次寒冬。这个寒冬持续时间很长，长达二三十年。
- 第二次是在86年左右，联结主义时代。
   - 1974年，反向传播算法第一次被哈佛博士生提出，但由于是寒冬，并没有受人重视
   - 直到1986年，hinton重新发明了BP算法，效果还不错，于是迎来了第二次兴起，大家又开始研究神经网络。
紧接着，又出来一个新的模型，也就是Vpnik的支持向量机（PGM也诞生了。这名字看起来很奇怪，为什么带个“机”字？因为经常带“机”的神经网络太火，加这个字是为了更好的发表）。跟近似黑盒的BP相比，SVM理论证明非常漂亮，更要命的是，支持向量机是全局最优解，而神经网络它是局部最优，于是，BP被SVM打趴了（也算是SVM的复仇吧），神经网络进入第二次寒冬了。
- 第三次是2006年，Hinton卧薪尝胆，坐了十多年冷板凳，潜心研究神经网络，终于提出了自编码和RBM，网络初始权重不再随机，而是有了更高的起点，再结合pre-training和fine-tune，解决了多层神经网络的训练问题，同时提出了深度学习这个概念，标记着深度学习的正式诞生（hinton是当之无愧的祖师爷）。这股热潮一直持续至到现在，大火到现在已经烧了十几年，越来越旺，甚至要灭掉机器学习了。什么时候会熄灭？这个真说不清楚。。。。

![](https://static.leiphone.com/uploads/new/article/740_740/201708/5991740b8d930.jpg?imageMogr2/format/jpg/quality/90)

这部分历史书里面也有简单介绍，看这张图，大概是分成三个阶段。我找了一些别的资料，让大家看得更加直观一些。

![](https://static.leiphone.com/uploads/new/article/740_740/201708/5991742abb212.jpg?imageMogr2/format/jpg/quality/90)

底图的是神经网络的一个电脑模拟图，不同网络连接起来，看起来就是很复杂很复杂。但是，再复杂也是有简单的神经元组成。这是一个神经元，生物上一些术语，这些概念大家也都清楚。核心部分就在于神经元在突触之间传递信息，从一个神经元到另外一个神经元。两个神经元之间有电位差，超过一定阈值时，就释放化学信号，神经递质。信息传递的过程，从一个神经元到另外一个神经元。

能不能用数学模型去模拟这个过程？这个是进一步的简化，就是三个神经元刺激传达到一个神经元，累加，电位差发生变化之后，然后出来两种结果，一种是兴奋，一种是抑制，它是两种状态。再往下看是信息的接收与传递过程，大家可以看一下。

![](https://static.leiphone.com/uploads/new/article/740_740/201708/5991754ca428a.jpg?imageMogr2/format/jpg/quality/90)

接下来就到了数学模型，模拟的就是神经元的结构。椭圆形的这一部分就是一个神经元，然后它跟上面有N个连接，每个连接的强度不同，对应于神经元的粗细；也就是这里面的W1j到Wij的权重，然后再做一个累加求和，再加激励函数处理，之后卡一个阈值，然后决定是不是要输出。
（注：阈值和偏置是一个东西吗？答：阈值是早期的偏置，那时候用符号函数，结合偏置b，变相实现了阈值功能，后期不用符号函数了，就是真正的偏置。阈值只是个概念，对应于生物神经网络，借助于偏置来实现）

这是数学模型上最根本的模拟，然后变过来就是这样一个公式。

![](https://static.leiphone.com/uploads/new/article/740_740/201708/599175e5bed30.jpg?imageMogr2/format/jpg/quality/90)

现在看一下他一些历史。这张图比书里更加详细。

这三次热潮
- 第一次是在58年，感知器诞生，这是第一次兴起；
- 第二次BP网络诞生；
- 第三次就是CNN和DBN的诞生，分别是Yann LeCun和Hinton两个人提出来，第三次兴起。

它在网络结构上的差别，第一个是单个神经元、到单层的神经网络，中间还有一个BP，BP过程加上了2到3层，会比这个更加复杂一些，最后到深度学习，他的层数是很多的，远不止前面的2到3层。

![](https://static.leiphone.com/uploads/new/article/740_740/201708/599176cc6cb5a.jpg?imageMogr2/format/jpg/quality/90)

从这张图上面我们可以对应到具体每个时期是哪个人。第一部分是基于一些逻辑推理的方法，最开始的一个神经元诞生是他们两个人提出来的，M-P模型（注意这不是multilayer perceptron，而是两个人名的简称，叫M-P更准确），可以解决与或非问题，控制论流行下的电子大脑时代。再到57年左右是感知器诞生，MLP，它跟之前的区别是有多个神经元，W1和W2的权重是可以自动训练的（基于Hebb学习法则）。这里有个问题，叫异或，这就异或门，也就是异或运算，逻辑运算的一种，类似的有与或非，而异或是难以用线性模型解决的。这个异或门就是神经网络的命门。图上面靠左的人就是Marvin Minsky，他写的书就直接导致了第一次寒冬。
然后是多层的感知机，也就是用了最开始的那个BP神经网络。这是在74年左右，就是BP神经网络诞生。那个时候美国和苏联撤走了神经网络相关研究经费，好多杂志甚至都不接收神经网络的论文，所以BP网络诞生的时候是没有什么影响力。
接下来是Hinton，他把BP重新设计了下。然后，接下来是支持向量机诞生，刚才提到一个它是全局最优解，解决问题比BP看起来更加漂亮，所以到这个时候就第二次寒冬来了。就图上这个过程，跟刚才总体的经过是一样的。

![](https://static.leiphone.com/uploads/new/article/740_740/201708/599178aaa3ddb.jpg?imageMogr2/format/jpg/quality/90)

这是几个阶段，第一个是单层网络，第二个两层，第三个多层。这三层网络在激活函数、异或问题和复杂问题上有所不同。像单层网络因为只有两种状态，一种激活或者抑制，所以是一个符号函数，函数值是1或者-1，异或函数解决不了，复杂问题也解决不了。
到了两层网络的时候，它的结构比之前复杂了，激活函数变了，不再是符号函数，中间是sigmoid。这个时候异或问题可以解决，但是复杂问题解决不了。到了多层网络，也就是现在的深度学习，激活函数主要是ReLU，比之前的sigmoid要简单，异或问题可以解决，复杂问题也可以解决。
所以现在深度学习模型中的激活函数主要是ReLU，而不是之前sigmoid。其实很早之前，大概是八九十年代，ReLU就已经出现了，只是当时人们选神经网络激励函数时，有一种偏执的迷信，sigmoid这样的激励函数，处处可导、连续、数学模型也漂亮，而ReLU很显然在零这个点是一个突变，是有问题的，不可导，所以看起来不漂亮。尽管它出来比较早，但是不怎么用它，直到后来，然后在深度学习里面经过多次的训练之后发现它的效果比sigmoid要好得多，因为sigmoid存在一个大问题，饱和，往正无穷或者负无穷这两个方向看，函数值趋近于1或者-1，没什么反应了，就只对就在0附近的值敏感，对非常大的或者非常小的点，淡定的很，不予理睬。这就是饱和问题。实际上，代价函数的梯度必须足够大，而且具有足够的预测性，指引学习方向，而饱和函数破坏了这一目标。sigmoid在小数据上表现较好，但是大数据集不佳，ReLU则由于提升系统识别性能和仿生的本质（神经元信号原理），在大数据集情形大显神威，重要性甚至超过隐含层权重。

![](https://static.leiphone.com/uploads/new/article/740_740/201708/59917aaca952c.jpg?imageMogr2/format/jpg/quality/90)

刚才是网络结构上变化，这个是在晶体管，就是在计算能力，还有数据、算法这三个因素，就直接导致深度学习的兴起。
第一个因素计算能力。计算能力主要是硬件层的，单层神经网络是在晶体管时代的，数据量也比较少，学习算法就是基本的一些简单的推理；到了两层神经网络，主要是用CPU来算，它的数据量大概是一千到一万，数据量是急剧增长的，算法主要用的BP。到了多层网络时候是主要通过集群或者GPU，甚至到现在还有更高级的TPU，数据量也是急剧的膨胀。它的训练方法跟BP相比有不同，这里面用的Pre-training，还有drop-out，主要防止过拟合的。
这三种不同阶段，它的网络结构、它的计算能力、数据量算法都是不同的。

![](https://static.leiphone.com/uploads/new/article/740_740/201708/59917c451ed9d.jpg?imageMogr2/format/jpg/quality/90)

这个说一下现在深度学习里面比较厉害的大神，Hinton绝对是这个领域的祖师爷；Yann LeCun主要在CNN上面贡献非常多；Bengio主要是在RNN这一块。这里面还提到一个人是Jordan，最近好像加入阿里了吧，右边这个女的是专门研究概率图的（跟支持向量机一起兴起）。还有吴恩达，主要这四个人（不一定准确，只是四王天王叫起来比较顺口，O(∩_∩)O哈哈~）。

![](https://static.leiphone.com/uploads/new/article/740_740/201708/59917c38d43a2.jpg?imageMogr2/format/jpg/quality/90)

随着深度学习的大红大紫，工业界一直在想办法争夺人才，把报的上名字的大神都找了个遍，图上这些大神一个一个的也落水了，到工业界去研究了，只有这个Bengio还一直处于中立状态，不过最近的消息好像他也加入了工业界。本书的作者是 Ian Goodfellow，GAN的发明人，Bengio是他的老师。
还有大神们的众多门生，相继发明了word2vec和fasttext，李飞飞及高徒andrej karpathy。。。
关系错综复杂，但数来数去也就那么几个人，“近亲繁殖”现象严重。

![](https://static.leiphone.com/uploads/new/article/740_740/201708/59917d26cc5b3.jpg?imageMogr2/format/jpg/quality/90)

这一部分是简单介绍一下前馈网络的一些基本结构。它每一种网络实际上是模拟某一个复杂的函数，它的目标是要学习每一层连接里面那个权重。权重达到了一个比较好的状态，就相当于这个函数已经达到了一个比较好的近似。书里面强调了一点：神经网络并不是目的，它不是为了完美的模拟大脑，而只是实现一种统计泛化，类似于一种函数近似机。因为人脑是非常复杂的，现在的网络结构远远达不到人脑的这种状态。
知名的网络结构主要两种，第一种是前馈，也就是说信息从一层一层往下流动，一路向前不回头（不算误差反向传播）。以CNN为例；第二种是反馈，就把上一次训练的状态留下来，作为本次的一个输入，它是前馈的一种扩展，特点是训练时会把上一次训练结果拿过来用，再决定下一步的训练。典型例子就是RNN，一边走一边回眸一笑，继续往前走。

![](https://static.leiphone.com/uploads/new/article/740_740/201708/59917edecb2f0.jpg?imageMogr2/format/jpg/quality/90)

深度学习之所以兴起有三个因素：海量的数据、计算能力、算法的突破。算法里面主要有几个RBM、BP，这是优化版的BP，还有一些训练方法pre-train，加上一些激活函数的变化。

![](https://static.leiphone.com/uploads/new/article/740_740/201708/59917f3316d6c.jpg?imageMogr2/format/jpg/quality/90)

这个图说的是数据。最开始的机器学习经常提到的Iris数据集，量级才在这个地方很小；后来到MNIST，量级在10的4次方左右；然后到ImageNet，到最后越来越大，就是说现在深度学习面临的问题会比之前复杂的多，数据也复杂得多。

![](https://static.leiphone.com/uploads/new/article/740_740/201708/59917fb00d6e9.jpg?imageMogr2/format/jpg/quality/90)

这个是MNIST的一个示例。Hinton给它的一个形容是机器学习界的果蝇，就像生物实验里面动不动研究一些遗传实验，牺牲了无数的果蝇。CNN里面也会经常拿它做练手，这是CNN里面入门的一个helloword（前馈神经网络的helloworld是异或问题）。

![](https://static.leiphone.com/uploads/new/article/740_740/201708/5991800ac60ca.jpg?imageMogr2/format/jpg/quality/90)

这个是，随着硬件性能的提升，模型的复杂度也越来越高。这个模型的复杂度就类似于大脑的复杂度，像果蝇的大脑其实相对于哺乳动物来说，没有那么复杂。所以大脑越复杂，它的连接的数目就越多。果蝇它的量级就比较小，但哺乳动物会比较高，当然人是最高的，也有可能有比人更高级的生物，只是我们不知道而已（比如普罗米修斯？）。

![](https://static.leiphone.com/uploads/new/article/740_740/201708/599180c2b281a.jpg?imageMogr2/format/jpg/quality/90)

现在随着时间推移的话，神经网络也就会不断的升级，基本上是每隔2.4年就会翻一倍。即便是现在的神经网络，他的计算能力还比不上一只青蛙。（据说百度大脑达到2-3岁婴儿的水平）

![](https://static.leiphone.com/uploads/new/article/740_740/201708/599180cd85b00.jpg?imageMogr2/format/jpg/quality/90)

神经网络能做什么，它现在的应用面已经非常广了，主要分为图像、语音、文字、游戏、无人驾驶等等，大家也都听的很多了。

![](https://static.leiphone.com/uploads/new/article/740_740/201708/5991816439310.jpg?imageMogr2/format/jpg/quality/90)

图像领域的，比如给一张图像，让机器识别图像里面的内容，比如人、自行车、石头等等，每个结果都给出一定的置信度。图像领域是深度学习威力最大的领域，传统方法基本都败北，翻天覆地。语音其次，NLP最难，NLU不好解，尚在改变过程中。

![](https://static.leiphone.com/uploads/new/article/740_740/201708/599182020b297.jpg?imageMogr2/format/jpg/quality/90)

再比如好玩一点的，国民老公和“先定一个小目标一个亿”王健林，能够根据图片识别出来是哪个人。还有明星的，经过整容以后脸都长的一个样，但是机器识别就能辨别出来，比如这是王珞丹和白百合。

这里着重提一下图像领域的应用进展。这是最开始的一版深度学习用于猫狗识别的例子，这个地方是一个多分类。

![](https://static.leiphone.com/uploads/new/article/740_740/201708/599182f11da89.jpg?imageMogr2/format/jpg/quality/90)

还有AlphaGo，和用CNN实现的玩游戏Flappy Bird。

![](https://static.leiphone.com/uploads/new/article/740_740/201708/59918308e4f75.jpg?imageMogr2/format/jpg/quality/90)

图像识别的发展历史。沿着时间顺序从右往左，2010年以来识别错误率是逐步下降的，在2012年左右有非常大的下降。它的原因是谷歌首次使用了深度学习（CNN），直接导致了接近10个点的下降。到了2014年也是谷歌的新一版的CNN，把错误率降到了6.7%。再到2015年微软的残差网络，错误率降到了3.57%。从数字上看好像没多大差别，但一方面越往上提升的空间越小，难度越大；第二个，里面有一个标志性的里程碑，人的错误率是5.1%，而这个时候机器是3.57%，已经超过了，所以说这是里程碑式的事件。里面用的数据集是李飞飞的开源数据集 ImageNet，在2016年的 ImageNet比赛里面中国团队就拿下了全部冠军，今年又拿下了一些冠军。在图像识别领域提升的空间不大了，今年这个比赛已经停止了，或许是已经没啥提升的空间了。

![](https://static.leiphone.com/uploads/new/article/740_740/201708/599184abea2a2.jpg?imageMogr2/format/jpg/quality/90)

这是一些超级网络的图示，最早的是8层网络，到了19层，再到22层，已经眼花缭乱看不清了。这个PPT是来自台大的李宏毅，大家可以找一下。

![](https://static.leiphone.com/uploads/new/article/740_740/201708/5991853d32aa6.jpg?imageMogr2/format/jpg/quality/90)


# 结束

