---
layout: post
title:  "【完整版】人类唯一的出路：变成人工智能"
date:   2019-06-04 23:41:00
categories: 人工智能
tags: 人工智能 AI 人类简史
excerpt: 深度思考
mathjax: true
---

* content
{:toc}


> * 全文地址：[【完整版】人类唯一的出路：变成人工智能](https://mp.weixin.qq.com/s?__biz=MjM5ODY2OTQyNg==&mid=2649768466&idx=1&sn=b6b1720b7a2243ab3907c58d2b22a602&chksm=bec3df0f89b456198c4960da99e46075f2bef121320f52a805b19f9143f5c083f6bac9430e97&token=832591528&lang=zh_CN#rd)
> * 原文地址：[英文版-
Neuralink and the Brain’s Magical Future](https://waitbutwhy.com/2017/04/neuralink.html)
> * Word文档版：[腾讯文档](https://share.weiyun.com/5y4D89d)

- ![](https://28oa9i1t08037ue3m1l0i861-wpengine.netdna-ssl.com/wp-content/uploads/2015/03/Logo-sometimes-Pixelmator-577.png)

# 人类唯一的出路：变成人工智能
Tim Urban，之前火热的文章《为什么有很多名人让人们警惕人工智能》也是出自他手，英文原文刊载于waitbutwhy.com，点击文末的阅读原文可以跳转到原文链接。
>Wait but Why的作者Tim Urban 是埃隆马斯克（特斯拉/SpaceX创始人）强烈推荐的科技博主。他写的AI文章是全世界转发量最高的。他的粉丝还包括：Facebook创始人马克扎克伯格，Facebook COO谢丽桑伯格等。Tim也是TED演讲平台上有史以来最受欢迎的演讲者之一

在一个由人工智能和“其他所有生物”组成的未来， 人类只有一条出路：“变成人工智能。

本翻译版本由谢熊猫君提供，全文共六万字。两百余张图片，分成六个章节，将分成五篇推送完成。
- 第一章：人类巨灵 （约7000字）简述人类语言、智能和人类巨灵的崛起
- 第二章：大脑 （约8000字）简述大脑结构，为了解脑机接口提供基础知识
- 第三章：脑机接口（约12000字）讲述脑机接口的基本原理和目前的技术水平
- 第四章：挑战（约8000字）讲述目前脑机接口跨越到全脑接口所要面临的挑战
- 第五章：魔法纪元（约13000字）全脑接口实现后未来的人类会是怎样
- 第六章：大融合（约10000字）人类唯一的出路：变成人工智能

注：原文请看:[【完整版】人类唯一的出路：变成人工智能](https://mp.weixin.qq.com/s?__biz=MjM5ODY2OTQyNg==&mid=2649768466&idx=1&sn=b6b1720b7a2243ab3907c58d2b22a602&chksm=bec3df0f89b456198c4960da99e46075f2bef121320f52a805b19f9143f5c083f6bac9430e97&token=832591528&lang=zh_CN#rd)

# [清华“天机”芯片登上Nature封面：全球首款异构融合类脑芯片](https://www.tmtpost.com/4099136.html)

2019-08-01 16:25

摘要： 
>这也是中国的人工智能芯片，首次登上Nature。

![](https://images.tmtpost.com/uploads/images/2019/08/20190801162443828.jpg?imageMogr2/strip/interlace/1/quality/85/thumbnail/1400x649/gravity/center/crop/!1400x649&ext=.jpg)
   
近日，权威科技杂志《自然》(Nature)封面报道了来自清华大学团队的“天机”类脑芯片，论文标题为“Towards artificial general intelligence with hybrid Tianjic chip architecture”(面向通用人工智能的异构融合“天机”芯片架构)。
![](https://img1.mydrivers.com/img/20190801/f583e6ba-4539-4b0c-9ffe-8e85758f1451.jpg)

传统芯片均基于冯诺依曼架构，清华天机则是一种类似人类大脑机制的非传统结构，类似Intel正在研究的神经拟态芯片Loihi。

其实早在2015年，清华团队就完成了第一代“天机”芯片，2017年进化为第二代，速度更快，性能更高，功耗更低，相比于当前世界先进的IBM TrueNorth，也具备功能更全、灵活性、扩展更好的优点，密度高出20%，速度高出至少10倍，带宽高出至少100倍。

最新一代天机芯片采用28nm工艺制造，核心面积仅仅3.8×3.8毫米，包含156个FCores核心，拥有大约40000个神经元和1000万个神经突触，可以同时支持机器学习算法和类脑电路。

它不仅算力高、功耗低、支持多种不同AI算法，而且采用了存算一体技术，不需要外挂DDR缓存，可大大节省空间、功耗和成本。

![](https://img1.mydrivers.com/img/20190801/S7d431361-7176-4c6e-aee6-8ed3e6360faf.jpg)

## 自行车

今天，一辆来自清华的无人驾驶自行车登上了Nature的封面。

在论文中，研究团队描述了天机芯片如何帮助机器响应语音命令，识别周围世界，避开障碍，并保持平衡，展示了搭载该芯片的自动驾驶自行车如何自动控制平衡，并在操场上对目标人物进行识别、跟随、自动避障。

这个无人智能自行车系统包括激光测速、陀螺仪、摄像头等传感器，刹车电机、转向电机、驱动电机等制动器，以及控制平台、计算平台、天机板级系统等处理平台等。

据介绍，天机芯片目前还是非常初步的研究，但是团队已经启动了下一代芯片的研发，预期明年初完成。

![](https://img1.mydrivers.com/img/20190801/c4ded3bb9e914162a8c3aba36747d54e.gif)

S型路线跟踪

![](https://img1.mydrivers.com/img/20190801/3d029837c4b042e0ac978e153ae819e6.gif)

语音控制“左转”

![](https://img1.mydrivers.com/img/20190801/23e7680d94764435bd4f6a3b7283d064.gif)

语音控制“直行和加速”
这辆自行车不仅可以平衡自身，还可以绕过障碍物，甚至可以响应简单的声音命令。
      
![](https://images.tmtpost.com/uploads/images/2019/08/1dd7f2fdf7a3a4de36bf7bba1e17aff3_1564647942.gif?imageMogr2/strip/interlace/1/quality/85/thumbnail/1400x786/gravity/center/crop/!1400x786&ext=.gif)

自行车能够按照声音命令改变方向或调整速度
 
自行车检测并跟踪移动的人，并在必要时避开障碍物

这辆自行车能够如此平衡、顺利的自主运行，靠的是自行车背后的大脑。它采用了一种名为“天机（Tianjic）”的新型计算机芯片，用于实时物体检测，跟踪，语音识别，避障和平衡控制。

邓磊介绍，无人自行车系统的语音识别、自主决策、视觉追踪功能运用了模拟大脑的模型，而目标探测、运动控制和躲避障碍功能运用了机器学习算法模型。

研究团队还指出：“通过随机将新变量实时引入环境中可以产生高时空复杂性，例如不同的道路条件、噪声、天气因素、多种语言、更多人等等。通过探索允许适应这些环境变化的解决方案，可以检查对AGI至关重要的问题，比如概括、稳健性和自主学习。”

这只来自清华的团队也凭借Tianjic芯片，登上了8月1日发布的最新一期Nature封面。

这也是中国的人工智能芯片，首次登上Nature。

![](https://images.tmtpost.com/uploads/images/2019/08/1ed34cccf47ff53e8c057e5d5493dbea_1564647942.png?imageMogr2/strip/interlace/1/quality/85/format/jpg/thumbnail/1400x489/gravity/center/crop/!1400x489&ext=.png)        

论文的通讯作者、清华大学精密仪器系教授、类脑计算中心主任施路平教授表示，虽然这还是非常初步的一个研究，但或许能够推动通用人工智能（AGI）计算平台的进一步发展。

![](https://images.tmtpost.com/uploads/images/2019/08/5171b180a36d673c0796c5600d2c28a1_1564647942.jpeg?imageMogr2/strip/interlace/1/quality/85/thumbnail/1400x919/gravity/center/crop/!1400x919&ext=.jpeg)       

本次的论文作者来自清华大学、北京灵汐科技、北京师范大学、新加坡理工大学和美国加州大学圣塔芭芭拉分校等机构。
- 论文链接：https://www.nature.com/articles/s41586-019-1424-8

![](https://images.tmtpost.com/uploads/images/2019/08/16a9e04d6af244ea188966b594f14dee_1564647943.jpeg?imageMogr2/strip/interlace/1/quality/85/thumbnail/1400x1848&ext=.jpeg)    

## 什么是AGI？

这款芯片可以同时融合两种方案正是其受到关注的关键所在。`通用人工智能`（Artificial General Intelligence，AGI），是一个尚未实现的研究课题，有时也被称作强人工智能，它所描述的机器智能可以理解或学习人类所能完成的任何智力任务。

![](https://static.leiphone.com/uploads/new/images/20190801/5d4264e2de9ca.png?imageView2/2/w/740)

对于AGI，部分人工智能学者认为，AGI的概念并不严肃，在实践中基本不可能实现。

另一些人则十分看好人工通用智能的发展，认为它有可能塑造人类的发展轨迹。

在Nature论文的新闻发布会中，施路平表示，“AGI是一个非常难的研究课题，但我们相信它是一定会实现的”。

施路平认为，发展通用人工智能的最佳方案之一是：<font color='red'>把人脑和电脑的优势结合起来</font>。

这种研究思路也就意味着要将`计算机科学导向`和`神经科学导向`这两种发展AGI的方法结合在一起。但是这两种方式在公式和编码方案上存在根本差异，想要结合困难重重。

也就是说，这种结合的核心挑战在于`脉冲神经网络`（SNN）和`人工神经网络`（ANN）的融合。

在生物大脑中，每个神经元都与各种输入相连。一些输入在神经元中产生激发，而另一些输入则抑制它，对于SNN（脉冲神经网络），在达到由变量（或者可能具有函数）描述的特定阈值状态时，神经元发出脉冲信号。

ANN则是是从信息处理角度对人脑神经元网络进行抽象，目前热门的AI神经网络CNN、RNN都属于ANN。

就可以理解SNN和ANN最大的差异
- ANN以精确的多位值处理信息
- 而SNN以脉冲处理信息

为了在一个平台上实现两种模型，脉冲需要表示为数字序列（1或0），以便与数字编码格式的ANN兼容。

当然，两者之间还存在其它差异，比如
- SNN在时空域中运行，而ANN依靠时钟在每个周期刷新信息。
- SNN的计算包括膜电位积分，阈值交叉和电位复位，ANN主要与乘法累加（MAC）操作和激活变换相关。
- SNN的处理需要比特可编程存储器和额外的高精度存储器来存储膜电位，发射阈值和不应期，ANN仅需要用字节存储器来进行激活存储和变换。

## 计算机科学+神经科学双导向，构建更普遍的通用平台

开发通用人工智能有两种主要方法：
- 一种是`神经科学导向`，植根于神经科学，并试图构建与大脑非常相似的电路。
- 另一种是`计算机科学导向`，以计算机科学为基础，并使用计算机来执行机器学习算法。

因为在制剂和编码方案的基本差异，这两种方法依赖于不同的和不兼容的平台，延缓了AGI的发展。

因此，通用人工智能的发展亟待一个支持更普遍的、基于计算机科学的人工神经网络以及神经科学启发的模型和算法的通用平台。

![](https://images.tmtpost.com/uploads/images/2019/08/7151e687d18f34311cd74692f5671527_1564647943.jpeg?imageMogr2/strip/interlace/1/quality/85/thumbnail/1400x792/gravity/center/crop/!1400x792&ext=.jpeg)      

### 两种方法的结合促进AGI发展

这款天机（Tianjic）芯片则集成了两种方法，以提供混合、协同平台。

天机芯片采用多核架构，可重构构建模块和采用混合编码方案的流线型数据流，不仅可以适应基于计算机科学的机器学习算法，还可以轻松实现脑启动电路和多种编码方案。

![](https://images.tmtpost.com/uploads/images/2019/08/b1739c3b76e2147df3d20e7fc224d119_1564647943.jpeg?imageMogr2/strip/interlace/1/quality/85/thumbnail/1400x1228/gravity/center/crop/!1400x1228&ext=.jpeg)      

“天机”芯片示意图

通过资源复用，天机芯片只需百分之三的额外面积即可同时运行计算机科学和神经科学导向的绝大多数神经网络模型，支持异构网络的混合建模，形成时空域协调调度系统，发挥它们各自的优势，既能降低能耗，提高速度，又能保持高准确度。

天机芯片同时具有多个功能核心，可轻松地重新配置，使其能够适应机器学习算法和大脑启发电路。研究人员通过将其中一个芯片整合到无人驾驶的自行车中来证明这种方法的潜力，这种自行车可以实现自我平衡，通过语音控制并且可以检测和避开障碍物。

![](https://images.tmtpost.com/uploads/images/2019/08/527f1b98d719c2ffe824a14a67540858_1564647944.jpeg?imageMogr2/strip/interlace/1/quality/85/thumbnail/1400x1606/gravity/center/crop/!1400x1606&ext=.jpeg)      

芯片评估建模示例

在论文中，该团队表示，仅使用一个芯片，就可以在无人驾驶自行车系统中同时处理多种算法和模型，实现实时物体检测、跟踪、语音控制、避障和平衡控制。

![](https://images.tmtpost.com/uploads/images/2019/08/db856781ceda5d8aa886e72bfbcf7ed9_1564647944.jpeg?imageMogr2/strip/interlace/1/quality/85/thumbnail/1400x1386/gravity/center/crop/!1400x1386&ext=.jpeg)     

基于天机芯片的自动驾驶自行车多模态集成示例图

## 跨学科组队，七个院系共同参与，七年磨一“芯”

在7月30日的电话新闻发布会中，论文通讯作者、清华大学精密仪器系教授`施路平`介绍了论文的研究思路。研究团队在接受媒体的采访时表示，从2012年孕育这项研究开始，团队遇到的最大挑战不来自于科学、也不来自技术，而是在于学科的分布不利于解决当前的问题。

因此，研究团队成立了七个院系组成的类脑计算研究中心，覆盖脑科学、计算机、微电子、电子、精仪、自动化、材料等学科。

团队成员之一，加州大学圣塔芭芭拉分校博士后邓磊表示，在芯片方面，遇到的最大挑战是**如何实现深度和高效的融合**。计算机科学导向和神经科学导向是目前流行的两类神经网络模型，这两种模型的语言、计算原理、信号编码方式、应用场景都有很大不同，所以需要的计算架构和存储架构大相径庭，甚至设计的优化目标都很不一样。一些深度学习加速器和神经形态芯片，基本上都是独立的设计体系，因此深度融合并不简单。

深度融合不是深度学习加速模块和神经形态模块简单的拼合，难点在于每部分的比例难以确认，因为现实中的应用复杂多变。而且，如果构建异构的混合模型，可能还需要在两个模块之间添加专门的信号转换单元，这又会有很多额外成本，所以，如何设计一套芯片架构兼容两类模型，可以灵活的配置同时又具有高性能，是团队芯片设计中的一大挑战。

2015年，施路平团队设计出第一代“天机芯”，经不断改进设计，2017年第二代“天机芯”问世。相比于当前世界先进的IBM的TrueNorth 芯片，2017年流片成功的第二代“天机芯”密度提升20%，速度提高至少10倍，带宽提高至少100倍，灵活性和扩展性更好。

也正如MIT科技评论报道所说，“该芯片暗示了**中国在开发自己的芯片设计能力方面取得的进展**。中国研究人员表明，他们可以制造专门的AI芯片以及任何芯片。” 

## 类脑可以超越人脑吗？

其实早在3年前，谷歌就曾在愚人节那天发布过一辆理想中的自动驾驶自行车。

在谷歌的想象中，这辆“自行车”不仅平衡力超高

![](https://images.tmtpost.com/uploads/images/2019/08/a4f3742b87d55b8989267c079983c4ff_1564647945.gif?imageMogr2/strip/interlace/1/quality/85/thumbnail/1400x763&ext=.gif)       

还能够自动通过红绿灯路口，自主导航找到你的位置。

![](https://images.tmtpost.com/uploads/images/2019/08/eb45d1790480037035f7d53a4b5a0428_1564647945.gif?imageMogr2/strip/interlace/1/quality/85/thumbnail/1400x763&ext=.gif)       

但作为“愚人节视频“发布，也说明了这一技术的难点和不易实现。

而在今天，清华施路平团队终于初步实现了这一想象。这样的黑科技似乎也带着一丝科幻色彩，让人畅想AGI到来的那天。

在接受采访时，施路平教授表示，类脑能否超越人脑的问题，其实和电脑是否能超越人脑的问题类似。

电脑在某些方面其实早就超过了人类，其精准快速的运算能力、强大的记忆让我们叹为观止。然而，目前在很多智能的层次，计算机和人脑还是有相当大的距离。 特别是对于不确定性的问题，比如学习、自主决策等领域。

计算机会逐渐缩小差距，至于最后能否全面超过人脑，施路平教授觉得从技术的层面来看会越来越多，“因为计算机的发展有一个特点，就是`它从不退步，它一直往前走`。但是我相信我们人是有智慧的，我们会在发展的过程当中来逐渐的完善我们对于研究领域的一个理解，来把控它的风险，因为我相信人们之所以对这个问题重视，是因为我们担心会不会像科幻电影说的那样毁灭人类。”

关于AGI是否会超越人类智慧，吴恩达在AI For Everyone课程中也表示，`完全的AGI的出现可能还需要几十甚至上百年，从时间上来说，我们也不需要多度担心`。


## [从“天机”芯片看脑科学与AI的融合](https://zhuanlan.zhihu.com/p/76323171)

许铁-巡洋舰科技

7月31日Nature杂志封面刊登了清华类脑计算团队的最新成果： 天机芯片以及由其操控的自行车。

Towards artificial general intelligence with hybrid Tianjic chip architecture[1]

Letter | Published: 31 July 2019

Towards artificial general intelligence with hybrid Tianjic chip architecture
这则信息在一天之内在AI圈子引起了热议，而大部分吃瓜群众的状态则是云里雾里。 这篇文章从脑与人工智能结合的潜力与背景， 看这系列最新工作的意义。

我们说这个新工作的核心是能够同时在芯片上高效实现人工神经网络ANN和脉冲神经网络SNN， 所谓的ANN和SNN， 事实上是神经网络发展过程的两个分支。 欲了解其背景先了解其历史。

### 神经网络家族的分合故事。

神经网络的故事从模拟单个神经元开始： 神经元是神经网络信息传输的“原子”。通过一定的方法连接这些原子，就可以得到具有智能的系统， 这算是整个人工智能“连接主义”流派的哲学根基。

那么如何构建这个认知的“原子” ？ 我们来看看最早的把连接主义引入机器学习的尝试。 最早的模拟大脑的单个神经元的尝试， 是Warren McCulloch 和 Walter Pitts 在1943 提出而来神经元的模型。 这个模型类似于某种二极管或逻辑门电路。 一定的输入进来，被神经元汇集加和， 如何这个和的总量大于一个阈值，神经元就放电， 小于一个阈值，神经元就不放电。 这个东西就好像某个微小的决策装置， 把很多因素加载在一起， 做一个最终的决策。 我们想象无数的二极管可以构成一个计算机，那么无数这这样的神经元不就可以构成一个具有计算功能的大脑吗？ 这就是感知器的概念。

这个高度简化的神经元事实上就是后来的人工神经网络ANN的基础， 简化得到的神经元事实上每一个的数学形式等价于一个加入了非线性过滤的线性回归。

![](https://www.zhihu.com/equation?tex=y+%3D+%5Cphi%28%5Csum+W_%7Bij%7DX_j+%2B+b%29+)

如果把无数这样的神经元连接起来， 就构成了所谓的人工神经网络（ANN）。

当下的深度学习工具， 无论是CNN还是RNN， 都是在这个方程基础上把更多的神经元连接起来加入不同的限制条件得来的。

然而事实上， 这个架构与真正的生物神经网络相差极远， 这个差距首要集中在单个神经元模型上。 刚刚的方程是一个把原来的生化过程简化到不能再简的结果。 这里面最致命的区别在于， spike。 通过观察上述方程我们可以看出， 神经网络输出y是一个实数。而事实上， 真实的生物神经元输出， 更加基接近的是一个0，1过程， 当神经元经历的电压超过一个数值， 它就放电。 那是不是说明这个spiking反而更简单？ 其实不是， 这里面人们忽略掉的一个信息就是spike timing以及背后的电压变化。 真实神经元的放电过程由一组微分方程(Hodykin Huxley equations 1952)表达 :

![](https://www.zhihu.com/equation?tex=+%5Cbegin%7Bgather%2A%7D+I+%3D+C_m+dV_m+%2F+dt+%2B%5Cbar%7Bg_K%7Dn%5E4%28V_m+-+V_K%29++%2B+%5Cbar%7Bg_%7BNa%7D%7Dm%5E3+h%28V_m+-+V_%7BNa%7D%29+%2B+%5Cbar%7Bg_l%7D%28V_m+-+V_l%29%5C%5C+dn%2Fdt+%3D+%5Calpha_n%28V_m%29%281-n%29+-+%5Cbeta_n%28V_m%29n%5C%5C+dm%2Fdt+%3D+%5Calpha_m%28V_m%29%281-n%29+-+%5Cbeta_m%28V_m%29n+%5C%5C+dh%2Fdt%3D+%5Calpha_h%28V_m%29%281-h%29+-+%5Cbeta_h%28V_m%29h%5C%5C+%5Clabel%7Beq%3A001%7D+++%5Cend%7Bgather%2A%7D+)

这组微分方程的解就是spiking的过程， 如下图是电压随时间的变化， 当电压积累达到一定阈值， 这个爆发的尖峰就是spike，通过spike ， 神经元可以向其它神经元发射信号。我们所谓的脑电波， 无非是大量这样的神经元的集体放电在颅外所检测到的一组信号。

![](https://pic2.zhimg.com/80/v2-d887fabe6d956a3c9b6949596eb595b1_hd.jpg)

如果用上述这种包含了重要生物细节spiking的神经元连接成网络， 我们就得到了SNN（脉冲神经网络） 也就是受， 无论SNN还是ANN，本质都是对生物神经网络的模拟， 但就其抽象程度且相差疏远。

我们看到用SNN可以用神经脉冲表达信息， 如果用ANN表达一个类似的事情是什么样的呢？ 我们用一个数字Y来表达时间窗的spike个数（频率）， 而丢弃了所有其它信息， 比如波形，相位， 不同神经元之间spike和spike之间的同步等。 这意味着什么？ 两种可能的解释：
- 1， 波形，相位， 不同的神经元之间的同步是没有意义的冗余， 去掉它们整个神经网络表达的信息没有变化， 神经元的系统等于取定时间窗后的平均发放。
- 2， 波形，相位， 不同神经元之间的同步包含很多有用的信息， 去掉它们， 可能丢失了一些关键性的信息。 然而在最粗粒化的信息处理阶段， 这种保留是足够的。

那么哪一个更准确呢？ 普林斯顿的大牛Williams Bialek 的一系列作品都指出， 神经元spike间的同步（相关性）包含和神经编码相关的关键性信息，也就是说除了平均值外， spike所包含的不同神经元之间的发放同步（或相关性）依然包含了大量的信息。

1, Weak pairwise correlations imply strongly correlated network states in a neural population 2006 Nauture [2]

2, Collective Behavior of Place and Non-place Neurons in the Hippocampal Network 2017 Neuron[3]

![](https://pic2.zhimg.com/80/v2-c53bb05a4cfa8510e3572ebde93a346d_hd.jpg)

Weak pairwise correlations imply strongly correlated network states in a neural population 2006 Nauture, 这张图说明了如果用0，1事件表达spike， 那么一个（视网膜网络）里的神经元的同步放电频率远高于用高斯独立假设得到的频率， 也就是说spike之间的同步不可忽略， 构成一种潜在编码
这两部作品的共同特点是说， 神经元spike发放之间的spike correlation可以编码大量的信息， 如果记录这些spike之间的pairwise correlation， 那么我们就可以恢复出神经活动里的大部分有用信息。

这意味着什么？ 假如神经元spike间的同步可以编码信息， 那么我们就可能用更少的spike编码更多的信息， 而这无疑对用最少的神经元放电得到更多的信息（稀疏性）大有帮助。 除此之外， 通过在spiking神经元的那组微分方程里加入更多的核膜常数（代表不同时间尺度的信息， 因为spike方程本身是一个包含大量不同时间尺度的非线性方程），我们可以得到大量局部存储的不同时间尺度的记忆（此处联想“忆阻器”）， 我们甚至可以得到某些类似LSTM非线性门的特性。这些， 都代表着Spiking Neural Network（SNN）相比当下ANN的优势。 用一个不恰当的比喻， ANN的神经元用实数表达每个神经元的状态， 而SNN好比进入到了复数域，有了相位。 在物理领域，实数到复数支撑了从经典力学到量子力学的升级。 有次看， 把SNN看成下一代的神经网络技术不言而喻。当然如果SNN这么好为什么现在工业没有用呢？ 难点在于SNN依赖于对微分方程的模拟， 对于当下的冯诺伊曼结构的计算机， 这是一个成本消耗非常大的运算。也就是说计算机为了模拟本来节省能量的生物计算可能更加耗能，同时也更加不好训练。 解决这个问题的方法， 显然是从基本硬件基础出发，去改良硬件的架构， 这也是神经拟态芯片的意义之所在。我们把树突和轴突直接用芯片来刻画， 无形之间， 就得到了一个长在硬件上的脉冲神经网络（SNN），它的能耗效率要比普通芯片高12-10000倍。

当然ANN也有一类专门的芯片来提高当下深度学习运行的效率，这就是深度学习芯片， 例如大家都了解得寒武纪等。

清华的这个天机芯片在于， 把神经拟态芯片和深度学习芯片得优势结合起来， 可以同时提高这两类神经网络ANN和SNN的效率。 我个人背景不是芯片， 所以此处不在深谈， 我们多从算法角度谈谈两者结合得意义。

![](https://pic3.zhimg.com/80/v2-ef41f7d6e1d84886b2ddda3a7e729346_hd.jpg)

Towards artificial general intelligence with hybrid Tianjic chip architecture
这一次Nature文章里的例子是自动驾驶自行车， 当然这个例子被很多人诟病，认为这个不就是一个简单的平衡游戏吗。 大家可以去github搜索cart pooling或者双足行走，这一类的toy model还不少吗？

然而我认为思考一个新发现的意义不在于它所干的那个任务low不low ， 而是看它是如何完成的。 最初的火车甚至跑不过马车，但是它的架构决定了它的上限和马车不可同日而语， 通过数年时间迭代，两者已是云泥之别。

那么我们来看一下让ANN和SNN同时在一个芯片上运行， 带来的潜力是什么。 一言以蔽之， 当下的深度学习模型，可以和大量没有被好好利用起来的计算神经科学模型， 天衣无缝的嫁接在一起。 这从无人驾驶自行车的网络架构可以略知一二。

![](https://pic3.zhimg.com/80/v2-7faf9a7000d36d819ae6fea27643819a_hd.jpg)

Towards artificial general intelligence with hybrid Tianjic chip architecture
我们来理解一下这个流程图， 首先， 这个架构可以把多模态信息融合。 比如视觉， 听觉。我们注意到， 处理听觉的是脉冲神经网络SNN（更多时间相关信息）。 处理视觉信号的网络是经典的CNN卷积神经网络，属于人工神经网络ANN家族。 然而故事还没有结束， 在CNN的下面， 有一个主管视觉追踪的CANN网络， 虽然只有一个字母之差， 这可不是卷积神经网络， 这四个字母的含义是continous attractor neural networks - 连续吸引子网络。所谓空间吸引子， 说的是一种特化了的循环神经网络， 网络的动力学导致一系列可以根据外界信号连续变化的吸引子构成， 人们通常认为，海马体内的位置细胞就是由这种连续吸引子产生的， 它们可以天然的和速度信号进行耦合， 形成对空间的神经表示， 这个CANN，就是一种连续吸引子网络， 它直接把视觉物体(人)转化为一个可以追踪的空间目标（之后可以用于躲避行人）。 大家注意， 这是一个典型的脱胎于计算神经科学的网络架构，矩阵的连接还用到了树突计算。

然后我们来看中间的那个模块， neural state machine：神经状态机。 这个网络把连续的听觉和视觉信号转化为离散的事件， 这些事件构成一个有限状态的机器，也就是我们通常说的马尔可夫链。 这一步大家已经可以看到和决策有关的网络的联系，因为一旦把连续变化的信号抽象成了这种离散的马尔可夫链， 下一步就可以交给决策网络来决策了， 这里的决策主动是动作输出， 可以控制自行车在保持平衡的同时躲避障碍， 并对周围物体发出警戒信号。 这个网络也是由一个脉冲神经网络SNN构成。

在这里， 我们不难看出这是一个典型的人工设计与机器学习结合的模块化网络， 不能不让我们想起这类工作的先行之作： Science(Eliasmith, Chris, et al. "A large-scale model of the functioning brain."science338.6111 (2012): 1202-1205.) 在这个工作里， 研究人员构建了一个叫spaun的模块化网络， 可以进行多任务学习。

Spaun的每个部分都是一个人工神经网络， 且可以与真实的脑区对应上， 比如视觉输入对应V1-V4 视皮层，它把真实的视觉信息压缩成一种低维度的编码（每个图像称为这一空间的一个点， 被称为pointer）。 这种低维的信息表示形式很容易放入到工作记忆模块里（working memory）， 最终由解码网络转换（decoding）， 被动作输出网络执行（motor）。 神经网络整体状态的调控由模拟basal ganglia的网络完成（Action Selection），它可以根据当下的任务整体调节信息的流动（如同一个综控系统， 调节每个网络之前的输入阀门）， 从而让大脑在不同的工作状态间灵活转换。 这也体现了功能大脑的概念， 我们不必拘泥于某个脑区的名称， 而是记住每个脑区对应信息处理的功能。最终我们通过监督学习或强化学习来让这个系统掌握8种截然不同的任务， 包括： 1， 抄写数字 2， 图像识别 3， 奖励学习， 4， 多个数字的工作记忆 5， 数数 6， 回答问题 7 简单的数学推理。

![](https://pic3.zhimg.com/80/v2-ca6d04c9ec0559dfca62dc7acccd5b2e_hd.jpg)

A large-scale model of the functioning brain

而当下清华的工作， 正是打造了适合这一类执行多任务的“虚拟”生物的硬件系统， 在之上， 你可以自由的搭建无论是经典的深度学习模型， 还是那些超前了的计算神经科学模型， 把他们一起组成模块化的网络， 执行多种多样的功能。

这个潜力也就不只局限在自行车了， 可以是流水线的机器人， 陪护老人的机器人，随便你去发挥想象力，无论上述那个机器人， 都需要进行多模块的信息整合以及多任务执行。假如这种建立在神经网络芯片上的模块化的网络系统可以以较低能耗长时间在真实环境里运作， 那么它带来的好处显然是特别巨大的， 这相当于引入了一个实时不间断的训练数据， 如果结合无监督学习， 强化学习，甚至神经进化等算法实时对网络进行优化，其潜力是无可限量的。

事实上， 类脑计算和AI的结合之潜力此处仅是冰山一角， 在巡洋舰之前的一些文章里，进行了更详尽的论述：
- 许铁-巡洋舰科技：[AI和脑科学是否会再度握手？](https://zhuanlan.zhihu.com/p/53779357)
- 许铁-巡洋舰科技：[深度学习与脑科学的握手-圣城会议归来](https://zhuanlan.zhihu.com/p/55925386)

参考
- https://www.nature.com/articles/s41586-019-1424-8
- https://www.nature.com/articles/nature04701
- https://www.ncbi.nlm.nih.gov/pubmed/29154129

