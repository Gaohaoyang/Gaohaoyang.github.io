---
layout: post
title:  "聚类算法-Cluster Algorithm"
date:   2020-12-26 10:50:00
categories: 机器学习
tags:  无监督学习 聚类 评估 sklearn 文本相似度
excerpt: 机器学习无监督学习中的聚类算法
author: 鹤啸九天
mathjax: true
---

* content
{:toc}

# 总结

- 聚类算法对比总结,[Overview of clustering methods](https://scikit-learn.org/stable/modules/clustering.html)
- ![](https://scikit-learn.org/stable/_images/sphx_glr_plot_cluster_comparison_0011.png)


​ scikit-learn 中聚类算法的比较, 详见官方中文[文档](https://scikit-learn.org.cn/view/108.html)

|Method name（方法名称）|Parameters（参数）|Scalability（可扩展性）|Usecase（使用场景）|Geometry (metric used)（几何图形（公制））|
|---|---|---|---|---|
|K-Means（K-均值）|number of clusters（聚类形成的簇的个数）|非常大的 n_samples, 中等的 n_clusters 使用 MiniBatch 代码）|通用, 均匀的 cluster size（簇大小）, flat geometry（平面几何）, 不是太多的 clusters（簇）|Distances between points（点之间的距离）|
|Affinity propagation|damping（阻尼）, sample preference（样本偏好）|Not scalable with n_samples（n_samples 不可扩展）|Many clusters, uneven cluster size, non-flat geometry（许多簇，不均匀的簇大小，非平面几何）|Graph distance (e.g. nearest-neighbor graph)（图距离（例如，最近邻图））|
|Mean-shift|bandwidth（带宽）|Not scalable with n_samples （n_samples不可扩展）|Many clusters, uneven cluster size, non-flat geometry（许多簇，不均匀的簇大小，非平面几何）|Distances between points（点之间的距离）|
|Spectral clustering|number of clusters（簇的个数）|中等的 n_samples, 小的 n_clusters|Few clusters, even cluster size, non-flat geometry（几个簇，均匀的簇大小，非平面几何）|Graph distance (e.g. nearest-neighbor graph)（图距离（例如最近邻图））|
|Ward hierarchical clustering|number of clusters（簇的个数）|大的 n_samples 和 n_clusters|Many clusters, possibly connectivity constraints（很多的簇，可能连接限制）|Distances between points（点之间的距离）|
|Agglomerative clustering|number of clusters（簇的个数）, linkage type（链接类型）, distance（距离）|大的 n_samples 和 n_clusters|Many clusters, possibly connectivity constraints, non Euclidean distances（很多簇，可能连接限制，非欧氏距离）|Any pairwise distance（任意成对距离）|
|DBSCAN|neighborhood size（neighborhood 的大小）|非常大的 n_samples, 中等的 n_clusters|Non-flat geometry, uneven cluster sizes（非平面几何，不均匀的簇大小）|Distances between nearest points（最近点之间的距离）|
|Gaussian mixtures（高斯混合）|many（很多）|Not scalable（不可扩展）|Flat geometry, good for density estimation（平面几何，适用于密度估计）|Mahalanobis distances to centers（ 与中心的马氏距离）|
|Birch|branching factor（分支因子）, threshold（阈值）, optional global clusterer（可选全局簇）.|大的 n_clusters 和 n_samples|Large dataset, outlier removal, data reduction.（大型数据集，异常值去除，数据简化）||


# 聚类效果评估

【2021-8-18】[聚类算法评估指标](https://www.biaodianfu.com/cluster-score.html)

聚类算法属于非监督学习，并不像分类算法那样可以使用训练集或测试集中得数据计算准确率、召回率等。那如何评估聚类算法得好坏呢？好的聚类算法,一般要求类簇具有：(即**高内聚**、**低耦合**)
- 高的类内 (intra-cluster) 相似度: 通过一个单一的量化得分来评估算法好坏
- 低的类间 (inter-cluster) 相似度: 通过将聚类结果与已经有“ground truth”分类进行对比。要么通过人类进行手动评估，要么通过一些指标在特定的应用场景中进行聚类用法的评估。不过该方法是有问题, 实际应用中往往都没label, 而且这些label只反映了数据集的一个可能的划分方法，它并不能告诉你存在一个不同的更好的聚类算法。

## 内部评价指标

如果聚类结果是类间相似性低，类内相似性高，那么内部评估方法会给予较高的分数评价。

不过内部评价方法的缺点是：
- 那些高分的算法不一定可以适用于高效的信息检索应用场景；
- 这些评估方法对某些算法有倾向性，如k-means聚类都是基于点之间的距离进行优化的，而那些基于距离的内部评估方法就会过度的赞誉这些生成的聚类结果。

内部评估方法可以基于特定场景判定一个算法要优于另一个，不过这并不表示前一个算法得到的结果比后一个结果更有意义。假设这种结构事实上存在于数据集中的，如果一个数据集包含了完全不同的数据结构，或者采用的评价方法完全和算法不搭
- 如k-means只能用于**凸集**数据集上，许多评估指标也是预先假设凸集数据集。在**非凸**数据集上不论是使用k-means还是使用假设凸集的评价方法，都是徒劳的。

### SSE(和方差)

该统计参数计算的是拟合数据和原始数据对应点的误差的平方和，计算公式：SSE=∑(Yi-yi)^2，SSE越接近于0，说明模型选择和拟合更好，数据预测也越成功。
- $\sum_{i=1}^{n} (y_i-\hat{y_i})^{2}$

示例：

```python
#断崖图选取最优K值
import pandas as pd  
from sklearn.cluster import KMeans  
import matplotlib.pyplot as plt  
# '利用SSE选择k'  
SSE = []  # 存放每次结果的误差平方和  
for k in range(1,9):  
    estimator = KMeans(n_clusters=k)  # 构造聚类器  
    estimator.fit(df[['calories','sodium','alcohol','cost']])  
    SSE.append(estimator.inertia_)  
N = range(1,9)  
plt.xlabel('k')  
plt.ylabel('SSE')  
plt.plot(N,SSE,'o-')  
plt.show()
```


### 轮廓系数 Silhouette Coefficient

轮廓系数适用于实际类别信息未知的情况。对于单个样本，设a是与它同类别中其他样本的平均距离，b是与它距离最近不同类别中样本的平均距离，其轮廓系数为:
- $s=\frac{b-a}{max(a,b)}$

- 对于一个样本集合，它的轮廓系数是所有样本轮廓系数的平均值。轮廓系数的取值范围是[-1,1]，同类别样本距离越相近不同类别样本距离越远，分数越高。
- 缺点：不适合基高密度的聚类算法DBSCAN。

```python
from sklearn import metrics
from sklearn.metrics import pairwise_distances
from sklearn import datasets
dataset = datasets.load_iris()
X = dataset.data
y = dataset.target

import numpy as np
from sklearn.cluster import KMeans
kmeans_model = KMeans(n_clusters=3, random_state=1).fit(X)
labels = kmeans_model.labels_
metrics.silhouette_score(X, labels, metric='euclidean')
```

### Calinski-Harabaz Index

在真实的分群label不知道的情况下，Calinski-Harabasz可以作为评估模型的一个指标。Calinski-Harabasz指标通过计算类中各点与类中心的距离平方和来度量类内的紧密度，通过计算各类中心点与数据集中心点距离平方和来度量数据集的分离度，CH指标由分离度与紧密度的比值得到。从而，CH越大代表着类自身越紧密，类与类之间越分散，即更优的聚类结果。
- $s(k)=\frac{tr(B_k)}{tr(W_k)} \frac{m-k}{k-1} $
- 其中m为训练样本数，k是类别个数，$B_k$是类别之间协方差矩阵，$W_k$是类别内部数据协方差矩阵，tr为矩阵的迹。也就是说，类别内部数据的协方差越小越好，类别之间的协方差越大越好，这样的Calinski-Harabasz分数会高。同时，数值越小可以理解为：组间协方差很小，组与组之间界限不明显。

- 优点
  - 当 cluster （簇）密集且分离较好时，分数更高，这与一个标准的 cluster（簇）有关。
  - 得分计算很快与轮廓系数的对比，最大的优势：快！相差几百倍！毫秒级。
- 缺点
  - 凸的簇的 Calinski-Harabaz index（Calinski-Harabaz 指数）通常高于其他类型的 cluster（簇），例如通过 DBSCAN 获得的基于密度的 cluster（簇）。所以不适合基于密度的聚类算法，DBSCAN。

```python
import numpy as np
from sklearn.cluster import KMeans

kmeans_model = KMeans(n_clusters=3, random_state=1).fit(X)
labels = kmeans_model.labels_
print(metrics.calinski_harabaz_score(X, labels))
```


### Compactness(紧密性)(CP)

CP计算每一个类各点到聚类中心的平均距离CP越低意味着类内聚类距离越近。著名的 K-Means 聚类算法就是基于此思想提出的。
- 缺点：没有考虑类间效果。

$\overline{C P}_{i}=\frac{1}{n_{i}} \sum_{x \in C_{i}}\left\|x_{i}-\mu_{i}\right\|$

$\overline{C P}=\frac{1}{k} \sum_{k=1}^{k} \overline{C P}_{k}$

### Separation(间隔性)(SP)

SP计算各聚类中心两两之间平均距离，SP越高意味类间聚类距离越远。缺点：没有考虑类内效果。

$\overline{S P}=\frac{2}{k_{2}-k} \sum_{i=1}^{k} \sum_{j=i+1}^{k}\left\|\mu_{i}-\mu_{j}\right\|$

### Davies-Bouldin Index(戴维森堡丁指数)(分类适确性指标)(DB)(DBI)

DB计算任意两类别的类内距离平均距离(CP)之和除以两聚类中心距离求最大值。DB越小意味着类内距离越小同时类间距离越大。该指标的计算公式：

$\mathrm{DB}=\frac{1}{n} \sum_{i=1}^{n} \max _{j \neq i}\left(\frac{\sigma_{i}+\sigma_{j}}{d\left(c_{i}, c_{j}\right)}\right)$

其中n是类别个数，ci是第i个类别的中心，σi是类别i中所有的点到中心的平均距离；d(ci,cj)中心点ci和cj之间的距离。算法生成的聚类结果越是朝着类内距离最小（类内相似性最大）和类间距离最大（类间相似性最小）变化，那么Davies-Bouldin指数就会越小。
- 缺点：因使用欧式距离所以对于环状分布聚类评测很差。

```python
from sklearn import datasets 
from sklearn.cluster import KMeans 
from sklearn.metrics import davies_bouldin_score 
from sklearn.datasets.samples_generator import make_blobs 

# loading the dataset 
X, y_true = make_blobs(n_samples=300, centers=4,  
                       cluster_std=0.50, random_state=0) 
# K-Means 
kmeans = KMeans(n_clusters=4, random_state=1).fit(X) 
# we store the cluster labels 
labels = kmeans.labels_ 
print(davies_bouldin_score(X, labels))
```

### Dunn Validity Index (邓恩指数)(DVI)

DVI计算任意两个簇元素的最短距离(类间)除以任意簇中的最大距离(类内)。DVI越大意味着类间距离越大同时类内距离越小。
- $D=\frac{\min _{1 \leq i<j \leq n} d(i, j)}{\max _{1 \leq k \leq n} d^{\prime}(k)}$

其中d(i,j)表示类别i,j之间的距离；d′(k)表示类别k内部的类内距离：
- 类间距离d(i,j)可以是任意的距离测度，例如两个类别的中心点的距离；
- 类内距离d′(k)可以以不同的方法去测量，例如类别kk中任意两点之间距离的最大值。
因为内部评估方法是搜寻类内相似最大，类间相似最小，所以算法生成的聚类结果的Dunn指数越高，那么该算法就越好。缺点：对离散点的聚类测评很高、对环状分布测评效果差。

```python
import pandas as pd 
from sklearn import datasets 
from jqmcvi import base 

# loading the dataset 
X = datasets.load_iris() 
df = pd.DataFrame(X.data) 

# K-Means 
from sklearn import cluster 
k_means = cluster.KMeans(n_clusters=3) 
k_means.fit(df) #K-means training 
y_pred = k_means.predict(df) 

# We store the K-means results in a dataframe 
pred = pd.DataFrame(y_pred) 
pred.columns = ['Type'] 

# we merge this dataframe with df 
prediction = pd.concat([df, pred], axis = 1) 

# We store the clusters 
clus0 = prediction.loc[prediction.Species == 0] 
clus1 = prediction.loc[prediction.Species == 1] 
clus2 = prediction.loc[prediction.Species == 2] 
cluster_list = [clus0.values, clus1.values, clus2.values] 

print(base.dunn(cluster_list))
```

## 外部评价指标

外部评估方法中，聚类结果是通过使用没被用来做训练集的数据进行评估。例如已知样本点的类别信息和一些外部的基准。这些基准包含了一些预先分类好的数据，比如由人基于某些场景先生成一些带label的数据，因此这些基准可以看成是金标准。这些评估方法是为了测量聚类结果与提供的基准数据之间的相似性。然而这种方法也被质疑不适用真实数据。

### 纯度（Purity）

纯度（Purity）是一种简单而透明的评估手段，为了计算纯度（Purity），我们把每个簇中最多的类作为这个簇所代表的类，然后计算正确分配的类的数量，然后除以N。形式化表达如下：$\text { purity }(\Omega, \mathbb{C})=\frac{1}{N} \sum_{k} \max _{j}\left|\omega_{k} \cap c_{j}\right|$

其中：
- Ω={w1,w2,…,wk}是聚类的集合，wk表示第k个聚类的集合。
- ℂ={c1,c2,…,cj}是文档集合，cj表示第J个文档。
- N表示文档总数。
上述过程即给每个聚类簇分配一个类别,且这个类别的样本在该簇中出现的次数最多，然后计算所有 K 个聚类簇的这个次数之和再归一化即为最终值。Purity值在0～1之间 ,越接近1表示聚类结果越好。

![](https://www.biaodianfu.com/wp-content/uploads/2020/09/Purity.png)

- 如图认为x代表一类文档，o代表一类文档，方框代表一类文档。如上图的 purity = ( 3+ 4 + 5) / 17 = 0.71，其中第一类正确的有5个，第二个4个，第三个3个，总文档数17。

当簇的数量很多的时候，容易达到较高的纯度——特别是，如果每个文档都被分到独立的一个簇中，那么计算得到的纯度就会是1。因此，不能简单用纯度来衡量聚类质量与聚类数量之间的关系。另外Purity无法用于权衡聚类质量与簇个数之间的关系。

```python
def purity(result, label):
    # 计算纯度

    total_num = len(label)
    cluster_counter = collections.Counter(result)
    original_counter = collections.Counter(label)

    t = []
    for k in cluster_counter:
        p_k = []
        for j in original_counter:
            count = 0
            for i in range(len(result)):
                if result[i] == k and label[i] == j: # 求交集
                    count += 1
            p_k.append(count)
        temp_t = max(p_k)
        t.append(temp_t)
    
    return sum(t)/total_num
```

### 标准化互信息（NMI）

互信息（Normalized Mutual Information）是用来衡量两个数据分布的吻合程度。也是一有用的信息度量，它是指两个事件集合之间的相关性。互信息越大，词条和类别的相关程度也越大。NMI (Normalized Mutual Information) 即归一化互信息
- $\mathrm{NMI}(\Omega, C)=\frac{I(\Omega ; C)}{(H(\Omega)+H(C) / 2)}$
- 其中,I表示互信息(Mutual Information), H为熵，当 log 取 2 为底时，单位为 bit，取 e 为底时单位为 nat。

互信息I(Ω;C)表示给定类簇信息C的前提条件下,类别信息Ω的增加量，或者说其不确定度的减少量。互信息还可以写出如下形式：H(Ω;C)=H(Ω)−H(Ω|C)
- $H(\Omega ; C)=H(\Omega)-H(\Omega \mid C)$
- 互信息的最小值为 0, 当类簇相对于类别只是随机的, 也就是说两者独立的情况下, Ω对于C未带来任何有用的信息.如果得到的Ω与C关系越密切, 那么I(Ω;C)值越大。如果Ω完整重现了C, 此时互信息最大
- 当K=N时,即类簇数和样本个数相等，MI 也能达到最大值。所以 MI 也存在和纯度类似的问题,即它并不对簇数目较大的聚类结果进行惩罚,因此也不能在其他条件一样的情况下,对簇数目越小越好的这种期望进行形式化。NMI 则可以解决上述问题,因为熵会随着簇的数目的增长而增大。当K=N时,H(Ω)会达到其最大值logN , 此时就能保证 NMI 的值较低。之所以采用(H(Ω)+H(C)/2作为分母是因为它是 I(Ω,C)的紧上界, 因此可以保证NMI∈[0 , 1]。计算方法见原文

```python
# sklearn
from sklearn.metrics.cluster import normalized_mutual_info_score
print(normalized_mutual_info_score([0, 0, 1, 1], [0, 0, 1, 1]))

# 不用sklearn

def NMI(result, label):
    # 标准化互信息

    total_num = len(label)
    cluster_counter = collections.Counter(result)
    original_counter = collections.Counter(label)
    
    # 计算互信息量
    MI = 0
    eps = 1.4e-45 # 取一个很小的值来避免log 0
    
    for k in cluster_counter:
        for j in original_counter:
            count = 0
            for i in range(len(result)):
                if result[i] == k and label[i] == j:
                    count += 1
            p_k = 1.0*cluster_counter[k] / total_num
            p_j = 1.0*original_counter[j] / total_num
            p_kj = 1.0*count / total_num
            MI += p_kj * math.log(p_kj /(p_k * p_j) + eps, 2)
    
    # 标准化互信息量
    H_k = 0
    for k in cluster_counter:
        H_k -= (1.0*cluster_counter[k] / total_num) * math.log(1.0*cluster_counter[k] / total_num+eps, 2)
    H_j = 0
    for j in original_counter:
        H_j -= (1.0*original_counter[j] / total_num) * math.log(1.0*original_counter[j] / total_num+eps, 2)
        
    return 2.0 * MI / (H_k + H_j)
```

### 调整互信息AMI（ Adjusted mutual information）

已知聚类标签与真实标签，互信息（mutual information）能够测度两种标签排列之间的相关性，同时忽略标签中的排列。有两种不同版本的互信息以供选择，一种是Normalized Mutual Information（NMI）,一种是Adjusted Mutual Information（AMI）

优点
- 随机（统一）标签分配的AMI评分接近0）
- 有界范围 [0, 1]: 接近 0 的值表示两个主要独立的标签分配，而接近 1 的值表示重要的一致性。此外，正好 0 的值表示 purely（纯粹） 独立标签分配，正好为 1 的 AMI 表示两个标签分配相等（有或者没有 permutation）。
- 对簇的结构没有作出任何假设: 可以用于比较聚类算法

缺点：
- 与 inertia 相反， MI-based measures 需要了解 ground truth classes，而在实践中几乎不可用，或者需要人工标注或手动分配（如在监督学习环境中）。然而，基于 MI-based measures （基于 MI 的测量方式）也可用于纯无人监控的设置，作为可用于聚类模型选择的 Consensus Index （共识索引）的构建块。
- NMI 和 MI 没有调整机会。

```python
from sklearn import metrics
labels_true = [0, 0, 0, 1, 1, 1]
labels_pred = [0, 0, 1, 1, 2, 2]

print(metrics.adjusted_mutual_info_score(labels_true, labels_pred))
```

### Rand index兰德指数

兰德指数 (Rand index, RI), 将聚类看成是一系列的决策过程,即对文档集上所有N(N-1)/2个文档 (documents) 对进行决策。当且仅当两篇文档相似时,我们将它们归入同一簇中。

Positive:
- TP 将两篇相似文档归入一个簇 (同 – 同)
- TN 将两篇不相似的文档归入不同的簇 (不同 – 不同)
Negative:
- FP 将两篇不相似的文档归入同一簇 (不同 – 同)
- FN 将两篇相似的文档归入不同簇 (同- 不同) (worse)
RI 则是计算「正确决策」的比率(精确率, accuracy)：
- $R I=\frac{T P+T N}{T P+F P+T F+F N}=\frac{T P+T N}{C_{N}^{2}}$

RI取值范围为[0,1]，值越大意味着聚类结果与真实情况越吻合。

```python
def contingency_table(result, label):
    
    total_num = len(label)
    
    TP = TN = FP = FN = 0
    for i in range(total_num):
        for j in range(i + 1, total_num):
            if label[i] == label[j] and result[i] == result[j]:
                TP += 1
            elif label[i] != label[j] and result[i] != result[j]:
                TN += 1
            elif label[i] != label[j] and result[i] == result[j]:
                FP += 1
            elif label[i] == label[j] and result[i] != result[j]:
                FN += 1
    return (TP, TN, FP, FN)

def rand_index(result, label):
    TP, TN, FP, FN = contingency_table(result, label)
    return 1.0*(TP + TN)/(TP + FP + FN + TN)
```

### 调整兰德系数 （Adjusted Rand index）

对于随机结果，RI并不能保证分数接近零。为了实现“在聚类结果随机产生的情况下，指标应该接近零”，调整兰德系数（Adjusted rand index）被提出，它具有更高的区分度：
- $\mathrm{ARI}=\frac{\mathrm{RI}-E[\mathrm{RI}]}{\max (\mathrm{RI})-E[\mathrm{RI}]}$
ARI取值范围为[-1,1]，值越大意味着聚类结果与真实情况越吻合。从广义的角度来讲，ARI衡量的是两个数据分布的吻合程度。

优点：
- 对任意数量的聚类中心和样本数，随机聚类的ARI都非常接近于0
- 取值在［－1，1］之间，负数代表结果不好，越接近于1越好
- 对簇的结构不需作出任何假设：可以用于比较聚类算法。
缺点：
- 与 inertia 相反，ARI 需要 ground truth classes 的相关知识，ARI需要真实标签，而在实践中几乎不可用，或者需要人工标注者手动分配（如在监督学习环境中）。然而，ARI 还可以在 purely unsupervised setting （纯粹无监督的设置中）作为可用于 聚类模型选择（TODO）的共识索引的构建块。

```python
from sklearn import metrics
labels_true = [0, 0, 0, 1, 1, 1]
labels_pred = [0, 0, 1, 1, 2, 2]

print(metrics.adjusted_rand_score(labels_true, labels_pred)) 
```

### F值方法

这是基于上述RI方法衍生出的一个方法，我们可以 FN 罚更多,通过取Fβ中的β大于 1, 此时实际上也相当于赋予召回率更大的权重：

$$\begin{array}{c}
\text { Precision }=P=\frac{T P}{T P+F P} \\
\text { Recall }=R=\frac{T P}{T P+F N} \\
F_{\beta}=\frac{\left(1+\beta^{2}\right) P \times R}{\beta^{2} P+R}
\end{array}$$

RI方法有个特点就是把准确率和召回率看得同等重要，事实上有时候我们可能需要某一特性更多一点，这时候就适合F值方法。

```python
def precision(result, label):
    TP, TN, FP, FN = contingency_table(result, label)
    return 1.0*TP/(TP + FP)

def recall(result, label):
    TP, TN, FP, FN = contingency_table(result, label)
    return 1.0*TP/(TP + FN)

def F_measure(result, label, beta=1):
    prec = precision(result, label)
    r = recall(result, label)
    return (beta*beta + 1) * prec * r/(beta*beta * prec + r)
```

### Fowlkes-Mallows scores

Fowlkes-Mallows Scores（FMI） FMI是成对的precision（精度）和recall（召回）的几何平均数。取值范围为 [0,1]，越接近1越好。定义为：
- $\mathrm{FMI}=\frac{T P}{\sqrt{(T P+F P)(T P+F N)}}$

```python
from sklearn import metrics
labels_true = [0, 0, 0, 1, 1, 1]
labels_pred = [0, 0, 1, 1, 2, 2]

print(metrics.fowlkes_mallows_score(labels_true, labels_pred))
```

### 调和平均V-measure

说V-measure之前要先介绍两个指标：
- 同质性（homogeneity）：每个群集只包含单个类的成员。
- 完整性（completeness）：给定类的所有成员都分配给同一个群集。
同质性和完整性分数基于以下公式得出（见原文）

V-measure是同质性homogeneity和完整性completeness的调和平均数，V-measure取值范围为 [0,1]，越大越好，但当样本量较小或聚类数据较多的情况，推荐使用AMI和ARI。公式：$v=2 \cdot \frac{h \cdot c}{h+c}$

优点：
- 分数明确：从0到1反应出最差到最优的表现；
- 解释直观：差的调和平均数可以在同质性和完整性方面做定性的分析；
- 对簇结构不作假设：可以比较两种聚类算法如k均值算法和谱聚类算法的结果。
缺点：
- 以前引入的度量在随机标记方面没有规范化，这意味着，根据样本数，集群和先验知识，完全随机标签并不总是产生相同的完整性和均匀性的值，所得调和平均值V-measure也不相同。特别是，随机标记不会产生零分，特别是当簇的数量很大时。
- 当样本数大于一千，聚类数小于10时，可以安全地忽略该问题。对于较小的样本量或更大数量的集群，使用经过调整的指数（如调整兰德指数）更为安全。
- 这些指标要求的先验知识，在实践中几乎不可用或需要手动分配的人作注解者（如在监督学习环境中）。

```python
from sklearn import metrics

labels_true = [0, 0, 0, 1, 1, 1]
labels_pred = [0, 0, 1, 1, 2, 2]

print(metrics.homogeneity_score(labels_true, labels_pred))
print(metrics.completeness_score(labels_true, labels_pred))
print(metrics.v_measure_score(labels_true, labels_pred))
```

### Jaccard 指数

该指数用于量化两个数据集之间的相似性，该值得范围为0-1.其中越大表明两个数据集越相似：
- $J(A, B)=\frac{|A \cap B|}{|A \cup B|}=\frac{T P}{T P+F P+F N}$
该指数和近年来的IOU计算方法一致

### Dice 指数

该指数是基于jaccard指数上将TP的权重置为2倍。
- $J(A, B)=\frac{|A \cap B|}{|A \cup B|}=\frac{2 T P}{2 T P+F P+F N}$

# sklearn聚类算法

- 参考：
  - [sklearn聚类算法详解](https://blog.csdn.net/ustbbsy/article/details/80960652)
  - [scikit-learn中的无监督聚类算法](https://www.cnblogs.com/xc-family/p/11006525.html)

- scikit-learn主要由分类、回归、聚类和降维四大部分组成，其中分类和回归属于有监督学习范畴，聚类属于无监督学习范畴，降维适用于有监督学习和无监督学习。scikit-learn的结构示意图如下所示：
- ![](https://img2018.cnblogs.com/blog/1286380/201906/1286380-20190611222013552-2012482171.png)

- scikit-learn中的聚类算法主要有：
  - K-Means(cluster.KMeans)
  - AP聚类（cluster.AffinityPropagation）
  - 均值漂移（cluster.MeanShift）
  - 层次聚类（cluster.AgglomerativeClustering）
  - DBSCAN（cluster.DBSCAN）
  - BRICH（cluster.Brich）
  - 谱聚类（cluster.Spectral.Clustering）
  - 高斯混合模型（GMM）∈期望最大化（EM）算法（mixture.GaussianMixture）

## 1 KMeans
 
### 1.1 算法描述

1.  随机选择k个中心
2.  遍历所有样本，把样本划分到距离最近的一个中心
3.  划分之后就有K个簇，计算每个簇的平均值作为新的质心
4.  重复步骤2，直到达到停止条件
    
 
- 停止条件：
  - 聚类中心不再发生变化；所有的距离最小；迭代次数达到设定值，
- 代价函数：误差平方和（SSE）
 
![](https://img-blog.csdn.net/20180724155815525?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3VzdGJic3k=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
 
### 1.2 算法优缺点
 
优点：
*   算法容易理解，聚类效果不错
*   具有出色的速度
*   当簇近似高斯分布时，效果比较好

缺点：
*   需要自己确定K值,k值的选定是比较难确定
*   对初始中心点敏感
*   不适合发现非凸形状的簇或者大小差别较大的簇
*   特殊值/离群值对模型的影响比较大
*    从数据先验的角度来说，在 Kmeans 中,我们假设各个 cluster 的先验概率是一样的,但是各个 cluster 的数据量可能是不均匀的。举个例子,cluster A 中包含了10000个样本,cluster B 中只包含了100个。那么对于一个新的样本,在不考虑其与A cluster、 B cluster 相似度的情况,其属于 cluster A 的概率肯定是要大于 cluster B的。

 
### 1.3 效果评价
 
- 从簇内的**稠密**程度和簇间的**离散**程度来评估聚类的效果。
- 常见的方法有轮廓系数`Silhouette Coefficient`和`Calinski-Harabasz Index`
 
#### 1.3.1 **轮廓系数**
 
- **轮廓系数**（Silhouette Coefficient）结合了聚类的**凝聚度**（Cohesion）和**分离度**（Separation），用于评估聚类的效果。该值处于-1~1之间，值越大，表示聚类效果越好。具体计算方法如下：
1.  对于第i个元素$x_i$，计算$x_i$与其同一个簇内的所有其他元素距离的平均值，记作$a_i$，用于量化簇内的凝聚度。
2.  选取$x_i$外的一个簇b，计算$x_i$与b中所有点的平均距离，遍历所有其他簇，找到最近的这个平均距离,记作$b_i$，用于量化簇之间分离度。
3.  对于元素$x_i$，轮廓系数$s_i = (b_i – a_i)/max(a_i,b_i)$
4.  计算所有x的轮廓系数，求出平均值即为当前聚类的整体轮廓系数
 
先是计算每一个样本的轮廓系数，然后计算所有样本的轮廓系数，求平均值作为整体轮廓系数
 
从上面的公式，不难发现若s_i小于0，a_i  > b_i, 说明x_i与其簇内元素的平均距离大于最近的其他簇，表示聚类效果不好。如果a_i趋于0，或者b_i足够大，那么s_i趋近与1，说明聚类效果比较好。

#### 1.3.2 Calinski-Harabasz Index
 
聚成几个类别合适？
 
![](https://img-blog.csdn.net/2018072416434555?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3VzdGJic3k=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
 
类别内部数据的**协方差**越小越好，类别之间的协方差越大越好，这样的Calinski-Harabasz分数会高。   
在scikit-learn中， Calinski-Harabasz Index对应的方法是metrics.calinski_harabaz_score。
 
```python
import numpy as np
from sklearn.cluster import KMeans
# 用kmeans自带的
kmeans_model = KMeans(n_clusters=3, random_state=1).fit(X)
labels = kmeans_model.labels_metrics.calinski_harabaz_score(X, labels)
# 货使用Calinski-Harabasz Index评估的聚类分数: 分数越高，表示聚类的效果越好
from sklearn.metrics import calinski_harabaz_score
print(calinski_harabaz_score(X, y_pre)) # 3088.084577541466
```
 
参考博客：[KMeans](http://www.cnblogs.com/pinard/p/6169370.html)
 
### 1.4 K值确定
 
*   结合业务分析，确定需要分类的个数，这种情况往往有业务上聚类的个数范围
*   手肘原则，选定不同的K值，计算每个k值时的代价函数。Kmeans聚类的效果评估方法是SSE，是计算所有点到相应簇中心的距离均值，当然，k值越大 SSE越小，我们就是要求出随着k值的变化SSE的变化规律，找到SSE减幅最小的k值，这时k应该是相对比较合理的值。
 
![](https://img-blog.csdn.net/20180724152054465?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3VzdGJic3k=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
 
如图，在k=3之后，代价函数变化缓慢，选择聚类的个数为3


- 算法特点
  - Distances between points（点之间的距离）
- 优点：
  - 算法容易理解，聚类效果不错
  - 具有出色的速度：O(NKt)
  - 当簇近似高斯分布时，效果比较好
- 缺点：
  - 需要人工预先确定初试K值，且该值和真是的数据分布未必吻合
  - 对初始中心点敏感
  - 不适合发现非凸形状的簇或者大小差别较大的簇
  - 特殊值/离散值（噪点）对模型的影响比较大
  - 算法只能收敛到局部最优，效果受初始值影响很大
  - 从数据先验的角度来说，在 Kmeans 中,我们假设各个 cluster 的先验概率是一样的,但是各个 cluster 的数据量可能是不均匀的。举个例子,cluster A 中包含了10000个样本,cluster B 中只包含了100个。那么对于一个新的样本,在不考虑其与A cluster、 B cluster 相似度的情况,其属于 cluster A 的概率肯定是要大于 cluster B的。
- 适用场景
  - 通用, 均匀的 cluster size（簇大小）, flat geometry（平面几何）, 不是太多的 clusters（簇）
  - 非常大的 n_samples、中等的 n_clusters 使用 MiniBatch code
  - 样本量<10K时使用k-means，>=10K时用MiniBatchKMeans
  - 不太适用于离散分类

- 代码

```python
print(__doc__)

# Author: Phil Roth <mr.phil.roth@gmail.com>
# License: BSD 3 clause

import numpy as np
import matplotlib.pyplot as plt

from sklearn.cluster import KMeans
from sklearn.datasets import make_blobs

plt.figure(figsize=(12, 12))

n_samples = 1500
random_state = 170
X, y = make_blobs(n_samples=n_samples, random_state=random_state)

# Incorrect number of clusters
y_pred = KMeans(n_clusters=2, random_state=random_state).fit_predict(X)

plt.subplot(221)
plt.scatter(X[:, 0], X[:, 1], c=y_pred)
plt.title("Incorrect Number of Blobs")

# Anisotropicly distributed data
transformation = [[0.60834549, -0.63667341], [-0.40887718, 0.85253229]]
X_aniso = np.dot(X, transformation)
y_pred = KMeans(n_clusters=3, random_state=random_state).fit_predict(X_aniso)

plt.subplot(222)
plt.scatter(X_aniso[:, 0], X_aniso[:, 1], c=y_pred)
plt.title("Anisotropicly Distributed Blobs")

# Different variance
X_varied, y_varied = make_blobs(n_samples=n_samples,
                                cluster_std=[1.0, 2.5, 0.5],
                                random_state=random_state)
y_pred = KMeans(n_clusters=3, random_state=random_state).fit_predict(X_varied)

plt.subplot(223)
plt.scatter(X_varied[:, 0], X_varied[:, 1], c=y_pred)
plt.title("Unequal Variance")

# Unevenly sized blobs
X_filtered = np.vstack((X[y == 0][:500], X[y == 1][:100], X[y == 2][:10]))
y_pred = KMeans(n_clusters=3,
                random_state=random_state).fit_predict(X_filtered)

plt.subplot(224)
plt.scatter(X_filtered[:, 0], X_filtered[:, 1], c=y_pred)
plt.title("Unevenly Sized Blobs")

plt.show()
```

 
## 2 DBSCAN
 
- DBSCAN（Density-Based Spatial Clustering of Application with Noise）基于密度的空间聚类算法。
- 两个参数：
  - * Eps邻域半径(epsilon,小量，小的值）  
  - * MinPts(minimum number of points required to form a cluster)定义核心点时的阈值。
 
3个点
* **核心点**：对应稠密区域内部的点
* **边界点**：对应稠密区域边缘的点
* **噪音点**：对应稀疏区域中的点
 
![](https://img-blog.csdn.net/20180708170337156?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3VzdGJic3k=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
 
上图红色为核心点，黄色为边界点，蓝色为噪音点
 
几个概念：
* **Ε邻域**：给定对象半径为Ε内的区域称为该对象的Ε邻域；
* **核心对象**：对于任一样本点，如果其Eps邻域内至少包含MinPts个样本点，那么这个样本点就是核心对象。（一个点）
* **密度可达**：对于样本集合D，给定一串样本点p1,p2….pn，p= p1,q= pn,假如对象pi从pi-1直接密度可达，那么对象q从对象p密度可达。
* **密度相连**：存在样本集合D中的一点o，如果对象o到对象p和对象q都是密度可达的，那么p和q密度相联。
* **直接密度可达**：对于样本集合D，如果样本点q在p的Ε邻域内，并且p为核心对象，那么对象q从对象p直接密度可达。
 
可以发现，**密度可达**是直接密度可达的**传递闭包**，并且这种关系是非对称的。密度相连是**对称**关系。DBSCAN目的是找到密度相连对象的最大集合。
- Eg: 假设半径Ε=3，MinPts=3，点p的E邻域中有点{m,p,p1,p2,o}, 点m的E邻域中有点{m,q,p,m1,m2},点q的E邻域中有点{q,m},点o的E邻域中有点{o,p,s},点s的E邻域中有点{o,s,s1}.
- 那么核心对象有p,m,o,s(q不是核心对象，因为它对应的E邻域中点数量等于2，小于MinPts=3)；
- 点m从点p直接密度可达，因为m在p的E邻域内，并且p为核心对象；
- 点q从点p密度可达，因为点q从点m直接密度可达，并且点m从点p直接密度可达；
- 点q到点s密度相连，因为点q从点p密度可达，并且s从点p密度可达。

- DBSCAN的聚类是一个不断**生长**的过程。先找到一个核心对象，从整个核心对象出发，找出它的直接密度可达的对象，再从这些对象出发，寻找它们直接密度可达的对象，一直重复这个过程，直至最后没有可寻找的对象了，那么一个簇的更新就完成了。也可以认为，簇是所有密度可达的点的集合。
- DBSCAN核心思想：从某个选定的**核心点**出发，不断向密度可达的区域扩张，从而得到一个包含**核心点**和**边界点**的最大化区域，区域中任意两点密度相连。
 
优点：
*   不需要指定cluster的数目，如K-means方法需要提前指定k（或多次运行，通过肘部法则获取k）
*   聚类的形状可以是**任意**的
*   能找出数据中的**噪音**，对噪音不敏感
*   算法应用参数少，只需要两个：Ε 和 MinPts
*   聚类结果几乎不依赖于节点的遍历顺序，即对样本顺序不敏感，对于处于簇类之间边界样本，可能会根据哪个簇类优先被探测到而其归属有所摆动。

缺点：
*   如果样本集较大时，聚类收敛时间较长，此时可以对搜索最近邻时建立的**KD树**或者**球树**进行规模限制来改进。
*   DBSCAN算法的聚类效果依赖于**距离公式**的选取，实际中常用的距离是欧几里得距离，由于‘`维数灾难`’，距离的度量标准已变得不再重要。
  - 分类器的性能随着特征数量的增加而不断提升，但过了某一值后，性能不升反而下降，这种现象称为维数灾难。对于维度灾难的理解：[维度灾难的理解](https://blog.csdn.net/ustbbsy/article/details/80961157)
*   不适合数据集中**密度差异很大**的情形，因为这种情形，参数Eps,MinPts不好选择。
  - 如果是密度大的，选一个小的邻域半径就可以把这些数据点聚类，但对于那些密度小的数据点，你设置的小的邻域半径，并不能把密度小的这些点给全部聚类。
 
聚类形状可以是任意的，来个图直观感觉一下：
 
![](https://img-blog.csdn.net/20180708172210797?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3VzdGJic3k=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
 
在sklearn中的应用
 
```python
from sklearn.cluster import DBSCAN
DBSCAN(eps=0.5,  # 邻域半径
  min_samples=5,    # 最小样本点数，MinPts
  metric='euclidean', # 可以自定义函数
  metric_params=None,
  algorithm='auto', # 'auto','ball_tree','kd_tree','brute',4个可选的参数 寻找最近邻点的算法，例如直接密度可达的点
  leaf_size=30, # balltree,cdtree的参数
  p=None, # 
  n_jobs=1)
```

- 完整版代码

```python
from sklearn.datasets.samples_generator import make_blobs
from sklearn.cluster import DBSCAN
import numpy as np
import matplotlib.pyplot as plt
from itertools import cycle  ##python自带的迭代器模块
from sklearn.preprocessing import StandardScaler

##产生随机数据的中心
centers = [[1, 1], [-1, -1], [1, -1]]
##产生的数据个数
n_samples=750
##生产数据:此实验结果受cluster_std的影响，或者说受eps 和cluster_std差值影响
X, lables_true = make_blobs(n_samples=n_samples, centers= centers, cluster_std=0.4,
                  random_state =0)


##设置分层聚类函数
db = DBSCAN(eps=0.3, min_samples=10)
# 自定义函数
#DB = skc.DBSCAN(eps=400,min_samples=min_samples_num, metric=lambda a, b: haversine(a,b)).fit(X)
##训练数据
db.fit(X)
##初始化一个全是False的bool类型的数组
core_samples_mask = np.zeros_like(db.labels_, dtype=bool)
'''
   这里是关键点(针对这行代码：xy = X[class_member_mask & ~core_samples_mask])：
   db.core_sample_indices_  表示的是某个点在寻找核心点集合的过程中暂时被标为噪声点的点(即周围点
   小于min_samples)，并不是最终的噪声点。在对核心点进行联通的过程中，这部分点会被进行重新归类(即标签
   并不会是表示噪声点的-1)，也可也这样理解，这些点不适合做核心点，但是会被包含在某个核心点的范围之内
'''
core_samples_mask[db.core_sample_indices_] = True

##每个数据的分类
lables = db.labels_

##分类个数：lables中包含-1，表示噪声点
n_clusters_ =len(np.unique(lables)) - (1 if -1 in lables else 0)

##绘图
unique_labels = set(lables)
'''
   1)np.linspace 返回[0,1]之间的len(unique_labels) 个数
   2)plt.cm 一个颜色映射模块
   3)生成的每个colors包含4个值，分别是rgba
   4)其实这行代码的意思就是生成4个可以和光谱对应的颜色值
'''
colors = plt.cm.Spectral(np.linspace(0, 1, len(unique_labels)))

plt.figure(1)
plt.clf()


for k, col in zip(unique_labels, colors):
    ##-1表示噪声点,这里的k表示黑色
    if k == -1:
        col = 'k'

    ##生成一个True、False数组，lables == k 的设置成True
    class_member_mask = (lables == k)

    ##两个数组做&运算，找出即是核心点又等于分类k的值  markeredgecolor='k',
    xy = X[class_member_mask & core_samples_mask]
    plt.plot(xy[:, 0], xy[:, 1], 'o', c=col,markersize=14)
    '''
       1)~优先级最高，按位对core_samples_mask 求反，求出的是噪音点的位置
       2)& 于运算之后，求出虽然刚开始是噪音点的位置，但是重新归类却属于k的点
       3)对核心分类之后进行的扩展
    '''
    xy = X[class_member_mask & ~core_samples_mask]
    plt.plot(xy[:, 0], xy[:, 1], 'o', c=col,markersize=6)

plt.title('Estimated number of clusters: %d' % n_clusters_)
plt.show()
```
 
## 3 OPTICS
 
- 是基于密度的聚类算法，OPTICS（Ordering Point To Idenfy the Cluster Structure）,不显式地生成数据聚类，只是对数据对象集合中的对象进行排序，得到一个有序的对象列表。
* 核心距离（core-distance）
给定参数eps，MinPts，使得某个样本点成为核心对象（核心点）的最小邻域半径，这个最小邻域半径为该样本点的核心距离。
 
在DBSCAN中，给定领域半径eps和MinPts可以确定一个核心对象，如果eps比较大，在核心对象的邻域半径eps内的点的总数就会大于你所设定的MinPts，所以核心距离就是一个核心点在满足MinPts时的一个最小邻域半径。
 
* 可达距离（reachability-distance）
    
 
rd(y,x)表示使得‘x为核心点’且‘y从x直接密度可达’同时成立的最小邻域半径。
 
[参考资料](https://blog.csdn.net/itplus/article/details/10089323)
 

 
## 4 Spectral Clustering 谱聚类
 
### 1）概述
- Spectral Clustering(SC,即谱聚类)，是一种基于图论的聚类方法,它能够识别任意形状的样本空间且收敛于全局最有解，其基本思想是利用样本数据的相似矩阵进行特征分解后得到的特征向量进行聚类.它与样本特征无关而只与样本个数有关。
- 基本思路：将样本看作顶点,样本间的相似度看作带权的边,从而将聚类问题转为图分割问题:找到一种图分割的方法使得连接不同组的边的权重尽可能低(这意味着组间相似度要尽可能低),组内的边的权重尽可能高(这意味着组内相似度要尽可能高).
 
### 2）图解过程
 
![](https://images2015.cnblogs.com/blog/1119747/201706/1119747-20170608142735434-772124776.png)
 
如上图所示，断开虚线，六个数据被聚成两类。
 
### 3）Spectral Clustering算法函数
 
a）核心函数：sklearn.cluster.SpectralClustering
 
因为是基于图论的算法，所以输入必须是对称矩阵。
 
b）主要参数(参数较多，[详细参数](http://scikitlearn.org/dev/modules/generated/sklearn.cluster.SpectralClustering.html#sklearn.cluster.SpectralClustering))
  - n_clusters：聚类的个数。（官方的解释：投影子空间的维度）
  - affinity：核函数，默认是'rbf'，可选："nearest\_neighbors"，"precomputed","rbf"或sklearn.metrics.pairwise\_kernels支持的其中一个 - 内核之一。
  - gamma :affinity指定的核函数的内核系数，默认1.0

c）主要属性
- labels_ ：每个数据的分类标签
 
- 算法特点
  - Graph distance (e.g. nearest-neighbor graph)（图形距离（例如最近邻图））
- 适用场景
  - 几个簇，均匀的簇大小，非平面几何
  - 中等的 n_samples, 小的 n_clusters
- 代码

```python
from sklearn.datasets.samples_generator import make_blobs
from sklearn.cluster import spectral_clustering
import numpy as np
import matplotlib.pyplot as plt
from sklearn import metrics
from itertools import cycle  ##python自带的迭代器模块

##产生随机数据的中心
centers = [[1, 1], [-1, -1], [1, -1]]
##产生的数据个数
n_samples=3000
##生产数据
X, lables_true = make_blobs(n_samples=n_samples, centers= centers, cluster_std=0.6,
                  random_state =0)

##变换成矩阵，输入必须是对称矩阵
metrics_metrix = (-1 * metrics.pairwise.pairwise_distances(X)).astype(np.int32)
metrics_metrix += -1 * metrics_metrix.min()
##设置谱聚类函数
n_clusters_= 4
lables = spectral_clustering(metrics_metrix,n_clusters=n_clusters_)

##绘图
plt.figure(1)
plt.clf()

colors = cycle('bgrcmykbgrcmykbgrcmykbgrcmyk')
for k, col in zip(range(n_clusters_), colors):
    ##根据lables中的值是否等于k，重新组成一个True、False的数组
    my_members = lables == k
    ##X[my_members, 0] 取出my_members对应位置为True的值的横坐标
    plt.plot(X[my_members, 0], X[my_members, 1], col + '.')

plt.title('Estimated number of clusters: %d' % n_clusters_)
plt.show()
```



## 5 Hierarchical Clustering 层次聚类

层次聚类（hierarchical clustering）可在不同层次上对数据集进行划分，形成树状的聚类结构。AggregativeClustering是一种常用的层次聚类算法。

其原理是：最初将每个对象看成一个簇，然后将这些簇根据某种规则被一步步合并，就这样不断合并直到达到预设的簇类个数。
- 这里的关键在于：如何计算聚类簇之间的距离？
- ①**最小**距离：单链接single-linkage算法，两个簇的样本对之间距离的最小值
- ②**最大**距离：单链接complete-linkage算法，两个簇的样本对之间距离的最大值
- ③**平均**距离：单链接average-linkage算法，两个簇的样本对之间距离的平均值
 
1）概述
- Hierarchical Clustering(层次聚类)：就是按照某种方法进行层次分类，直到满足某种条件为止。
- 主要分成两类：
  - a）**凝聚**：从下到上。首先将每个对象作为一个簇，然后合并这些原子簇为越来越大的簇，直到所有的对象都在一个簇中，或者某个终结条件被满足。
  - b）**分裂**：从上到下。首先将所有对象置于同一个簇中，然后逐渐细分为越来越小的簇，直到每个对象自成一簇，或者达到了某个终止条件。（较少用）
 
2）算法步骤（凝聚）
- a）将每个对象归为一类, 共得到N类, 每类仅包含一个对象. 类与类之间的距离就是它们所包含的对象之间的距离.
- b）找到最接近的两个类并合并成一类, 于是总的类数少了一个.
- c）重新计算新的类与所有旧类之间的距离.  
- d）重复第2步和第3步, 直到最后合并成一个类为止(此类包含了N个对象).
 
3）图解过程
- ![](https://images2015.cnblogs.com/blog/1119747/201706/1119747-20170608152215168-1660153616.png)

4）Hierarchical Clustering算法函数
- a）sklearn.cluster.**AgglomerativeClustering**
- b）主要参数([详细参数](http://scikit-learn.org/dev/modules/generated/sklearn.cluster.AgglomerativeClustering.html#sklearn.cluster.AgglomerativeClustering))
  - n_clusters：一个整数，指定分类簇的数量, None时表示不限类目，此时需要threshlod > 0
  - connectivity：一个数组或者可调用对象或者None，用于指定连接矩阵
  - affinity：一个字符串或者可调用对象，用于计算距离。可以为：’euclidean’，’l1’，’l2’，’mantattan’，’cosine’，’precomputed’，如果 linkage=’ward’，则affinity必须为’euclidean’
  - memory：用于缓存输出的结果，默认为不缓存
  - n_components：在 v-0.18中移除
  - compute_full_tree：通常当训练了n_clusters后，训练过程就会停止，但是如果compute_full_tree=True，则会继续训练从而生成一颗完整的树
  - linkage：一个字符串，用于指定链接算法, 指定层次聚类判断相似度的方法，有三种：
    - ‘ward’：组间距离等于两类对象之间的最小距离, **单链接** single-linkage，采用dmin
    - ‘complete’：组间距离等于两组对象之间的最大距离, **全链接** complete-linkage算法，采用dmax
    - ‘average’：组间距离等于两组对象之间的平均距离, **均连接** average-linkage算法，采用davg
  - pooling_func：一个可调用对象，它的输入是一组特征的值，输出是一个数
- c）主要属性
  - labels_： 每个数据的分类标签
  - n_leaves_：分层树的叶节点数量
  - n_components：连接图中连通分量的估计值
  - children：一个数组，给出了每个非节点数量

参考资料 [聚类算法](http://www.cnblogs.com/lc1217/p/6963687.html)

```python

from sklearn import cluster
from sklearn.metrics import adjusted_rand_score
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets.samples_generator import make_blobs

"""
    产生数据
"""
def create_data(centers,num=100,std=0.7):
    X,labels_true = make_blobs(n_samples=num,centers=centers, cluster_std=std)
    return X,labels_true

"""
    数据作图
"""
def plot_data(*data):
    X,labels_true=data
    labels=np.unique(labels_true)
    fig=plt.figure()
    ax=fig.add_subplot(1,1,1)
    colors='rgbycm'
    for i,label in enumerate(labels):
        position=labels_true==label
        ax.scatter(X[position,0],X[position,1],label="cluster %d"%label),
        color=colors[i%len(colors)]

    ax.legend(loc="best",framealpha=0.5)
    ax.set_xlabel("X[0]")
    ax.set_ylabel("Y[1]")
    ax.set_title("data")
    plt.show()

"""
    测试函数
"""  
def test_AgglomerativeClustering(*data):
    X,labels_true=data
    clst=cluster.AgglomerativeClustering()
    predicted_labels=clst.fit_predict(X)
    print("ARI:%s"% adjusted_rand_score(labels_true, predicted_labels))

"""
    考察簇的数量对于聚类效果的影响
"""
def test_AgglomerativeClustering_nclusters(*data):
    X,labels_true=data
    nums=range(1,50)
    ARIS=[]
    for num in nums:
        clst=cluster.AgglomerativeClustering(n_clusters=num)
        predicted_lables=clst.fit_predict(X)
        ARIS.append(adjusted_rand_score(labels_true, predicted_lables)) 

    fig=plt.figure()
    ax=fig.add_subplot(1,1,1)
    ax.plot(nums,ARIS,marker="+")
    ax.set_xlabel("n_clusters")
    ax.set_ylabel("ARI")
    fig.suptitle("AgglomerativeClustering")
    plt.show()   

"""
    考察链接方式对聚类结果的影响
"""   
def test_agglomerativeClustering_linkage(*data):
    X,labels_true=data
    nums=range(1,50)
    fig=plt.figure()
    ax=fig.add_subplot(1,1,1)
    linkages=['ward','complete','average']
    markers="+o*"
    for i,linkage in enumerate(linkages): 
        ARIs=[]
        for num in nums:
            clst=cluster.AgglomerativeClustering(n_clusters=num,linkage=linkage)
            predicted_labels=clst.fit_predict(X)
            ARIs.append(adjusted_rand_score(labels_true, predicted_labels))
        ax.plot(nums,ARIs,marker=markers[i],label="linkage:%s"%linkage)

    ax.set_xlabel("n_clusters")
    ax.set_ylabel("ARI")
    ax.legend(loc="best")
    fig.suptitle("AgglomerativeClustering")
    plt.show()
centers=[[1,1],[2,2],[1,2],[10,20]]
X,labels_true=create_data(centers, 1000, 0.5)
test_AgglomerativeClustering(X,labels_true)
plot_data(X,labels_true)
test_AgglomerativeClustering_nclusters(X,labels_true)
test_agglomerativeClustering_linkage(X,labels_true)

```

当n_clusters=4时，ARI指数最大，因为确实是从四个中心点产生的四个簇。三种链接方式随分类簇的数量的总体趋势相差无几。但是单链接方式ward的峰值最大，且峰值最大的分类簇的数量刚好等于实际生成的簇的数量。
- ![](https://img-blog.csdn.net/20170915221725257)


- 算法特点
  - Distances between points（点之间的距离）
- 适用场景
  - 很多的簇，可能连接限制
  - 大的 n_samples 和 n_clusters

### 代码

```python
# Authors: Gael Varoquaux
# License: BSD 3 clause (C) INRIA 2014

print(__doc__)
from time import time

import numpy as np
from scipy import ndimage
from matplotlib import pyplot as plt

from sklearn import manifold, datasets

digits = datasets.load_digits(n_class=10)
X = digits.data
y = digits.target
n_samples, n_features = X.shape

np.random.seed(0)

def nudge_images(X, y):
    # Having a larger dataset shows more clearly the behavior of the
    # methods, but we multiply the size of the dataset only by 2, as the
    # cost of the hierarchical clustering methods are strongly
    # super-linear in n_samples
    shift = lambda x: ndimage.shift(x.reshape((8, 8)),
                                  .3 * np.random.normal(size=2),
                                  mode='constant',
                                  ).ravel()
    X = np.concatenate([X, np.apply_along_axis(shift, 1, X)])
    Y = np.concatenate([y, y], axis=0)
    return X, Y

X, y = nudge_images(X, y)
#----------------------------------------------------------------------
# Visualize the clustering
def plot_clustering(X_red, labels, title=None):
    x_min, x_max = np.min(X_red, axis=0), np.max(X_red, axis=0)
    X_red = (X_red - x_min) / (x_max - x_min)
    plt.figure(figsize=(6, 4))
    for i in range(X_red.shape[0]):
        plt.text(X_red[i, 0], X_red[i, 1], str(y[i]),
                 color=plt.cm.nipy_spectral(labels[i] / 10.),
                 fontdict={'weight': 'bold', 'size': 9})
    plt.xticks([])
    plt.yticks([])
    if title is not None:
        plt.title(title, size=17)
    plt.axis('off')
    plt.tight_layout(rect=[0, 0.03, 1, 0.95])

#----------------------------------------------------------------------
# 2D embedding of the digits dataset
print("Computing embedding")
X_red = manifold.SpectralEmbedding(n_components=2).fit_transform(X)
print("Done.")

from sklearn.cluster import AgglomerativeClustering

for linkage in ('ward', 'average', 'complete', 'single'):
    clustering = AgglomerativeClustering(linkage=linkage, n_clusters=10)
    t0 = time()
    clustering.fit(X_red)
    print("%s :\t%.2fs" % (linkage, time() - t0))
    plot_clustering(X_red, clustering.labels_, "%s linkage" % linkage)
plt.show()
```

## AP聚类（近邻传播）

### 2.1 简介
 
- **AP**(Affinity Propagation)通常被翻译为**近邻**传播算法或者**亲和力**传播算法，是在2007年的Science杂志上提出的一种新的聚类算法。AP算法的基本思想是将全部数据点都当作潜在的聚类中心(称之为exemplar)，然后数据点两两之间连线构成一个网络(相似度矩阵)，再通过网络中各条边的消息(responsibility和availability)传递计算出各样本的聚类中心。
 
### 2.2 算法原理
 
概念：
*   **吸引度（responsibility）矩阵R：**其中r(i,k)描述了数据对象k适合作为数据对象i的聚类中心的程度，表示的是从i到k的消息。
*   **归属度（availability）矩阵A：**其中a(i,k)描述了数据对象i选择数据对象k作为其据聚类中心的适合程度，表示从k到i的消息。
*   **相似度（similarity）矩阵S：**通常S（i，j）取i，j的欧氏距离的负值，当i=j时，通常取整个矩阵的最小值或者中位数(Scikit-learn中默认为中位数)，取得值越大则最终产生的类数量越多。
    
 
算法步骤
1. 算法初始，吸引度矩阵和归属度矩阵均初始化为0矩阵。
2. 更新吸引度矩阵
  - ![](https://img2018.cnblogs.com/blog/1286380/201906/1286380-20190611232607528-1884635591.png)
3. 更新归属度矩阵
  - ![](https://img2018.cnblogs.com/blog/1286380/201906/1286380-20190611232654242-523365032.png)
4. 根据衰减系数λ对两个公式进行衰减
  - ![](https://img2018.cnblogs.com/blog/1286380/201906/1286380-20190611232707915-1899619062.png)
5. 重复步骤2/3/4直至矩阵稳定或者达到最大迭代次数，算法结束。最终取a+r最大的k作为聚类中心。
 
### 2.3 算法特点
--------
*   Graph distance (e.g. nearest-neighbor graph)（图形距离（例如，最近邻图））

- 优点：
  *   与其他聚类算法不同,AP聚类不需要指定K(经典的K-Means)或者是其他描述聚类个数的参数
  *   一个聚类中最具代表性的点在AP算法中叫做Examplar,与其他算法中的聚类中心不同,examplar是原始数据中确切存在的一个数据点,而不是由多个 数据点求平均而得到的聚类中心
  *   多次执行AP聚类算法,得到的结果完全一样的，即不需要进行随机选取初值步骤.
  *   AP算法相对于Kmeans优势是不需要指定聚类数量,对初始值不敏感
  *   模型对数据的初始值不敏感。
  *   对初始相似度矩阵数据的对称性没有要求。
  *   相比与k-centers聚类方法，其结果的平方差误差较小。
- 缺点：
  *   AP算法需要事先计算每对数据对象之间的相似度，如果数据对象太多的话，内存放不下，若存在数据库，频繁访问数据库也需要时间。
  *   AP算法的时间复杂度较高，一次迭代大概O(N3)
  *   聚类的好坏受到参考度和阻尼系数的影响
    
 
### 2.4 适用场景
--------
*   许多簇，不均匀的簇大小，非平面几何
*   不可扩展的 n_samples
*   特别适合高维、多类数据快速聚类
*   图像、文本、生物信息学、人脸识别、基因发现、搜索最优航线、 码书设计以及实物图像识别等领域

### 代码

```python
# -*- coding:utf-8 -*-

import numpy as np
import matplotlib.pyplot as plt
import sklearn.datasets as ds
import matplotlib.colors
from sklearn.cluster import AffinityPropagation
from sklearn.metrics import euclidean_distances

#聚类算法之AP算法:
#1--与其他聚类算法不同,AP聚类不需要指定K(金典的K-Means)或者是其他描述聚类个数的参数
#2--一个聚类中最具代表性的点在AP算法中叫做Examplar,与其他算法中的聚类中心不同,examplar
#是原始数据中确切存在的一个数据点,而不是由多个数据点求平均而得到的聚类中心
#3--多次执行AP聚类算法,得到的结果完全一样的，即不需要进行随机选取初值步骤.
#算法复杂度较高,为O(N*N*logN),而K-Means只是O(N*K)的复杂度，当N》3000时,需要算很久
#AP算法相对于Kmeans优势是不需要指定聚类数量,对初始值不敏感

#AP算法应用场景：图像、文本、生物信息学、人脸识别、基因发现、搜索最优航线、 码书设计以及实物图像识别等领域

#算法详解: http://blog.csdn.net/helloeveryon/article/details/51259459

if __name__=='__main__':
    #scikit中的make_blobs方法常被用来生成聚类算法的测试数据，直观地说，make_blobs会根据用户指定的特征数量、
    # 中心点数量、范围等来生成几类数据，这些数据可用于测试聚类算法的效果。
    #函数原型：sklearn.datasets.make_blobs(n_samples=100, n_features=2,
    # centers=3, cluster_std=1.0, center_box=(-10.0, 10.0), shuffle=True, random_state=None)[source]
    #参数解析：
    # n_samples是待生成的样本的总数。
    #
    # n_features是每个样本的特征数。
    #
    # centers表示类别数。
    #
    # cluster_std表示每个类别的方差，例如我们希望生成2类数据，其中一类比另一类具有更大的方差，可以将cluster_std设置为[1.0, 3.0]。

    N=400
    centers = [[1, 2], [-1, -1], [1, -1], [-1, 1]]
    #生成聚类算法的测试数据
    data,y=ds.make_blobs(N,n_features=2,centers=centers,cluster_std=[0.5, 0.25, 0.7, 0.5],random_state=0)
    #计算向量之间的距离
    m=euclidean_distances(data,squared=True)
    #求中位数
    preference=-np.median(m)
    print 'Preference:',preference

    matplotlib.rcParams['font.sans-serif'] = [u'SimHei']
    matplotlib.rcParams['axes.unicode_minus'] = False
    plt.figure(figsize=(12,9),facecolor='w')
    for i,mul in enumerate(np.linspace(1,4,9)):#遍历等差数列
        print 'mul=',mul
        p=mul*preference
        model=AffinityPropagation(affinity='euclidean',preference=p)
        af=model.fit(data)
        center_indices=af.cluster_centers_indices_
        n_clusters=len(center_indices)
        print ('p=%.1f'%mul),p,'聚类簇的个数为:',n_clusters
        y_hat=af.labels_

        plt.subplot(3,3,i+1)
        plt.title(u'Preference：%.2f，簇个数：%d' % (p, n_clusters))
        clrs=[]
        for c in np.linspace(16711680, 255, n_clusters):
            clrs.append('#%06x' % c)
            for k, clr in enumerate(clrs):
                cur = (y_hat == k)
                plt.scatter(data[cur, 0], data[cur, 1], c=clr, edgecolors='none')
                center = data[center_indices[k]]
                for x in data[cur]:
                    plt.plot([x[0], center[0]], [x[1], center[1]], color=clr, zorder=1)
            plt.scatter(data[center_indices, 0], data[center_indices, 1], s=100, c=clrs, marker='*', edgecolors='k',
                        zorder=2)
            plt.grid(True)
        plt.tight_layout()
        plt.suptitle(u'AP聚类', fontsize=20)
        plt.subplots_adjust(top=0.92)
        plt.show()
```


 
## 6 Mean-shift 均值迁移
 
### 1)概述

- Mean-shift（即：均值迁移）的基本思想：在数据集中选定一个点，然后以这个点为圆心，r为半径，画一个圆(二维下是圆)，求出这个点到所有点的向量的平均值，而圆心与向量均值的和为新的圆心，然后迭代此过程，直到满足一点的条件结束。(Fukunage在1975年提出)
- 后来Yizong Cheng 在此基础上加入了 核函数 和 权重系数 ，使得Mean-shift 算法开始流行起来。目前它在聚类、图像平滑、分割、跟踪等方面有着广泛的应用。
 
### 2）图解过程
 
为了方便大家理解，借用下几张图来说明Mean-shift的基本过程。
 
![](https://images2015.cnblogs.com/blog/1119747/201706/1119747-20170608134949465-1606232390.png)
 
由上图可以很容易看到，Mean-shift 算法的核心思想就是不断的寻找新的圆心坐标，直到密度最大的区域。
 
### 3）Mean-shift 算法函数
 
a）核心函数：sklearn.cluster.MeanShift(核函数：RBF核函数)
由上图可知，圆心(或种子)的确定和半径(或带宽)的选择，是影响算法效率的两个主要因素。所以在sklearn.cluster.MeanShift中重点说明了这两个参数的设定问题。
 
b）主要参数
- bandwidth ：半径(或带宽)，float型。如果没有给出，则使用sklearn.cluster.estimate_bandwidth计算出半径(带宽).（可选）
- seeds :圆心（或种子），数组类型，即初始化的圆心。（可选）
- bin_seeding ：布尔值。如果为真，初始内核位置不是所有点的位置，而是点的离散版本的位置，其中点被分类到其粗糙度对应于带宽的网格上。将此选项设置为True将加速算法，因为较少的种子将被初始化。默认值：False.如果种子参数(seeds)不为None则忽略。
 
c）主要属性
- cluster\_centers\_ : 数组类型。计算出的聚类中心的坐标。
- labels_ :数组类型。每个数据点的分类标签。、

- 算法特点
  - Distances between points（点之间的距离）
  - 圆心(或种子)的确定和半径(或带宽)的选择，是影响算法效率的两个主要因素。
  - 该算法不是高度可扩展的，因为在执行算法期间需要执行多个最近邻搜索。
  - 该算法保证收敛，但是当质心的变化较小时，算法将停止迭代。
  - 通过找到给定样本的最近质心来给新样本打上标签。
- 适用场景
  - 许多簇，不均匀的簇大小，非平面几何
  - 不可扩展的 n_samples
  - 适用于类别数量未知，且样本数量<10K
  - 目前它在聚类、图像平滑、分割、跟踪等方面有着广泛的应用。

### 代码

```python
print(__doc__)

import numpy as np
from sklearn.cluster import MeanShift, estimate_bandwidth
from sklearn.datasets.samples_generator import make_blobs

# #############################################################################
# Generate sample data
centers = [[1, 1], [-1, -1], [1, -1]]
X, _ = make_blobs(n_samples=10000, centers=centers, cluster_std=0.6)

# #############################################################################
# Compute clustering with MeanShift

# The following bandwidth can be automatically detected using
bandwidth = estimate_bandwidth(X, quantile=0.2, n_samples=500)

ms = MeanShift(bandwidth=bandwidth, bin_seeding=True)
ms.fit(X)
labels = ms.labels_
cluster_centers = ms.cluster_centers_

labels_unique = np.unique(labels)
n_clusters_ = len(labels_unique)

print("number of estimated clusters : %d" % n_clusters_)

# #############################################################################
# Plot result
import matplotlib.pyplot as plt
from itertools import cycle

plt.figure(1)
plt.clf()

colors = cycle('bgrcmykbgrcmykbgrcmykbgrcmyk')
for k, col in zip(range(n_clusters_), colors):
    my_members = labels == k
    cluster_center = cluster_centers[k]
    plt.plot(X[my_members, 0], X[my_members, 1], col + '.')
    plt.plot(cluster_center[0], cluster_center[1], 'o', markerfacecolor=col,
             markeredgecolor='k', markersize=14)
plt.title('Estimated number of clusters: %d' % n_clusters_)
plt.show()
```



## 7 BIRCH 
========
 
- [BIRCH](https://www.cnblogs.com/pinard/p/6179132.html)

### 简介

- Birch(利用层次方法的平衡迭代规约和聚类)：就是通过聚类特征(CF)形成一个聚类特征树，root层的CF个数就是聚类个数。

### 算法原理

- 相关概念
  - 聚类特征(CF)：每一个CF是一个三元组,可以用（N，LS，SS）表示.其中N代表了这个CF中拥有的样本点的数量;LS代表了这个CF中拥有的样本点各特征维度的和向量,SS代表了这个CF中拥有的样本点各特征维度的平方和。
  - 如上图所示：N = 5 
  - LS=(3+2+4+4+3,4+6+5+7+8)=(16,30)
  - SS =(32+22+42+42+32,42+62+52+72+82)=(54,190)
  - ![](https://img2018.cnblogs.com/blog/1286380/201906/1286380-20190612003329358-1354759305.png)
- 聚类过程
  - 对于上图中的CF Tree,限定了B=7,L=5， 也就是说内部节点最多有7个CF(CF90下的圆),而叶子节点最多有5个CF(CF90到CF94)。叶子节点是通过双向链表连通的。
  - ![](https://img2018.cnblogs.com/blog/1286380/201906/1286380-20190612003526254-1167462718.png)
- 特点
  - Euclidean distance between points（点之间的欧式距离）
- 适用场景
  - 大数据集，异常值去除，数据简化
  - 大的 n_clusters 和 n_samples

- 完整版代码

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets.samples_generator import make_blobs
from sklearn.cluster import Birch

# X为样本特征，Y为样本簇类别， 共1000个样本，每个样本2个特征，共4个簇，簇中心在[-1,-1], [0,0],[1,1], [2,2]
X, y = make_blobs(n_samples=1000, n_features=2, centers=[[-1,-1], [0,0], [1,1], [2,2]], cluster_std=[0.4, 0.3, 0.4, 0.3],
                  random_state =9)

##设置birch函数
birch = Birch(n_clusters = None)
##训练数据
y_pred = birch.fit_predict(X)
##绘图
plt.scatter(X[:, 0], X[:, 1], c=y_pred)
plt.show()
```
 
## 8 GaussianMixtureModel(混合高斯模型，GMM)

- 正太分布也叫高斯分布，正太分布的概率密度曲线也叫高斯分布概率曲线
 
### 1）介绍
 
- 聚类算法大多数通过相似度来判断，而相似度又大多采用**欧式距离**长短作为衡量依据。
- 而GMM采用了新的判断依据：**概率**，即通过属于某一类的概率大小来判断最终的归属类别。
- GMM的基本思想就是：**任意形状的概率分布都可以用多个高斯分布函数去近似**，也就是说GMM就是有多个单高斯密度分布（Gaussian）组成的，每个Gaussian叫一个"Component"，这些"Component"线性加成在一起就组成了 GMM 的概率密度函数，也就是下面的函数。
 
### 2）原理

- 高斯混合模型是由K个高斯分布（正态分布）函数组成，而该算法的目的就是找出各个高斯分布最佳的均值、方差、权重。
- ![](https://img2018.cnblogs.com/blog/1286380/201906/1286380-20190612010058732-915728099.png)
  - 指定K的值，并初始随机选择各参数的值
  - E步骤。根据当前的参数，计算每个点由某个分模型生成的概率
  - M步骤。根据E步骤估计出的概率，来改进每个分模型的均值、方差和权重
  - 重复步骤2、3，直至收敛。

![](https://images2015.cnblogs.com/blog/1119747/201706/1119747-20170612143601915-492097161.png)
 
列出来公式只是方便理解下面的函数中为什么需要那些参数。
K：模型的个数，即Component的个数（聚类的个数）
![](https://images2015.cnblogs.com/blog/1119747/201706/1119747-20170612145911306-173735240.png)_为第k个高斯的权重_
 
p（x |k） 则为第k个高斯概率密度,其均值为μk,方差为σk
上述参数，除了K是直接给定之外，其他参数都是通过EM算法估算出来的。(有个参数是指定EM算法参数的)
 
### 3）GaussianMixtureModel 算法函数
 
a）from sklearn.mixture.GaussianMixture
b）主要参数（[详细参数](http://scikit-learn.org/dev/modules/generated/sklearn.mixture.GaussianMixture.html#sklearn.mixture.GaussianMixture)）
- n_components ：高斯模型的个数，即聚类的目标个数
- covariance_type : 通过EM算法估算参数时使用的协方差类型，默认是"full"
- full：每个模型使用自己的一般协方差矩阵
- tied：所用模型共享一个一般协方差矩阵
- diag：每个模型使用自己的对角线协方差矩阵
- spherical：每个模型使用自己的单一方差


- 算法特点
  - Mahalanobis distances to centers（Mahalanobis 与中心的距离）
- 优点
  - 可以给出一个样本属于某类的概率是多少
  - 不仅用于聚类，还可用于概率密度的估计
  - 可以用于生成新的样本点
- 缺点
  - 需要指定K值
  - 使用EM算法来求解
  - 往往只能收敛于局部最优
- 适用场景
  - 平面几何，适用于密度估计
  - Not scalable（不可扩展）

- 代码

```python
import matplotlib as mpl
import matplotlib.pyplot as plt

import numpy as np

from sklearn import datasets
from sklearn.mixture import GaussianMixture
from sklearn.model_selection import StratifiedKFold

print(__doc__)

colors = ['navy', 'turquoise', 'darkorange']


def make_ellipses(gmm, ax):
    for n, color in enumerate(colors):
        if gmm.covariance_type == 'full':
            covariances = gmm.covariances_[n][:2, :2]
        elif gmm.covariance_type == 'tied':
            covariances = gmm.covariances_[:2, :2]
        elif gmm.covariance_type == 'diag':
            covariances = np.diag(gmm.covariances_[n][:2])
        elif gmm.covariance_type == 'spherical':
            covariances = np.eye(gmm.means_.shape[1]) * gmm.covariances_[n]
        v, w = np.linalg.eigh(covariances)
        u = w[0] / np.linalg.norm(w[0])
        angle = np.arctan2(u[1], u[0])
        angle = 180 * angle / np.pi  # convert to degrees
        v = 2. * np.sqrt(2.) * np.sqrt(v)
        ell = mpl.patches.Ellipse(gmm.means_[n, :2], v[0], v[1],
                                  180 + angle, color=color)
        ell.set_clip_box(ax.bbox)
        ell.set_alpha(0.5)
        ax.add_artist(ell)
        ax.set_aspect('equal', 'datalim')

iris = datasets.load_iris()

# Break up the dataset into non-overlapping training (75%) and testing
# (25%) sets.
skf = StratifiedKFold(n_splits=4)
# Only take the first fold.
train_index, test_index = next(iter(skf.split(iris.data, iris.target)))


X_train = iris.data[train_index]
y_train = iris.target[train_index]
X_test = iris.data[test_index]
y_test = iris.target[test_index]

n_classes = len(np.unique(y_train))

# Try GMMs using different types of covariances.
estimators = {cov_type: GaussianMixture(n_components=n_classes,
              covariance_type=cov_type, max_iter=20, random_state=0)
              for cov_type in ['spherical', 'diag', 'tied', 'full']}

n_estimators = len(estimators)

plt.figure(figsize=(3 * n_estimators // 2, 6))
plt.subplots_adjust(bottom=.01, top=0.95, hspace=.15, wspace=.05,
                    left=.01, right=.99)


for index, (name, estimator) in enumerate(estimators.items()):
    # Since we have class labels for the training data, we can
    # initialize the GMM parameters in a supervised manner.
    estimator.means_init = np.array([X_train[y_train == i].mean(axis=0)
                                    for i in range(n_classes)])

    # Train the other parameters using the EM algorithm.
    estimator.fit(X_train)

    h = plt.subplot(2, n_estimators // 2, index + 1)
    make_ellipses(estimator, h)

    for n, color in enumerate(colors):
        data = iris.data[iris.target == n]
        plt.scatter(data[:, 0], data[:, 1], s=0.8, color=color,
                    label=iris.target_names[n])
    # Plot the test data with crosses
    for n, color in enumerate(colors):
        data = X_test[y_test == n]
        plt.scatter(data[:, 0], data[:, 1], marker='x', color=color)

    y_train_pred = estimator.predict(X_train)
    train_accuracy = np.mean(y_train_pred.ravel() == y_train.ravel()) * 100
    plt.text(0.05, 0.9, 'Train accuracy: %.1f' % train_accuracy,
             transform=h.transAxes)

    y_test_pred = estimator.predict(X_test)
    test_accuracy = np.mean(y_test_pred.ravel() == y_test.ravel()) * 100
    plt.text(0.05, 0.8, 'Test accuracy: %.1f' % test_accuracy,
             transform=h.transAxes)

    plt.xticks(())
    plt.yticks(())
    plt.title(name)

plt.legend(scatterpoints=1, loc='lower right', prop=dict(size=12))

plt.show()
```


# 案例


## 文本聚类

### 字符串相似度

字符串的相似性比较应用场合很多，像拼写**纠错**、文本**去重**、上下文**相似性**等。

评价字符串相似度最常见的办法就是：把一个字符串通过插入、删除或替换这样的编辑操作，变成另外一个字符串，所需要的最少编辑次数，这种就是**编辑距离**（edit distance）度量方法，也称为**Levenshtein距离**。海明距离是编辑距离的一种特殊情况，只计算**等长**情况下替换操作的编辑次数，只能应用于两个等长字符串间的距离度量。

其他常用的度量方法还有：`Jaccard` distance、`J-W`距离（Jaro–Winkler distance）、`余弦`相似性（cosine similarity）、`欧氏`距离（Euclidean distance）等。

### 代码

【2021-11-8】[Python Levenshtein 计算文本之间的距离](https://blog.csdn.net/u014657795/article/details/90476489)
- pip install difflib
- pip install python-Levenshtein

```python
import difflib

query_str = '市公安局'
s1 = '广州市邮政局'
s2 = '广州市公安局'
s3 = '广州市检查院'

seq = difflib.SequenceMatcher(None, s1, s2)
ratio = seq.ratio()
print 'difflib similarity1: ', ratio

# difflib 去掉列表中不需要比较的字符
seq = difflib.SequenceMatcher(lambda x: x in ' 我的雪', str1,str2)
ratio = seq.ratio()
print 'difflib similarity2: ', ratio

print(difflib.SequenceMatcher(None, query_str, s1).quick_ratio())  
print(difflib.SequenceMatcher(None, query_str, s2).quick_ratio())  
print(difflib.SequenceMatcher(None, query_str, s3).quick_ratio())  
# 0.4
# 0.8 --> 某一种相似度评判标准下的最相似的文本……
# 0.08695652173913043

import Levenshtein
# (1) 汉明距离，str1和str2必须长度一致。是描述两个等长字串之间对应位置上不同字符的个数
Levenshtein.hamming('hello', 'world') # 4
Levenshtein.hamming('abc', 'abd') # 1
# (2) 编辑距离（也成Levenshtein距离）。是描述由一个字串转化成另一个字串最少的操作次数，在其中的操作包括插入、删除、替换
Levenshtein.distance('hello', 'world') # 4
Levenshtein.distance('abc', 'abd') # 1
Levenshtein.distance('abc', 'aecfaf') # 4
# (3) 莱文斯坦比。计算公式 r = (sum - ldist) / sum, 其中sum是指str1 和 str2 字串的长度总和，ldist是类编辑距离
# 注意：这里的类编辑距离不是2中所说的编辑距离，2中三种操作中每个操作+1，而在此处，删除、插入依然+1，但是替换+2
# 这样设计的目的：ratio('a', 'c')，sum=2,按2中计算为（2-1）/2 = 0.5,’a’,'c’没有重合，显然不合算，但是替换操作+2，就可以解决这个问题。
Levenshtein.ratio('hello', 'world') # 0.2
Levenshtein.ratio('abc', 'abd') # 0.6666666666666666
Levenshtein.ratio('abc', 'aecfaf') # 0.4444444444444444
# (4) jaro距离
Levenshtein.jaro('hello', 'world') # 0.43333333333333335
Levenshtein.jaro('abc', 'abd') # 0.7777777777777777
Levenshtein.jaro('abc', 'aecfaf') # 0.6666666666666666
# (5) Jaro–Winkler距离
Levenshtein.jaro_winkler('hello', 'world') # 0.43333333333333335
Levenshtein.jaro_winkler('abc', 'abd') # 0.8222222222222222
Levenshtein.jaro_winkler('abc', 'aecfaf') # 0.7


```



## pyecharts可视化

将高维向量通过 t-sne降维，再用pyecharts（1.*以上版本）可视化出来
- t-sne降维

```python
# 原始数据文件格式：[query, 0.23, 0.43, ..., 1.23], 将query嵌入到768的高维空间里
data_file = 'newhouse/all_data_embedding.txt'
#query_vec = pd.read_csv(data_file)
#query_vec
num = 0
n = 769
query_dict = {}
for line in open(data_file):
    num += 1
    #if num > 50:
    #    break
    arr = line.strip().split(',')
    query= arr[0]
    vec = arr[1:]
    if len(arr) != 769 or not query:
        logging.error('格式异常: query={}, len={}, line={}'.format(query,len(arr),arr[:3]))
        continue
    query_dict[query] = vec
query_list = list(query_dict.values())
query_label = list(query_dict.keys())
print(len(query_dict.keys()))

# ----- t-SNE 降维 -----
import numpy as np 
from sklearn.manifold import TSNE

X = np.array(query_list)
#X = np.array([[0,0,0],[0,1,1],[1,0,1],[1,1,1]])
tsne = TSNE(n_components = 3)
tsne.fit_transform(X)
X_new = tsne.embedding_ # 降维后的3维矩阵
print(query_label[10],X_new[10]) # 输出label、降维后的向量

# ----- pca ------
from sklearn.decomposition import PCA

pca=PCA(n_components=3)

```
- 数据加载、可视化

```python

import random
from pyecharts import options as  opts
from pyecharts.charts import Scatter3D
from pyecharts.faker import Faker

# --------- 加载数据 ---------
vec_tsne = np.load('/home/wangqiwen004/work/nlu_data/newhouse/vec_tsne.npy')
vec_label = np.load('/home/wangqiwen004/work/nlu_data/newhouse/vec_label.npy')
vec_label = vec_label.reshape((-1,1)) # label是1维时，需要转换成矩阵，才能拼接
print(vec_label.shape,vec_tsne.shape)
vec_tsne[:3].tolist()
np.hstack((vec_tsne[:3], vec_label[:3]))
# --------- 数据格式化 ---------
#Scatter_data = [(random.randint(0,50),random.randint(0,50),random.randint(0,50)) for i in range(50)]
#Scatter_data = vec_tsne[:10].tolist()
N = 50000
Scatter_data = np.hstack((vec_tsne, vec_label))[:N].tolist()

# --------- 绘图 ---------
c = (
    Scatter3D(init_opts = opts.InitOpts(width='1500px',height='900px'))  #初始化
    .add("句子向量（t-sne）",Scatter_data,
         grid3d_opts=opts.Grid3DOpts(
            width=100, depth=100, rotate_speed=20, is_rotate=False
        ))
    #设置全局配置项
    .set_global_opts(
        title_opts=opts.TitleOpts(title="新房驻场客服query分布（部分N={}）".format(min(N,vec_label.shape[0]))),  #添加标题
        visualmap_opts=opts.VisualMapOpts(
            max_=50, #最大值
            pos_top=50, # visualMap 组件离容器上侧的距离
            range_color=Faker.visual_color  #颜色映射                                         
        )
    )
)
c.render("新房驻场客服-query空间关系.html")
#c.render_notebook() # 渲染到jupyter notebook页面
```



# 结束


