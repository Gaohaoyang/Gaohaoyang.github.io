---
layout: post
title:  "面试指南-The Guide for Interview"
date:   2018-10-31 20:08:00
categories: 人生规划
excerpt : AI初学者不要过于迷信企业项目，忽略基本功的学习，不要mnist都没研究透彻就嫌简单，直奔难度更大的resnet，好高骛远，得不偿失，学东西请务必戒骄戒躁，一步一个脚印，才能越走越远。企业项目的事实跟你想象的大不一样
tags: 实习 应届生 AI 算法工程师 面试
---

* content
{:toc}


# 面试


## 企业需求

企业招聘要求
- 为公司解决问题，带来收益 ＞ 个人学习成长 
- 帮上司排忧解难，超出预期的解决问题

企业需要什么样的人
- 【2018-8-22】这里的企业特指互联网企业，算法研发岗
- 【2018-10-16】阿里不是很重视校招，喜欢捡现成的，即便是校招，要求也是很高，2015年阿里校招面试，我面了两天，8+9=17人，都是清北中之类的，硕士以上，通过率不到30%，当时的标准是：大公司实习经历+学校学历好+理论基础好+动手能力强，只要有一个面试官犹豫不决，就否了。
- 【2020-5-21】面试本身就是不公平的，40-50分钟内，面试官试图通过几个点(问题)来感知候选人全方位的能力，这本身就不容易，关键这几个点还是面试官在自己熟悉的领域里挑选的（甚至提前准备好答案），不能答错，也不能答不出来，或者牵强附会，更不能指出面试官的错误，还要适当主导面试进程，不停地展示自己的技能，难…不同面试可能会出现截然相反的评论。所以，通过与否一定程度上靠缘分…

## 优秀候选人

如何成为那20%？
- 二八定律无处不在，20% 的候选人占据了市场上 80% 的offer，反之，另外 80% 的候选人只能苦哈哈的陷入那 20% offer中竞争

如何成为20%？

个人经验：
1. 代码算法：
  - C字符串函数
  - 基本算法（如快排等，需要熟练掌握）
  - 剑指Offer：面试经常出相似的题，类似书籍《编程珠玑》、《编程之美》、《程序员面试宝典》
  - LeetCode（增强动手能力）【2019-06-06】[程序员小吴](https://www.cxyxiaowu.com/)的[图解Leetcode](https://github.com/MisterBooo/LeetCodeAnimation)
1. 计算机功底
  - 熟练使用Linux开发环境：shell+vim+awk+git
  - 操作系统、web（浏览器工作原理）、cpu、内存
  - 数学基础：概率、线代、优化、微积分等
1. 数据挖掘（特征工程）
  - excel、awk、python
  - Hadoop/Hive/Spark
  - [教你如何迅速秒杀掉：99%的海量数据处理面试题](https://link.zhihu.com/?target=http%3A//blog.csdn.net/v_july_v/article/details/7382693)。（基本每次都有一道海量数据处理的面试题）
  - 分析技巧：趋势、比较、细分，参考：[数据分析的三个常用方法：数据趋势、对比和细分分析](http://www.woshipm.com/data-analysis/676038.html)
  - 注：数据分析没有想象的那么容易，那么低级，占据了算法工程师高达70-80%的时间
  - 要点：<font color='red'>知其然，知其所以然</font>
    - 深入理解业务，培养数据敏感度，见微知著
    - 不要只关心算法，不理业务！
    - 开阔视野，与时俱进，复合人才：[人人都是产品经理](http://www.woshipm.com/)、极客公园、36Kr、雷锋、机器之心等
1. 机器学习：
  - 李航《统计学习方法》（读3遍都不为过）
  - Coursera Stanford《Machine Learning》（讲得很基础，但是没有告诉你所以然）
  - Coursera 台湾大学《机器学习高级技法》、李宏毅的深度学习等（里面详解了SVM，Ensemble等模型的推导，优劣）
  - 此处资料太多。。。
1. 项目经验
  - 请详细地回忆自己做过的项目
    - 项目用了什么算法，为什么用它，有什么优缺点等。
    - 注意：逻辑严密、思路清晰
1. 技术影响力
  - 技术竞赛：阿里天池、Kaggle、KDD等
  - 顶会论文
  - 书籍出版
  - 最好有自己的github，有自己的项目（不只是clone），并且定期更新

面试技巧
- 自信、淡定：一紧张就会漏洞百出，发挥失常 —— “不就一次面试嘛，没什么大不了的”
- 三思而行：开口前多换位思考，面试官到底想问什么，为什么会这样问，怎么回答比较好
- 反客为主：主动引导，从跌倒的地方爬起来
- 个人品质：诚实可靠，虚怀若谷，积极上进，尽量不要让面试官难堪
学习方法
- 构建自己的知识体系，终身学习
- 想牛人看齐，主动营造积极向上的环境
- 时刻反思哪里做的不好，下次改进

## 面试流程（面试官）

面试官代表着公司的形象，一言一行要礼貌、谨慎；我们面别人时，同时也是被面，其中的每个表情，每句话都会被记住，甚至“发扬光大”

基本流程：
- （1）面试前
  - 给候选人倒水，嘘寒问暖，路上是否堵车，有没有紧急的事情
  - 尊重对方感受，别让人等太久（尤其是特殊时间段，如午饭）
- （2）面试中
轻松的氛围，不宜过于严肃，咄咄逼人，或者挑衅，不合适的质疑
面试内容要覆盖全面，参考以下《面试要求》
- （3）面试后，问候选人是否有问题，或者没问到的优点，面试感觉如何
- （4）送别时，最好亲自送出门
  - 关于结果，需要统一话术：
    - 很满意：当天给，询问是否有要求、顾虑
    - 比较满意：几天内给答复，适当指出优点（暗示）
    - 不太满意：一个星期内等消息（几天后短信回复结果），同时可以指出优点（避免候选人丧气）和不足（暗示）
    - 很不满意：坦诚指出不足，提供有价值的资料，让候选人有收获，感觉到被尊重
- 面试结论及时同步出来，尽可能减小面试官之间的方差
  - 如果拿捏不定是否通过，就加面，或者就否掉（犹豫意味着候选人还不够优秀，企业招聘一般要超出标准的，面试造火箭，工作拧螺丝）

注：
> 实习生面试最好由资深的人面，或者经过面试培训（口头交待也行）的新人，发offer、指导人应该由资深的人负责


## 面试要求

面试时，要时刻注意候选人心态+情绪，刚开始不宜直接出算法题，先聊项目，稳定情绪，完成冷启动，然后在从项目入手出题目，由浅入深，不停追问，挖掘出候选人的知识体系+思考方法+个人品质，直至不会。如果出现不适（发抖，面色凝重，生气），要适时调整

| 事项 | 重要性 | 时间比例 | 考察点 | 题目 | 备注| 
|---|--|---|------------------------|----------|---------|
| 基本功 | B | 20-30% |（1）编程题目（不敢写或写的不好的人动手不多），算法基础<br>（2）灵活多变，分别从广度和深度上扩充，不断拔高，直到不会。<br>（3）观察编程功能+潜力，挤掉刷题党 | 经典题目：排序，树，图，文本处理 | 看个人的开发习惯<br>①代码熟练程度<br>②性能意识（时空复杂度）<br>③代码风格（洁癖）|
| 理论知识| C | 10-25% | 机器学习、深度学习经典算法的原理、优缺点、关系、应用经验 | ①LR，GBDT,RF,XGBOOST区别<br>②tf-idf，word2vec，fasttext | ①理论体系的完整度<br>②理解是否透彻|
| 项目 | C | 30-50% | ①前因后果，逻辑推理，解决问题的方式<br>②多找几个点，深入问，多几个为什么，怎么样<br>③以点带面，询问相关技术体系（考察广度和深度） | ①让候选自己说最好的项目<br>②优缺点，如何改进<br>③开放问题：如何识别暴恐分子？ | 核实项目真实性，逻辑推理| 
|个人品质 | A | 10% | ①是否踏实（不会就不会，大方承认）<br>②是否谦逊（被人质疑怎么办）<br>③学习方法（看什么书，做笔记）<br>④是否好相处（油盐不进，自言自语，攻击型）| ①你觉得自己的优缺点是啥<br>②这个地方有问题吧，怎么能这样？<br>③你这个想法很不错（故意夸张）<br>④碰到不会的问题会怎么办 | 第一位| 
| 求职意愿 | A | 5% | 目前拿到哪些offer，来面试的目的，侧重点（薪资、职位、地理位置等） | 你有什么要了解的（看候选人关心什么：待遇/氛围/地域等） | 面试态度：凑数、有机会就试试、无所谓、很想来。。。 |

注：
> 个人品质最容易忽略，因为大部分人的品质都没啥问题，即便是有问题的，也难以通过几次面试锁定，所以这一块一般指态度

## 面试问题

### 数学问题

#### 赛马问题

#### 最优停止理论


#### 等概率抽样

[水库抽样算法精简总结](http://www.voidcn.com/article/p-ecjrnlye-eo.html)

- 空间亚线性算法：由于大数据算法中涉及到的数据是海量的，数据难以放入内存计算，所以一种常用的处理办法是不对全部数据进行计算，而只向内存里放入小部分数据，仅使用内存中的小部分数据，就可以得到一个有质量保证的结果。
- 数据流算法：是指数据源源不断地到来，根据到来的数据返回相应的部分结果。适用于两种情况：
  - 第一、数据量非常大仅能扫描一次时，可以把数据看成数据流，把扫描看成数据到来。
  - 第二、数据更新非常快，不能把所有数据都保存下来再计算结果，此时可以把数据看成是一个数据流。
在一些情况下，空间亚线性算法也叫数据流算法。

#### 任意分布函数拟合（反采样/逆采样）

用均匀分布拟合任意分布，用于随机数生成

[逆采样(Inverse Sampling)和拒绝采样(Reject Sampling)原理详解](https://blog.csdn.net/anshuai_aw1/article/details/84840446)
- 通过F的反函数将一个0到1均匀分布的随机数转换成了符合exp分布的随机数，注意，以上推导对于CDF可逆的分布都是一样的


### 机器学习

[百度2015校招机器学习笔试题](http://www.itmian4.com/thread-7042-1-1.html)

#### 机器学习流程

机器学习流程
- 1 抽象成数学问题 
  - 明确问题是进行机器学习的第一步。机器学习的训练过程通常都是一件非常耗时的事情，胡乱尝试时间成本是非常高的。
  - 这里的抽象成数学问题，指的我们明确我们可以获得什么样的数据，目标是一个分类还是回归或者是聚类的问题，如果都不是的话，如果划归为其中的某类问题。
- 2 **获取数据** 
  - 数据决定了机器学习结果的上限，而算法只是尽可能逼近这个上限。
  - 数据要有代表性，否则必然会过拟合。
  - 而且对于分类问题，数据偏斜不能过于严重，不同类别的数据数量不要有数个数量级的差距。
  - 而且还要对数据的量级有一个评估，多少个样本，多少个特征，可以估算出其对内存的消耗程度，判断训练过程中内存是否能够放得下。如果放不下就得考虑改进算法或者使用一些降维的技巧了。如果数据量实在太大，那就要考虑分布式了。
- 3 **特征预处理与特征选择** 
  - 良好的数据要能够提取出良好的特征才能真正发挥效力。
  - 特征预处理、数据清洗是很关键的步骤，往往能够使得算法的效果和性能得到显著提高。归一化、离散化、因子化、缺失值处理、去除共线性等，数据挖掘过程中很多时间就花在它们上面。这些工作简单可复制，收益稳定可预期，是机器学习的基础必备步骤。
  - 筛选出显著特征、摒弃非显著特征，需要机器学习工程师反复理解业务。这对很多结果有决定性的影响。特征选择好了，非常简单的算法也能得出良好、稳定的结果。这需要运用特征有效性分析的相关技术，如相关系数、卡方检验、平均互信息、条件熵、后验概率、逻辑回归权重等方法。
- 4 **训练模型与调优** 
  - 直到这一步才用到我们上面说的算法进行训练。现在很多算法都能够封装成黑盒供人使用。但是真正考验水平的是调整这些算法的（超）参数，使得结果变得更加优良。这需要我们对算法的原理有深入的理解。理解越深入，就越能发现问题的症结，提出良好的调优方案。
- 5 **模型诊断**
  - 如何确定模型调优的方向与思路呢？这就需要对模型进行诊断的技术。
  - 过拟合、欠拟合 判断是模型诊断中至关重要的一步。常见的方法如交叉验证，绘制学习曲线等。过拟合的基本调优思路是增加数据量，降低模型复杂度。欠拟合的基本调优思路是提高特征数量和质量，增加模型复杂度。
  - 误差分析 也是机器学习至关重要的步骤。通过观察误差样本，全面分析误差产生误差的原因:是参数的问题还是算法选择的问题，是特征的问题还是数据本身的问题……
  - 诊断后的模型需要进行调优，调优后的新模型需要重新进行诊断，这是一个反复迭代不断逼近的过程，需要不断地尝试， 进而达到最优状态。
- 6 **模型融合** 
  - 一般来说，模型融合后都能使得效果有一定提升。而且效果很好。
  - 工程上，主要提升算法准确度的方法是分别在模型的前端（特征清洗和预处理，不同的采样模式）与后端（模型融合）上下功夫。因为他们比较标准可复制，效果比较稳定。而直接调参的工作不会很多，毕竟大量数据训练起来太慢了，而且效果难以保证。
- 7 **上线运行** 
  - 这一部分内容主要跟工程实现的相关性比较大。工程上是结果导向，模型在线上运行的效果直接决定模型的成败。不单纯包括其准确程度、误差等情况，还包括其运行的速度(时间复杂度)、资源消耗程度（空间复杂度）、稳定性是否可接受。
  - 这些工作流程主要是工程实践上总结出的一些经验。并不是每个项目都包含完整的一个流程。这里的部分只是一个指导性的说明，只有大家自己多实践，多积累项目经验，才会有自己更深刻的认识。
  - 故，基于此，七月在线每一期ML算法班都特此增加特征工程、模型调优等相关课。比如，这里有个公开课视频《特征处理与特征选择》。


#### 判别式和生成式

- **判别**方法：由数据直接学习决策函数 Y = f（X），或者由条件分布概率 P（Y\|X）作为预测模型，即判别模型。
  - 常见的判别模型有：K近邻、SVM、决策树、感知机、线性判别分析（LDA）、线性回归、传统的神经网络、逻辑斯蒂回归、boosting、条件随机场
- **生成**方法：由数据学习联合概率密度分布函数 P（X,Y）,然后求出条件概率分布P(Y\|X)作为预测的模型，即生成模型。
  - 常见的生成模型有：朴素贝叶斯NB、隐马尔可夫模型HMM、高斯混合模型GMM、文档主题生成模型（LDA）、限制玻尔兹曼机RBM
- 由生成模型可以得到判别模型，但由判别模型得不到生成模型。

#### 过拟合

[机器学习之Logistic回归(逻辑蒂斯回归）](http://blog.csdn.net/sinat_35512245/article/details/54881672)

正则化是针对过拟合而提出的，以为在求解模型最优的是一般优化最小的经验风险，现在在该经验风险上加入模型复杂度这一项（正则化项是模型参数向量的范数），并使用一个rate比率来权衡模型复杂度与以往经验风险的权重，如果模型复杂度越高，结构化的经验风险会越大，现在的目标就变为了结构经验风险的最优化，可以防止模型训练过度复杂，有效的降低过拟合的风险。

奥卡姆剃刀原理，能够很好的解释已知数据并且十分简单才是最好的模型。
L1和L2正则先验分别服从什么分布，L1是拉普拉斯分布，L2是高斯分布。

L1和L2区别
- L1范数（L1 norm）是指向量中各个元素绝对值之和，也有个美称叫“稀疏规则算子”（Lasso regularization）。
  - 比如 向量A=[ 1，-1，3 ]， 那么A的L1范数为 \|1\|+\|-1\|+\|3\|. 
  - 简单总结一下就是：
    - L1范数: 为x向量各个元素绝对值之和。
    - L2范数: 为x向量各个元素平方和的1/2次方，L2范数又称Euclidean范数或Frobenius范数
    - Lp范数: 为x向量各个元素绝对值p次方和的1/p次方. 
  - 在支持向量机学习过程中，L1范数实际是一种对于成本函数求解最优的过程，因此，L1范数正则化通过向成本函数中添加L1范数，使得学习得到的结果满足稀疏化，从而方便人类提取特征。
  - L1范数可以使权值稀疏，方便特征提取。
- L2范数可以防止过拟合，提升模型的泛化能力。

#### 为什么要做归一化

- [为什么一些机器学习模型需要对数据进行归一化？](http://www.cnblogs.com/LBSer/p/4440590.html)
- [深度学习中的归一化](http://www.julyedu.com/video/play/69/686)

不是所有模型都需要归一化：概率模型不需要归一化，因为它们不关心变量的值，而是关心变量的分布和变量之间的条件概率，如决策树、RF。而像Adaboost、GBDT、XGBoost、SVM、LR、KNN、KMeans之类的最优化问题就需要归一化。

#### 梯度下降

梯度下降法并不是下降最快的方向，它只是目标函数在当前的点的切平面（当然高维问题不能叫平面）上下降最快的方向。在Practical Implementation中，牛顿方向（考虑海森矩阵）才一般被认为是下降最快的方向，可以达到Superlinear的收敛速度。梯度下降类的算法的收敛速度一般是Linear甚至Sublinear的（在某些带复杂约束的问题）。

为什么不用牛顿法？
- 计算量大，目标函数的二阶导数(Hessian Matrix)
- 小批量情形下，牛顿法对二阶导的估计噪音太大
- 目标函数非凸时，牛顿法容易受鞍点/极值点影响

梯度消失/弥散
- （1）梯度消失：
  - 根据链式法则，如果每一层神经元对上一层的输出的偏导乘上权重结果都小于1的话，那么即使这个结果是0.99，在经过足够多层传播之后，误差对输入层的偏导会趋于0。
  - 可以采用ReLU激活函数有效的解决梯度消失的情况。
- 为什么会有梯度消失？
  - 神经网络的训练中，通过改变神经元的权重，使网络的输出值尽可能逼近标签以降低误差值，训练普遍使用BP算法，核心思想是，计算出输出与标签间的损失函数值，然后计算其相对于每个神经元的梯度，进行权值的迭代。
  - 梯度消失会造成权值更新缓慢，模型训练难度增加。造成梯度消失的一个原因是，许多激活函数将输出值挤压在很小的区间内，在激活函数两端较大范围的定义域内梯度为0，造成学习停止。
  - 反向传播中链式法则带来的连乘，如果有数很小趋于0，结果就会特别小（梯度消失）；如果数都比较大，可能结果会很大（梯度爆炸）。
- （2）梯度膨胀：
  - 根据链式法则，如果每一层神经元对上一层的输出的偏导乘上权重结果都大于1的话，在经过足够多层传播之后，误差对输入层的偏导会趋于无穷大。

LSTM为什么优于RNN
- 推导forget gate，input gate，cell state， hidden information等的变化；因为LSTM有进有出且当前的cell informaton是通过input gate控制之后叠加的，RNN是叠乘，因此LSTM可以防止梯度消失或者爆炸。

知识点链接：[一文清晰讲解机器学习中梯度下降算法（包括其变式算法）](http://blog.csdn.net/wemedia/details.html?id=45460)


#### LR

把LR从头到脚都给讲一遍。建模，现场数学推导，每种解法的原理，正则化，LR和maxent模型啥关系，LR为啥比线性回归好。有不少会背答案的人，问逻辑细节就糊涂了。原理都会? 那就问工程，并行化怎么做，有几种并行化方式，读过哪些开源的实现。还会，那就准备收了吧，顺便逼问LR模型发展历史

- 逻辑回归和线性回归首先都是广义线性回归
- 其次经典线性模型的优化目标函数是最小二乘，而逻辑回归则是似然函数， 
- 另外线性回归在整个实数域范围内进行预测，敏感度一致，而分类范围，需要在[0,1]。逻辑回归就是一种减小预测范围，将预测值限定为[0,1]间的一种回归模型，因而对于这类问题来说，逻辑回归的鲁棒性比线性回归的要好。

[机器学习之Logistic回归(逻辑蒂斯回归）](http://blog.csdn.net/sinat_35512245/article/details/54881672)

#### SVM

SVM，全称是support vector machine，中文名叫支持向量机。SVM是一个面向数据的分类算法，它的目标是为确定一个分类超平面，从而将不同的数据分隔开。

扩展：支持向量机学习方法包括构建由简至繁的模型：线性可分支持向量机、（近似）线性支持向量机及非线性支持向量机。
- 当训练数据线性可分时，通过硬间隔最大化，学习一个线性的分类器，即线性可分支持向量机，又称为硬间隔支持向量机；
- 当训练数据近似线性可分时，通过软间隔最大化，也学习一个线性的分类器，即线性支持向量机，又称为软间隔支持向量机；
- 当训练数据线性不可分时，通过使用核技巧及软间隔最大化，学习非线性支持向量机。

LR与SVM区别

联系：
- 1、LR和SVM都可以处理分类问题，且一般都用于处理线性二分类问题（在改进的情况下可以处理多分类问题） 
- 2、两个方法都可以增加不同的正则化项，如L1、L2等等。所以在很多实验中，两种算法的结果是很接近的。

区别：
- 1、LR是参数模型，SVM是非参数模型。
- 2、从目标函数来看，区别在于逻辑回归采用的是Logistical Loss，SVM采用的是hinge loss.这两个损失函数的目的都是增加对分类影响较大的数据点的权重，减少与分类关系较小的数据点的权重。
- 3、SVM的处理方法是只考虑Support Vectors，也就是和分类最相关的少数点，去学习分类器。而逻辑回归通过非线性映射，大大减小了离分类平面较远的点的权重，相对提升了与分类最相关的数据点的权重。
- 4、逻辑回归相对来说模型更简单，好理解，特别是大规模线性分类时比较方便。而SVM的理解和优化相对来说复杂一些，SVM转化为对偶问题后,分类只需要计算与少数几个支持向量的距离,这个在进行复杂核函数计算时优势很明显,能够大大简化模型和计算。
- 5、Logic 能做的 SVM能做，但可能在准确率上有问题，SVM能做的Logic有的做不了。

资料
- [支持向量机通俗导论（理解SVM的三层境界）](https://www.cnblogs.com/v-July-v/archive/2012/06/01/2539022.html)
- [机器学习之深入理解SVM](http://blog.csdn.net/sinat_35512245/article/details/54984251)
- [机器学习常见问题](http://blog.csdn.net/timcompp/article/details/62237986)

#### xgb

为什么xgb用泰勒展开
- XGBoost使用了一阶和二阶偏导, 二阶导数有利于梯度下降的更快更准. 使用泰勒展开取得二阶倒数形式, 可以在不选定损失函数具体形式的情况下用于算法优化分析.本质上也就把损失函数的选取和模型算法优化/参数选择分开了. 这种去耦合增加了XGBoost的适用性。

xgb如何寻找最优特征
- XGBoost在训练的过程中给出各个特征的评分，从而表明每个特征对模型训练的重要性.。XGBoost利用梯度优化模型算法, 样本是不放回的(想象一个样本连续重复抽出,梯度来回踏步会不会高兴)。但XGBoost支持子采样, 也就是每轮计算可以不使用全部样本。

xgb和gbdt区别
- XGBoost类似于GBDT的优化版，不论是精度还是效率上都有了提升。
与GBDT相比，具体的优点有：
- 损失函数是用泰勒展式二项逼近，而不是像GBDT里的就是一阶导数；
- 对树的结构进行了正则化约束，防止模型过度复杂，降低了过拟合的可能性；
- 节点分裂的方式不同，GBDT是用的基尼系数，XGBoost是经过优化推导后的。
- 特征选择：借鉴RF，可并行

[集成学习的总结](https://xijunlee.github.io/2017/06/03/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/)

#### CRF与HMM

哪个不属于CRF模型对于HMM和MEMM模型的优势（ ）
- A. 特征灵活 
- B. 速度快 
- C. 可容纳较多上下文信息 
- D. 全局最优 

解答：
- 首先，CRF，HMM(隐马模型)，MEMM(最大熵隐马模型)都常用来做序列标注的建模。
- 隐马模型一个最大的缺点就是由于其输出独立性假设，导致其不能考虑上下文的特征，限制了特征的选择。
- 最大熵隐马模型则解决了隐马的问题，可以任意选择特征，但由于其在每一节点都要进行归一化，所以只能找到局部的最优值，同时也带来了标记偏见的问题，即凡是训练语料中未出现的情况全都忽略掉。
- 条件随机场则很好的解决了这一问题，他并不在每一个节点进行归一化，而是所有特征进行全局归一化，因此可以求得全局的最优值。

### 计算机基础考题

hash冲突：关键字值不同的元素可能会映象到哈希表的同一地址上就会发生哈希冲突。解决办法：
- 1）**开放定址法**：当冲突发生时，使用某种探查(亦称探测)技术在散列表中形成一个探查(测)序列。沿此序列逐个单元地查找，直到找到给定 的关键字，或者碰到一个开放的地址(即该地址单元为空)为止（若要插入，在探查到开放的地址，则可将待插入的新结点存人该地址单元）。查找时探查到开放的 地址则表明表中无待查的关键字，即查找失败。
- 2）**再哈希法**：同时构造多个不同的哈希函数。
- 3）**链地址法**：将所有哈希地址为i的元素构成一个称为同义词链的单链表，并将单链表的头指针存在哈希表的第i个单元中，因而查找、插入和删除主要在同义词链中进行。链地址法适用于经常进行插入和删除的情况。
- 4）建立**公共溢出区**：将哈希表分为基本表和溢出表两部分，凡是和基本表发生冲突的元素，一律填入溢出表。


### 应用考题

几种类型：
- 纯leetcode题目：无背景信息，无需过多介绍，目标明确，但题目有一定随机性，容易让候选人发懵
- 背景相关题目：包含领域（如NLP）背景，根据候选简历信息而定

过程：题目从易到难，逐级提升，直至不会
- 易：具备编程基础就能写出来 → 看编程熟悉程度，代码风格，异常条件
- 中：性能提升，引入基础数据结构知识
- 难：知识点深入扩展（横向、纵向）


#### ML：嫌疑犯侦测

- 题目：设计一套机器学习系统，通过身份证信息+神态识别火车站里的嫌疑犯
- 数据：只有1000个样本，其中10个正例，990个负例。
- 知识点：抽样、不平衡、过拟合、评估指标（召回+准确+精确+F1）

机器学习系统设计：
- 数据处理：采集、缺失值、异常值
  - 训练集、验证集、测试集划分
- 特征工程：正负样本不平衡、特征变换（连续、离散）
  - 样本不平衡：欠采样（删除→SMOTE）、过采样（复制→easy ensamble）
  - 特征变换
- 模型：分类还是回归？还是异常检测？（i-forest，3倍标准差）
  - 过拟合、欠拟合，如何解决
  - 如何调参？线搜索、网格搜索、贝叶斯优化、学习率。。。
  - 时间不够，如何快速迭代
- 预测：模型如何加载？分布式情形？
- 评估：重精确还是召回？
  - 离线：k-fold交叉验证，F1，精确召回（表达式），AUC、ROC
  - 在线：小流量灰度实验、ab-test或inter-leaving
- 服务部署：如何形成闭环？系统监控、字段监控、数据上云
- 其它：产品策略不当、排期紧张怎么办？多人如何协作分配？

注意：
> 不要迷信刷题！要从题目中找知识体系的漏洞


#### ML：LR回归

考察机器人基本流程熟悉程度：
- 数据准备：训练集、测试集、验证集
- 评估指标：分类、回归
- 模型定义：网络结构，用sklearn、pytorch、TensorFlow实现
- 优化器：adam
- 训练
- 推断

#### ML：优化算法——训练曲线

模型训练环节，损失函数出现不同情形，如何快速定位？
- ![](https://pic4.zhimg.com/80/v2-287f28b3a1f34657c3a7dba0e6d4b55f_720w.jpg)
- 知识点
  - 学习率：学习率越大，波动越剧烈，学习率越小，波动越平缓。
  - 过拟合
  - batch size：batchsize越小，波动越剧烈，batchsize越大，波动越平缓。
  - 梯度下降算法：梯度就是曲线的斜率，如果要最小化目标函数，反向传播过程中，每个参数在梯度方向上减小一定幅度，最终网络收敛到一个局部最优值，减小的幅度大小由学习率决定。

[链接](https://www.zhihu.com/question/472162326/answer/2308198711)

训练的时候 loss 不下降
- 模型结构问题。当模型结构不好、规模小时，模型对数据的拟合能力不足。
- 训练时间问题。不同的模型有不同的计算量，当需要的计算量很大时，耗时也会很大
- 权重初始化问题。常用的初始化方案有全零初始化、正态分布初始化和均匀分布初始化等，合适的初始化方案很重要，之前提到过神经网络初始化为0可能会带来的影响
- 正则化问题。L1、L2以及Dropout是为了防止过拟合的，当训练集loss下不来时，就要考虑一下是不是正则化过度，导致模型欠拟合了。正则化相关可参考正则化之L1 & L2
- 激活函数问题。全连接层多用ReLu，神经网络的输出层会使用sigmoid 或者 softmax。激活函数可参考常用的几个激活函数。在使用Relu激活函数时，当每一个神经元的输入为负时，会使得该神经元输出恒为0，导致失活，由于此时梯度为0，无法恢复。
- 优化器问题。优化器一般选取Adam，但是当Adam难以训练时，需要使用如SGD之类的其他优化器。常用优化器可参考机器学习中常用的优化器有哪些？
- 学习率问题。学习率决定了网络的训练速度，但学习率不是越大越好，当网络趋近于收敛时应该选择较小的学习率来保证找到更好的最优点。所以，我们需要手动调整学习率，首先选择一个合适的初始学习率，当训练不动之后，稍微降低学习率。
- 梯度消失和爆炸。这时需要考虑激活函数是否合理，网络深度是否合理，可以通过调节sigmoid -> relu，假如残差网络等，相关可参考为什么神经网络会有梯度消失和梯度爆炸问题？如何解决？
- batch size问题。过小，会导致模型损失波动大，难以收敛，过大时，模型前期由于梯度的平均，导致收敛速度过慢。
- 数据集问题。
  - （1）数据集未打乱，可能会导致网络在学习过程中产生一定的偏见
  - （2）噪声过多、标注有大量错误时，会导致神经网络难以学到有用的信息，从而出现摇摆不定的情况，噪声、缺失值、异常值
  - （3）数据类别不均衡使得少数类别由于信息量不足，难以学到本质特征，样本不均衡相关可以看样本不均衡及其解决办法。
- 特征问题。特征选择不合理，会使网络学习难度增加。之前有提到过特征选择的文章，如何找到有意义的组合特征,特征选择方法

2 测试的时候 loss 不下降
- 训练的时候过拟合导致效果不好 
  - 交叉检验，通过交叉检验得到较优的模型参数;
  - 特征选择，减少特征数或使用较少的特征组合，对于按区间离散化的特征，增大划分的区间;
  - 正则化，常用的有 L1、L2 正则。而且 L1正则还可以自动进行特征选择;
  - 如果有正则项则可以考虑增大正则项参数;
  - 增加训练数据可以有限的避免过拟合;
  - Bagging ,将多个弱学习器Bagging 一下效果会好很多，比如随机森林等.
  - 早停策略。本质上是交叉验证策略，选择合适的训练次数，避免训练的网络过度拟合训练数据。
  - DropOut策略。
- 应用场景不同导致。本来训练任务是分类猫和狗，测试用的皮卡丘和葫芦娃。
- 噪声问题。训练数据大概率都是经过去噪处理的，而真实测试时也应该去除噪声。

#### ML：P/R指标

- 实现precision、recall计算


#### NLP：词频统计

- 题目：统计简历里的Top 5关键词
- 数据：简历文件cv.txt（或者两会专题新闻）
- 思路：逐行读入，分词，用一个字典存储频次，排序，输出top 5。
- 性能：时间复杂度O(N)+O(nlogn)，空间复杂度n——N是所有单词数，n是去重后的单词数
- 改进：
  - **时间**复杂度优化：能不能更快？
    - 排序环节：只需top5，排序时不用计算所有频次，堆排序（小根堆） + 单向冒泡
  - **空间**复杂度优化：
    - 读取环节：非得遍历完整个文件？牺牲准确率（类似bloom filter），分布式，随机抽样，水库抽样（等概率流式采样），逆采样和拒绝采样
- 发散：
  - 预计会是什么样的词？（业务敏感度 + NLP功底）—— 标点符号、停用词，高频词，需要加tf-idf权重
  - 如何挑关键词？分词、词性标注、相似词（字面+语义word2vec）
  - 数据量大：
    - 单机（4G以内）：不要一次加载到内存
    - 内存装不下（20G）：磁盘分片存储，逐个加载到内存（linux流式处理），可以分别取top5吗？
    - 磁盘装不下（1T）：分布式计算，Hadoop，MapReduce代码→HQL→jobtracker数据倾斜

#### NLP：新词发现

- 背景：word2vec 向量训练，cbow、skip-gram模式
- 需求：从文档中挖掘新出现的top k词汇, 长度3以内 （词频统计进阶版）
  - 输入：文档 D = { s1, s2, ..., sn}
  - 输出：W = [ [w1, 23], [w2, 10], ..., [wk, ck] ]
- 方法：
  - 思路：是否使用 tf-idf ？
  - 传统方法：n-gram思路，挨个遍历uni-gram, bi-gram, tri-gram, 取topk

#### NLP：MLM输出

- 背景：BERT模型用了MLM模型
- 需求：准备MLM语料，用于模型训练
  - 输入：文档 D = { s1, s2, ..., sn}, 其中 si = [ '这是一条句子' ] (m维)，musk策略与BERT类似
  - 输出：M = { [s1', m1], [s2', m2], ..., [sn', mn]}
- 方法
  - 思路：是否了解BERT的掩码策略？
  - 传统：

#### NLP：语义向量

- 背景：句向量 embedding
- 需求：找出与query最相似的几个句子
  - 输入：句向量集合 D = { v1, v2, ..., vn}, 其中 vi = [ 2, 5, 1 ] (m维)，任意向量 vx
  - 输出：与vx最相似的 top k个句子
- 方法：
  1. 每来一个vx，依次遍历D，得到距离集合，排序，取 top k输出 —— 时间复杂度 O(nm)+O(nlogn)，空间复杂度 O(n)，重复计算
  1. 改进：提前聚类，计算D中各向量距离
  1. 改进：类似 kd树，m维距离映射到一维数组上，就近取top k

#### NLP：NSP预测

- 背景：
- 需求：
  - 输入：
  - 输出：
- 方法：

#### NLP：编辑距离

- 背景：
- 需求：
  - 输入：
  - 输出：
- 方法：
  - 动态规划

#### NLP：transformer QKV计算

- 背景：
- 需求：
  - 输入：
  - 输出：
- 方法：




### 软素质

为什么离开上家公司

【2021-10-10】猎头：面试被问**短板是什么**、**为什么要离开上一家公司**，怎么办？
1. 建议坦诚但积极。本质上不是面试官想知道答案，而是想通过你的回答，看出你是一个什么样的人。而且，不少公司都会做背景调查，所以如果离职原因和真实的原因相差太远，那么在做背调时很难过关。回答起来确实是有些不舒服的，这是面试官在刻意施压，撇开具体的回答不说，光从回答问题的方式，就能看出如何化解压力，同时还可以了解是否有自知之明。
2. 如果回答扭扭捏捏，甚至被面试官看出来编造答案，那么十有八九很难通过的，所以要坦诚。但是，这些问题本身就是负面的，如果负面的回答出来，那面试结果也可想而知，所以你要用积极的话再说一遍。
3. 比如，面试中问有哪些短板。我自己在应聘一家企业的企业大学校长一职的时候就被CEO问过这个问题。根据刚才那个**坦诚但积极**的回答方式，回答：首先非常坦诚的说，自己在做执行的过程中，往往因为追求速度，而忽视细节。并且举一个真实的例子，在某个项目中，我因为决策速度太快，在一个执行细节上考虑不周全，结果险些造成损失。但是如果就这么回答，显然，对方是不会请一个这么急躁的人来做企业大学校长的。我接着说道：第一，通过复盘我意识到了，自己的确有决策时候太过追求效率的缺点。其次，现在自己养成了一个习惯，也就是每次在帮自己做决策时，必须要拿出，笔和纸，用金字塔原理，把问题拆解清楚，做到不重复不遗漏。所以像上次那样的决策失误，几乎就再也没有出现过。这就是积极，不但告诉了面试官如何在具体的某件事情上进步，而且我的学习能力快，这个优点，也符合企业大学的这个角色的要求。
4. 尤其是不要把一盆脏水全泼到上一家公司，对前东家大放阙词，这会给HR传递一个非常不好的信息，你是一个刺头。其实，正常来说，一个人的离职原因，HR们也心知肚明，要么就是钱没给够，要么就是发展不够顺。你可以更注重，对于应聘公司的仰慕出发，谈一谈这家公司，在你的职业发展目标中扮演什么样的角色。是因为这家公司吸引了你，而不是因为上一家公司耽误了你。


# 算法工程师

【2021-9-18】[数十位算法工程师的经验总结](https://www.toutiao.com/w/i1711154806904832/)

总结mm中对于算法工作的论述，虽然有调侃之意，但对于指导工作难道没有意义吗？
1. 从0到1，用**简单模型**；可解释性强，简单易实现，能快速验证思路，也有利于奠定后期的提升空间；（指标：78％）
2. 加强**特征工程**工作，特征离散化，特征交叉，新增特征等等。（指标：81％）
3. 换用经典的**高级模型**，如从LR到GBDT，再到DeepFM，DIN等等。（指标：85％）
4. **精细化**调整，分人群优化，针对badcase优化；（指标：85.5％）
5. **模型调参**，尝试各种前沿模型结构，调整数据采样方式，增加统计特征，清洗数据等等（85.6％）
6. 继续想各种**鬼点子**优化（85.66％）
运气好的话，赶上环比波动，赶紧上线总结（到底是不是模型带来的，就是玄学了，哈哈）；

运气不好的话，模型指标莫名其妙下跌，大家怎么应对呢？


# 算法入门

[没有企业项目经验怎么办？](http://ai.yanxishe.com/page/questionDetail/8455)

## 初学者的困惑

AI初学者不要过于迷信企业项目，忽略基本功的学习，不要mnist都没研究透彻就嫌简单，直奔难度更大的resnet，好高骛远，得不偿失，学东西请务必戒骄戒躁，一步一个脚印，才能越走越远。企业项目的事实跟你想象的大不一样。

## 真相

- ①企业项目侧重工程实践，尤其重视数据闭环，从采集到入库（Hive+Storm+HBase），到模型训练（单机+分布式），到线上预估（qps，压测），到效果评估（ab test），再到业务报表和数据回流，整个过程里算法比重不到20%
- ②算法上越简单越好，如果能用LR就不会用神经网络，能用规则就不用模型，尤其注重风险可控性，学院派看来模型太low，然后一旦让学院派来做，却又无从下手，做出来的系统无法上线运行。工业界的算法往往滞后于学术界几年，一方面因为推广速度，另一方面，学术界的模型算法往往过于理想，停留在Demo阶段，对实际应用场景缺乏了解，很多paper上的方法其实行不通，一个paper导向，固定的实验集上取得好的效果就行，一个却是实打实的应用场景，必须可实施，二者截然不同。所以，千万不要眼高手低，把学院派的作风带到工作中，还埋怨企业算法low，多思考为什么
- ③结果导向，不管什么模型算法，最终谁带来的业务收益大谁就是赢家。这点很好理解，学校的项目大多没有实际应用，toy example，玩玩而已，出发点一开始就不是为了应用，而是所谓的“高大上”的算法，一到实际场景，发现各种漏洞，堵了一个又来一个，真烦。企业面向的是实际场景，有营收压力，解决问题是首要的
- ④系统思维，企业项目涉及的点非常多，一个部分没考虑到位就可能带来灾难性的后果，这些思维是学院派不具备的，一个完整的项目如果带到培训班里，学院估计会嫌蛮，讲那么多，大部分都是工程，只有一部分是算法，觉得没学到“干货”，大部分人认为的干货就是各种牛叉的算法模型，其它都不是。
总之，企业更加侧重工程能力，系统思维，结果导向，这些是培训不具备的。

## 如何具备企业项目能力？

- ①实习，参与到企业项目中，慢慢体会，实际应用总是跟想象的不一样。大多数实习生做不了多少核心工作，大多打杂，提升工程能力
- ②精耕细作，找一个小项目，哪怕是mnist，想各种办法，不停的优化，尽可能提升泛化能力，黑白mnist玩腻了，换服装领域试试？换cfair-10试试？换web demo，实施手写识别试试？很多人容易犯的错就是，浅尝辄止，好高骛远，以为跑一边github的demo，就会深度学习了，too simple，sometimes naive！这种学习态度是不可能学会深度学习的
- ③多看多动手多做笔记，构建自己的知识体系。

## [停止学习框架，专注基础知识](https://ai.yanxishe.com/page/blogDetail/10548)
- 【2019-05-08】[Stop Learning Frameworks](https://sizovs.net/2018/12/17/stop-learning-frameworks/)

### 观点：直接学tensorflow、pytorch，别学数学+python了
- 我们每天学习编程语言、框架和库。我们知道的工具越新越好。但这一切都是在浪费时间！
- 时间是我们拥有的最宝贵的资源。时间是有限的，不可更新的，并且是你不能买到的。
- 科技就像时尚一样，它也在以光速变化。为了赶上时间的变化，我们需要跑得很快。这场比赛没有赢家，因为它没有终点。
> 技术变了又变，但它们都有共通性。正确地设置优先级：你需要把 80% 的时间花在基础学习上，然后剩下 20% 的时间留给框架，库和工具的学习

- 技术存在的时间越长，学习它就越安全。
- 不要急于学习新技术——它有很高的消亡概率。
- 时间是最好的导师，它会证明哪些技术值得学习，所以请学会等待。
- 十年过去了，我经历了 50 个不同的软件项目。感谢这些建议，我学到的所有东西都可以跨公司、团队、跨领域使用。今天，我所学的知识仍然有用。我没有浪费时间。
- 只有深入研究项目的本质，你才会发现它们都是相似的：
   - 编程语言是不同的，但设计是相似的。
   - 框架是不同的，但设计模式是可以通用的。
   - 开发者是不同的，但与人打交道的规则是统一的。
- 记住：框架、库和工具是会变化的。时间是宝贵的。
- 请将宝贵的时间花在可移植的技能上：
   - 微服务→框架进化体系结构
   - 新的编程语言→干净的代码，设计模式，DDD
   - 量少安全→精简编码原则
   - 高端→容错的模式
   - 容器→持续交付
   - Angular→网页、HTTP 和 REST

<font color="red">初学者要恶补的是基础知识，一口吃不了胖子，踏实点儿，知识体系完善了，才能在不同项目中游刃有余</font>

## 什么是优秀

【2021-8-12】

（1）我眼中的优秀：
①理论基础，广度深度兼备，自圆其说，具备相对完整的知识体系，尤其是基础知识，sota技术半年一更新，但基础知识几十年不变
②工程能力，linux，数据库，机器学习，web服务，编程语言等
③学习能力和好奇心，短时间内掌握某门技术，较强的领域迁移能力，同时不但探索未知，走出舒适区
④项目思维，业务理解，任务拆分，数据分析，使命必达。

我经常问候选人的一道题，生活中常见的暴恐识别，70%的人提到分类（未必分得清与回归的关系），50%的人分得清训练集/测试集/验证集，30%的人正确设置评估指标/知晓特征工程，20%的人意识到不均衡问题，10%知道模型部署/漂移，5%能转化到异常检测，1%随手用TensorFlow写出LR代码

（2）怎么在简历中证明？

技术博客/笔记，github项目和竞赛可以支撑①②，论文支持①，实习支持②③④；以上非必须，有更好，不过别“露馅”：技术博客/github几年不更新；论文也只是挂挂名，打打杂，核心亮点吱吱呜呜；实习项目说不清楚背景、目标、指标、技术链路、问题

（3）应届生/在校生如何准备？
①找项目练手，全程参与，不一定非得实习（参考：https://www.yanxishe.com/questionDetail/8455），kaggle上任务多得是，还有优秀“答案”，比赛除了名次，更重要的是培养业务/项目思维，与高手过招，打通技术领域隔阂
②从项目中提炼技术点，看书/博客深入研究、总结，沉淀出自己的笔记. 30-40%候选人能解释清楚项目中的技术点，但只有10%的人能说清楚为什么这么做
③代码练习，触类旁通；阿里校招时，有个985学霸说leetode刷了3-5遍，我变了下题目，跪了。

（4）应届生典型画像
①沉迷各种sota算法，看不上数据挖掘/机器学习/架构，不停造demo，无法落地
②视野狭窄，局限在某个小领域，工作后拿锤子找钉子
③业务思维不足，忽略数据背后的意义，导致数据敏感度不足，遇到业务问题一筹莫展
④迷茫，不知道学什么，该做什么，被动完成任务，没有主动思考项目/任务目标

（5）优秀的人不怕卷：同样是学习，有的人看几个月资料，过半年忘光光；有的人却能写本书