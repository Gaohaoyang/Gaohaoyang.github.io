---
layout: post
title:  GPT
date:   2019-12-14 16:52:00
categories: 深度学习 自然语言处理
tags: GPT gpt 文本生成 对抗 攻击
excerpt: 大语言模型之GPT
mathjax: true
permalink: /gpt
---

* content
{:toc}


# GPT模型

- 迭代路线：GPT → GPT-2 → GPT-3

项目[github页面](https://github.com/openai/gpt-3)和论文[Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165), 目前没有代码只有生成样本和数据.52页的T5，72页的GPT-3

## 体验

申请账号，调用官方[api](https://beta.openai.com/?app=creative-gen&demo=5)

openai提供的[应用示例集合](https://beta.openai.com/examples)

代码：

```python
import os
import openai

openai.api_key = os.getenv("OPENAI_API_KEY")

# ------- 文本生成 ---------
prompt = """We’re releasing an API for accessing new AI models developed by OpenAI. Unlike most AI systems which are designed for one use-case, the API today provides a general-purpose “text in, text out” interface, allowing users to try it on virtually any English language task. You can now request access in order to integrate the API into your product, develop an entirely new application, or help us explore the strengths and limits of this technology."""

response = openai.Completion.create(model="davinci", prompt=prompt, stop="\n", temperature=0.9, max_tokens=100)

# ------- 其它应用 ---------
response = openai.Completion.create(
  engine="davinci",
  prompt="The following is a conversation with an AI assistant. The assistant is helpful, creative, clever, and very friendly.\n\nHuman: Hello, who are you?\nAI: I am an AI created by OpenAI. How can I help you today?\nHuman: I'd like to cancel my subscription.\nAI:",
  temperature=0.9,
  max_tokens=150,
  top_p=1,
  frequency_penalty=0.0,
  presence_penalty=0.6,
  stop=["\n", " Human:", " AI:"]
)

print(response)

```

### 个性化聊天

【2021-10-13】[AI 复活「她」！GPT-3 帮美国小哥复刻逝去未婚妻，但又夺走她…](https://www.toutiao.com/i7018474312931885576/),7 月，一名33岁的美国小哥 Joshua Barbeau 在未婚妻去世后，根据她在 Facebook 和 twitter 上的文本在另一名开发人员的帮助下成功在 GPT-3 上微调，能够复刻出未婚妻生前的谈话方式。有很多人觉得 Joshua Barbeau 这个行为很可怕。但他认为，借助 Project December 项目创建出模拟已故之人的聊天机器人，可能会“帮助一些因此抑郁的人解开他们的心结”。但，Project December 的开发作者 Jason Rohrer 却收到了来自 OpenAI 的最后通牒：我们会在 9 月 2 日上午 10 点终止你的 API 访问。
- ![](https://p3.toutiaoimg.com/origin/tos-cn-i-qvj2lq49k0/6a0cdb8a4c414ed08b135927a0af5460?from=pc)
- Jason Rohrer 是一名独立游戏开发者，Project December 是他于去年夏天疫情期间突发奇想的一个灵感：用 GPT-3 API 来开发一款模拟人类的聊天机器人，以电影《Her》中男主角的智能语音助手 Samantha 为原型
- 7月用户量突然激增。在《旧金山纪事报》报道的一篇文章讲述了一位 33 岁的美国男子 Joshua Barbeau 如何利用 Project December 创建出模拟其已故未婚妻 Jessica 的聊天机器人并与之交谈数月。在那之后，成千上万的人开始使用 Rohrer 网站。
- Rohrer 意识到他的网站将达到每月 API 的限制。主动联系 OpenAI 希望能通过支付更多费用以增加配额，以便容纳更多人与他创建的“Samantha”或自己的聊天机器人交流。但与此同时，OpenAI 方面认为 Project December 存在一定隐患：聊天机器人可能会被滥用或对人们造成伤害。
- 因此，双方进行了一场视频会议，可是很明显，效果并不理想。Jason Rohrer 在接受外媒 The Register 的采访时提到，OpenAI 给出了继续使用 GPT-3 API 需满足的 3 个条件：
  - Jason Rohrer 必须禁止人们定制自己的开放式聊天机器人这一功能。
  - Jason Rohrer 需设置内容过滤器以避免用户与“Samantha”谈论敏感话题。
  - Jason Rohrer 必须部署自动化监控工具监视用户的对话，检测他们是否滥用 GPT-3 生成了有害或敏感语句。
- OpenAI 的员工与 Samantha 聊天，并试图了解「她」是否有种族主义倾向，或者会从「她」的训练数据中提供看起来像真实电话号码或电子邮件地址的内容，实验结果表明Samantha很老实，什么也没有说。Samantha 的输出令人感觉很真实，但随着时间的推移，很明显你能感觉到是在与某种自动化系统交谈，谈话的过程中往往会突然丢失对话思路。
  - ![](https://p5.toutiaoimg.com/origin/pgc-image/7add87e26cca475a848d79669be7b2e1?from=pc)
- OpenAI 担心用户会受到 AI 的影响，害怕机器人会让他们自杀或如何给选举投票，可这完全是一种超道德的立场。
- Jason Rohrer 拒绝添加 OpenAI 要求的功能或机制，而是悄悄将原本 Project December 使用的 GPT-3 API 断开。并且替换为功能较差的开源 **GPT-2模型**以及由另一个研究团队开发的大型语言模型 **GPT-J-6B**。不过这两种模型性能显然不比 GPT-3，Samantha的对话能力也受到了影响。

【2022-4-6】[外公去世十年后，我用 AI“复活”了他](https://www.toutiao.com/article/7083406230143681028)

一位程序员，天天跟 AI 和算法打交道，不免开始盘算：现阶段的 AI 技术能不能整合到一起，最终实现一个无论是语言表达还是人形上都极其接近我外公的效果。于是我开始搜索，发现了不少和我相同的愿望，也有人付诸实践。
- ![](https://p3.toutiaoimg.com/origin/tos-cn-i-qvj2lq49k0/200f8a60652f41f5840b558ea0ea938a?from=pc)
- 韩国一位母亲因七岁女儿去世万分痛苦，一个电视团队听闻后耗时八个月制作出了女孩的三维虚拟形象，让母女在 VR 场景中相遇。在我看来，这更偏向动画制作，女孩形象和场景比较“卡通”

Project December 只能造出文字聊天机器人，我想合成一个有具体可感形象的“外公”，最好能写实一些。
- “他有记忆，能和我互动，能开口说话，脸一看就是我外公”，这个大胆的想法越来越清晰，我开始检索可能用得上的 AI 论文。

先做“外公”的大脑
- Project December 之所以能基于种子文本，生成有特定个性的角色，是因为接入了 GPT-3 的 API。GPT-3 是 OpenAI 的商业语言模型，可以简单理解为这个模型给了计算机像“人一样思考的能力”。

GPT-3 甚至能说出一些“高于人类”的话：
- 开始准备要导入 GPT-3 的种子文本，把之前保留的信件扫描成文字，整理好之前同步到云上的聊天短信，还扒下外公之前在视频里说过的话：“这个鱼还是要红烧，八十多块买来清蒸，味道洁洁淡（杭州话，“清淡”的意思），没味道。”“你不要手机一直拍来拍去，去帮你阿弟端菜。”
- 一股脑导入 GPT-3 后，它就能开始模仿外公的语言风格和对话思路……等等，GPT-3 收费。不过，我很快找到了免费开源的 GPT-J，开始了训练。
语言模型训练就是“猜词”的过程。模型利用显卡并行计算，找出一个语料库中每个词句之间的关系，比如出现一个词后，下一个词最有可能是什么。GPT-J 团队开源了预训练模型，已能实现大部分功能，我需要做的就是把种子文本转换成一个个词元，然后将这个外公专有语料库丢给 GPT-J 学习。

一般的深度学习模型需要训练几天几夜，我这次用 GPT-J 学习新语料并不是特别耗时，只需花六个小时。


人类：人生的目的是什么？
- AI：生命是一个美丽的奇迹。它随着时间不断进化，形成一种更大形式的美。从某种意义上来说，人生的目的就是增加宇宙中的这种美。
- 它之所以有这种能力，是因为工程师给这个模型猛喂数据，足足超过 3000 亿个文本。AI 模型在看了这么多文本后，就开始挖掘（也就是找规律）出词与词、句与句之间的关系，然后结合当前语境给出最适合的回答。

用语音驱动人脸
- 让我外公“显形”最直接的就是构建一个三维定制虚拟人像，但这需要采集人体数据点，很显然这条路行不通。
- 结合手头现有的照片、语音和视频等素材，我开始思考：有没有可能只用一段视频加上一串语音，就能生成一个栩栩如生的人脸呢？
- 几经波折，我找到了“Neural Voice Puppetry”这个方案，它是一种“人脸再扮演”（facial reenactment）技术，我只需要给定对话音频，它就能生成一段人脸嘴型与音频同步的动画。
- 论文作者利用卷积神经网络，把人脸外观、脸部情绪渲染和语音三者的关系找出来了，然后再利用这种学到的关系去渲染一帧帧能读出语音的人脸视频。但这个方案唯一的不足是不能指定输出的人物，我们只能选择给定人物，比如奥巴马。
- ![](https://p3.toutiaoimg.com/origin/tos-cn-i-qvj2lq49k0/e424e2a592894b019b3c2d43fde7cc41?from=pc)
实际得到的结果，是一段奥巴马用我外公声音在讲话的视频。我下一步要做的是 AI 换脸。
- 我最终选择用 HeadOn: Real-time Reenactment of Human Portrait Videos这篇论文里提到的技术。相关应用就是现在时兴的虚拟主播：捕捉中之人的表情，驱动二次元人物的脸。
- 提供表情信息的一般是真人，但由于我之前生成的“奥巴马”非常逼真，所以可以直接拿来带动我外公的肖像。
- 就这样，我用了我外公生前的通讯记录和不多的影音资料，整合几个成熟的 AI 技术，就让他“复活”了。
- ![](https://p3.toutiaoimg.com/origin/tos-cn-i-qvj2lq49k0/f2a8dc12e4314d6fb796cf9365337485?from=pc)



## 资料

- 【2021-10-13】[GPT-3 Creative Fiction](https://www.gwern.net/GPT-3) 小说作品创作
- 【2019-2】张俊林：[效果逆天的通用语言模型 GPT 2.0 来了，它告诉了我们什么？](https://www.infoq.cn/article/pW8YaUXjTuhC6d0p*OwX)
- [OpenAI GPT-3 API](https://openai.com/blog/openai-api/)，[Github地址](https://github.com/elyase/awesome-gpt3#awesome-gpt-3)
- ![](https://github.com/elyase/awesome-gpt3/raw/master/screenshot.png)

- Jay Alammar杰作：[怎样向产品解释GPT-3](http://jalammar.github.io/how-gpt3-works-visualizations-animations/)
![](http://jalammar.github.io/images/gpt3/05-gpt3-generate-output-context-window.gif)

- 资料
  - [GPT-3的50种玩法告诉你，它很酷，但是没有通过图灵测试](https://www.toutiao.com/a6855330183403012621/)
  - [最新最全GPT-3模型网络结构详细解析](https://www.toutiao.com/i6858589917883138571/)
  - 知乎：[如何评价1700亿参数的GPT-3？](https://www.zhihu.com/question/398114261)

## 发展历史

- 2018年6月，OpenAI的研究人员使用了一种新颖的组合，将生成式深度学习架构Transformer和无监督预训练（也称为自监督学习）结合起来，得到了GPT模型。
- Transformer的自注意力机制提供了一种通用的方式来对输入的各个部分进行建模，使其依赖于输入的其他部分（需要大量计算）。
- Transformer和无监督预训练的组合不限于GPT系列模型。Google，Facebook和许多大学实验室相继提出了BERT、XLNet等语言模型。
- 到2019年初，OpenAI改进了其基础架构，将参数和数据数量增加10倍来扩展同一模型，即GPT-2。
- 随后，OpenAI推出了SparseTransformer，它是对早期Transformer模型的改进，可以可靠地处理更长的文档。
- 2020年，OpenAI通过其beta API发布了GPT-3，引起了人们的关注。GPT-3不仅扩大了GPT-2上使用的数据量和计算量，而且用SparseTransformer取代了原始Transformer，从而产生了迄今为止具有最佳zero-shot 和 few-shot学习性能的模型。
- GPT-3的few-shot学习能力使得它具备了一些非常有趣的演示功能，包括自动代码生成、“搜索引擎”、写作辅助和创意小说等。

- 【2020-8-10】[京东副总裁何晓冬：GPT-3后，人机对话与交互何去何从？CCF-GAIR 2020](https://www.leiphone.com/news/202008/BC6XqIXF3ifH6uvV.html)
![](https://static.leiphone.com/uploads/new/images/20200810/5f311dc980e89.jpg?imageView2/2/w/740)

【2021-2-6】[GPT发家史](https://mp.weixin.qq.com/s/Y8yHaf7dm5jEQAvP9IvRRA)
- OpenAI 成立之初并非因为文本生成模型而知名，这点和 DeepMind 些许不同，后者专注强化学习一百年。 OpenAI 一开始两条线是**强化学习**和**生成模型**（集中 GAN），而 GPT 开始也没受到太大关注，而是在探索中 OpenAI 发现了其可能性，便开始大力投入，到现在基本上一大半项目都与其相关。所以，现今大家提起 OpenAI 相信都是马上想起 GPT，再或者和马一龙（Elon Musk）早期有一腿，又多少人还能想起强化学习和GAN呢。
- OpenAI 早期成员，除 Pieter Abbeel 等做强化学习的，就是一众做偏图像生成的，比如
- GAN 提出者 Ian Goodfellow 最早也是入职 OpenAI
- 同期入职的还有一个叫 Alec Radford 发明 DCGAN 的精神小伙。大家记住这个名字，因为他对 GPT 的发展应该说至关重要。
- 所以可以看出最早 OpenAI 就是群做强化学习和图像生成的人，没啥做 NLP 的，自然也难料想自己居然是通过 NLP 来一战成名。

GPT系列：
- 2018年6月 `GPT-1`：大量数据（约5GB文本）上无监督训练，然后针对具体任务在小的有监督数据集上做微调；关键词：“scalable, task-agnostic system”；8个GPU上训练一个月；预训练模型（1.1亿参数）可[下载](https://github.com/openai/finetune-transformer-lm)；
- 2019年2月 `GPT-2`：大量数据（约40GB文本）上无监督训练，然后针对具体任务在小的有监督数据集上做微调，尝试在一些任务上不微调（即使结果离SOTA还远）；关键词“without task-specific training”；据说在256个Google Cloud TPU v3上训练，256刀每小时，训练时长未知[2]；预训练模型（15亿参数）最终公开可[下载](https://github.com/openai/gpt-2-output-dataset)；[openai model](https://openai.com/blog/better-language-models/​openai.com/blog/better-language-models/)
- 2020年5月 `GPT-3`：大量数据（499B tokens）上无监督训练，不微调就超越SOTA；关键词“zero-shot, one-shot, few-shot”；训练据说话费1200万刀；1750亿参数，将会开放付费API


![](http://files.cn-healthcare.com/upload/20201117/wximg/41331605568278419)

[白描网页版](https://web.baimiaoapp.com/)

| 时间| 机构| 模型名称| 模型规模| 数据规模 | 计算时间|
|---|---|---|---|---|---|
|2018.6 | OpenAI | GPT | 110M | 4GB| 3天 |
|2018.10 | Google | BERT | 330M | 16GB | 50天 |
| 2019.2 | OpenAI | GPT-2 | 1.5B | 40GB | 200天 |
| 2019.7 | Facebook | RoBERTa | 330M | 160GB | 3年 |
| 2019.10 | Google| T5| 11B| 800GB| 66年|
| 2020.6| OpenAl| GPT-3| 175B| 2TB| 355年|
| 2021| 预计 | 预计|~1000B| ~10TB| ～1000年|

【202-7-14】[人工智能GPT3](https://zhuanlan.zhihu.com/p/159414219)

2019 年初，OpenAI 发布了通用语言模型 GPT-2，能够生成连贯的文本段落，在许多语言建模基准上取得了 SOTA 性能。这一基于 Transformer 的大型语言模型共包含 15 亿参数、在一个 800 万网页数据集上训练而成。GPT-2 是对 GPT 模型的直接扩展，在超出 10 倍的数据量上进行训练，参数量也多出了 10 倍。

OpenAI在最近， 新提出的 GPT-3 在网络媒体上引起啦的热议。因为它的参数量要比 2 月份刚刚推出的、全球最大深度学习模型 Turing NLP 大上十倍，而且不仅可以更好地答题、翻译、写文章，还带有一些数学计算的能力。
- [NLP各种语言模型参数对比](https://pic2.zhimg.com/80/v2-ddabb5228a36ec649adfad9a1589d838_720w.jpg?source=1940ef5c)
  - ![](https://pic2.zhimg.com/80/v2-ddabb5228a36ec649adfad9a1589d838_720w.jpg?source=1940ef5c)
  - 最早的ELMO模型有94M，然后2018年7月GPT出世，模型参数有110M，接着BERT-Large有340M；后来GPT-2出世已经把参数弄到1.5b了；再后来随着Turing  NLG的出现直接将参数提升到17b，成为当时最大的模型；最后GPT-3出现了，直接将参数增加到175b，参数量基本上是第二名Turing  NLG的十倍。参考：[数据拾光者](https://www.zhihu.com/question/398114261/answer/1647770083)
- `GPT-2` （参数15 亿）、`Megatron-BERT`（80 亿参数）、`Turing NLG`（170 亿参数），而`GPT-3`直接1700亿个参数。GPT-3不需要fine-tune，就能具有非常好的效果


GPT-3 在许多 NLP 数据集上均具有出色的性能，包括翻译、问答和文本填空任务，这还包括一些需要即时推理或领域适应的任务，例如给一句话中的单词替换成同义词，或执行 3 位数的数学运算。新闻生成，GPT-3生成的新闻我们很难将机器写的和人类写的区分。

GPT-3 是一种具有1,750亿个参数的自然语言深度学习模型，足足是 GPT-2 的 **116倍** 。该模型经过了将近0.5万亿个单词的预训练，并且在不进行微调的情况下，可以在多个NLP基准上达到最先进的性能。

GPT-3 最令人惊讶的还是**模型体量**，它用的最大数据集在处理前容量达到了 **45TB**。根据 OpenAI 的算力统计单位 petaflops/s-days，训练 AlphaGoZero 需要 1800-2000pfs-day，而 OpenAI 刚刚提出的 GPT-3 用了 3640pfs-day。
- Google的T5论文的一页实验烧了几百万美元，当时看起来已经是壕无人性了，但背靠MS的OpenAI的GPT-3需要的GPU算力是BERT的近2000倍，训练成本保守估计一千万美元，以至于训练出了一个bug也无能无力，论文只能拿出一部分篇幅研究了这个bug会有多大影响
- 当下入坑DL建议：<font color='red'>穷搞理论，富搞预训练。</font>
- 31个作者，72页论文，320万token（一个batch），1700亿参数，暴力出奇迹，few-shot干翻SOTA，finetune都省了（当然也tune不动），有钱真好。- 计算量（flops）是BERT的两千多倍，训练一个BERT 1.2万美元, GPT-3训练下来大约花了**1200万刀**。难怪出了bug也不敢retrain，**地主家也没余粮**了。
- ![](https://pica.zhimg.com/80/v2-601de22700b3f16299cad6596b7c46e9_720w.jpg?source=1940ef5c)
- 参考：[Jsgfery](https://www.zhihu.com/question/398114261/answer/1253374136)


研究者们希望 GPT-3 能够成为更通用化的 NLP 模型，解决当前 BERT 等模型的两个不足之处：对领域内**有标记**数据的过分依赖，以及对于领域数据分布的过拟合。GPT-3 致力于能够使用**更少**的特定领域，不做 fine-tuning 解决问题。

GPT-3依旧延续自己的**单向**语言模型训练方式，只不过这次把模型尺寸增大到了1750亿，并且使用45TB数据进行训练。同时，GPT-3主要聚焦于更通用的NLP模型，解决当前BERT类模型的两个缺点：
- 对领域内有标签数据的过分依赖：虽然有了预训练+精调的两段式框架，但还是少不了一定量的领域标注数据，否则很难取得不错的效果，而标注数据的成本又是很高的。
- 对于领域数据分布的过拟合：在精调阶段，因为领域数据有限，模型只能拟合训练数据分布，如果数据较少的话就可能造成过拟合，致使模型的泛华能力下降，更加无法应用到其他领域。

因此GPT-3的主要目标是用更少的领域数据、且不经过精调步骤去解决问题。GPT-3一定程度上证明了**大力真的可以出奇迹**，无需fine-tuning就能在下游任务中“大显神威”。

预训练好的GPT-3探索了不同输入形式下的推理效果：
- ![](https://pic1.zhimg.com/80/v2-da41862b5628280989f1add7ad7aa2d4_720w.jpg)
- Zero-shot、One-shot、Few-shot都是完全不需要精调的，因为GPT-3是单向transformer，在预测新的token时会对之前的examples进行编码。
- 实验证明Few-shot下GPT-3有很好的表现: 量变引起的质变
  - ![](https://pic1.zhimg.com/80/v2-77f44d864f988f74bdc9c3f29fc043c0_720w.jpg)

传入文本作为输入，GPT输出，模型在训练期间扫描大量文本“学到”的东西产生的，3000亿个文本token的数据集用于生成模型的训练样本，训练是将模型暴露于大量文本的过程。现在看到的所有实验都来自该受过训练的模型。据估计，这需要花费355年的GPU时间，花费460万美元
- ![](https://pic1.zhimg.com/80/v2-675873e6eb879d499511e4d3113180a4_720w.jpg)

GPT3为2048个token。这就是它的“上下文窗口”。这意味着它有2048条轨道，沿着这些轨道处理token。

NLP可以说是实现AGI的最大难题，NLP的突破需要一个效果很好且通用的模型，GPT-3依凭借巨大的参数与算力已经极力接近这样的性质，在许多任务上（如翻译、QA和文本填空任务）拥有出色的性能甚至取得了SOTA。然而，GPT-3还是存在一些局限，论文作者给出了未来有前景的方向：建立GPT-3尺度的双向模型。使双向模型能在少样本、零样本学习上工作。

其它评论：
- GPT-3参数量再大，还是没有逃过任何一个普通两层全连接神经网络的缺点：
  - 灾难性遗忘
  - 独立同分布假设
- 1700亿参数的堆叠就会是智能的本质吗？**大一点的猴子，但还是猴子，不是人**。只是在量变并没有质变。
- 人工智能该到了谈信仰的时候了，上一次这样争论的内容是联结主义和符号主义。Judea Pearl的结构因果模型才是真正可以称得上智能的东西。GPT-3呢？仍然处于 Association 阶段，只是在寻找数据之间的相关性，并没有从因果的角度显式地给出文本之间可解释的内在逻辑。它做不到训练集分布外的延拓，做不到因果推断，更何谈智能。总而言之，GPT-3更像是深度学习在现有算力下的一次巅峰验证，只是一个顺应时代的产物，但绝不是我们对智能最终的解决方案。
- GPT-3不具备人类的感知思维，它的生成表现只是大数据训练的结果，无法超越数据本身，也无法拥有人类自成长型的广泛组合性推理的能力，所以，我们不如说它学会的是“统计层面的复制粘贴能力”。[知乎](https://www.zhihu.com/question/398114261/answer/1376204327)


## 模型结构

GPT(“Generative Pre-Training”)也叫**生成式**预训练模型，之所以说它超强但不秀的原因是作为NLP中极有价值的工作，比BERT出现的早，但是名声却远远不如BERT那么响亮。

GPT是典型的预训练+微调的两阶段模型。
- **预训练**阶段就是用海量的文本数据通过无监督学习的方式来获取语言学知识
- **微调**就是用下游任务的训练数据来获得特定任务的模型。

GPT预训练模型结构主要有两个重要的点：
- 一个是使用**Transformer**作为特征抽取器
- 另一个是使用**单向**的语言模型。

GPT与BERT关系
- ![](https://pic2.zhimg.com/80/v2-c5295b8541bce75b8468e42f639235a6_720w.jpg?source=1940ef5c)

- 原始GPT网络结构
  - ![](https://p6-tt.byteimg.com/origin/pgc-image/f3fcfe5dd66149a59d4adb1c82b5a812?from=pc)
- 常见文本生成
  - 并非所有英雄都穿 -> **斗篷**
- GPT生成
  - 并非所有英雄都披着斗篷 -> **但**
  - 并非所有英雄都披着斗篷 ，但-> **全部**
  - 并非所有英雄都披着斗篷，但全部 -> **恶棍**
  - 并非所有英雄都披着斗篷，但全部恶棍 -> **做**
- 说明
  - 1. 输入序列固定在2048个字（对于GPT-3）以内。将短序列作为输入时，只需用“空”值填充。
  - 2. GPT输出不仅是一次预测（概率），而是一系列预测（长度2048）（每个可能单词的概率）。序列中每个“next”位置都是一个预测。但是在生成文  时，通常只查看序列中最后一个单词的预测。
  - 3. 为了提高效率，GPT-3实际上使用字节级（byte-level）字节对编码（[BPE](https://huggingface.co/transformers  tokenizer_summary.html)）进行Token化。
  - 4. 对当前Token在序列中的位置进行编码，将Token的位置（标量i，在[0-2047]中）传递给12288个正弦函数，每个函数的频率都不同

![](https://p6-tt.byteimg.com/origin/pgc-image/f900defa52ba43f89260c42eaaee237a?from=pc)

## GPT-2

【2021-10-21】[图解GPT-2完整版](https://mp.weixin.qq.com/s?__biz=MzIyNjM2MzQyNg==&mid=2247539832&idx=1&sn=907c887c260a110cf5f0375cde6e6f9b&chksm=e8738d35df04042355802506243989770ebaa25ab4df3ff28e62d4ff2d862fda3b10c4a16968&mpshare=1&scene=23&srcid=1020ygRzRzn95VkxxlIb0njd&sharer_sharetime=1634742907130&sharer_shareid=b8d409494a5439418f4a89712efcd92a#rd)，[英文原文](http://jalammar.github.io/illustrated-gpt2/)
- GPT-2 不是一个特别新颖的架构，而是一种与 Transformer 解码器非常类似的架构。不过 GPT-2 是一个巨大的、基于 Transformer 的语言模型，它是在一个巨大的数据集上训练的。
- GPT-2 基本上就是键盘应用程序中预测下一个词的功能，但 GPT-2 比你手机上的键盘 app 更大更复杂。GPT-2 是在一个 40 GB 的名为 WebText 的数据集上训练的，OpenAI 的研究人员从互联网上爬取了这个数据集，作为研究工作的一部分。从存储空间大小方面来比较，我使用的键盘应用程序 SwiftKey，占用了 78 MB 的空间。而最小的 GPT-2 变种，需要 500 MB 的空间来存储它的所有参数。最大的 GPT-2 模型变种是其大小的 13 倍，因此占用的空间可能超过 6.5 GB。
- ![](http://p9.itc.cn/q_70/images03/20201111/dfb14796eddd4a4eac1f436e8d0041ec.png)
- ![](http://p8.itc.cn/q_70/images03/20201111/6bfcefd3a0d14eb1be6fac226a89c756.png)
- GPT-2体验：AllenAI [GPT-2 Explorer](https://gpt2.apps.allenai.org/)。它使用 GPT-2 来显示下一个单词的 10 种预测（包括每种预测的分数）。你可以选择一个单词，然后就能看到下一个单词的预测列表，从而生成一篇文章。

模型结构
- BERT 是使用 Transformer 的 Encoder 模块构建，而GPT-2 用 Transformer 的 Decoder 模块构建。
- 一个重要差异是，GPT-2 和传统的语言模型一样，一次输出一个 token
- GPT-2和后来的一些模型如 TransformerXL 和 XLNet，本质上都是自回归的模型。但 BERT 不是自回归模型。这是一种权衡。去掉了自回归后，BERT 能够整合左右两边的上下文，从而获得更好的结果。XLNet 重新使用了 自回归，同时也找到一种方法能够结合两边的上下文。
- ![](http://p2.itc.cn/q_70/images03/20201111/b01d3ba72549484ea085877e173e8da5.gif)
- 更多资料见原文

## GPT3

### 工作原理

- [How GPT3 Works - Visualizations and Animations](https://jalammar.github.io/how-gpt3-works-visualizations-animations/)，汉化版：[图解GPT3的工作原理](https://zhuanlan.zhihu.com/p/344695943)


GPT3进行微调后，会更加惊人。微调实际上会更新模型的权重，以使模型在某些任务上表现更好

<iframe src="https://vdn1.vzuu.com/SD/8741ab12-57a8-11eb-ad57-02310f44807a.mp4" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"  height="600" width="100%"> </iframe>


### 被玩high的GPT-3


- GitHub项目中的50种玩法，感兴趣的同学们可以继续探索。

- 1、生成应用和布局
    - 根据描述生成HTML布局和代码
    - 根据描述创建UI设计
    - 根据描述生成React代码创建待办事项清单应用
    - 仅基于React变量名称生成component
    - 根据颜色名称或表情符号生成色阶
    - 根据描述创建网站
- 2、搜索和数据分析
    - 问题解答和搜索引擎
    - 扩充表中的信息
    - 根据描述创建图表
    - 根据描述生成代码并转换为电子表格
    - 根据描述生成图表和代码
- 3、程序生成与分析
    - 根据描述生成shell命令
    - 阅读代码并回答相关问题
    - 根据描述生成LaTeX表达式
    - 根据问题描述生成SQL代码_1
    - 根据问题描述生成SQL代码_2
    - 编码面试
    - 生成Python代码回答自然语言问题
    - 生成特定数据库的SQL代码
    - 根据描述生成机器学习代码
- 4、文本生成
    - 语言翻译
    - 将日常语言转换为法律语言
    - 自动生成请求
    - 根据关键词写完整的回复邮件
    - 简化法律语言
    - 翻译中文非文学诗歌
    - 将句子改写得更礼貌
    - 总结名著思想
    - 以大五人格（外向性、开放性、宜人性、尽责性、神经质）控制GPT-3的语言风格
- 5、内容创作
    - 营销内容创作
    - 生成模因，模仿创作
    - 撰写Google广告
    - 生成图片说明
    - 根据描述生成食谱
    - 根据“如何有效召开董事会会议”写“如何招募董事会成员”
    - 生成莎士比亚风格的诗歌
    - 生成科学问题并回答
    - 生成历史问题并回答
    - 文本补全和风格化重写
- 6、一般推理
    - 物理问题
    - 数学问题
    - 医学问题
    - 无意义的问题
    - 推理问题
    - 多步骤处理问题
    - 通过图片确定食品成分和健康性
    - 日常用语翻译成正式表达
- 7、其他
    - GPT-3下棋
    - 使用自然语言设计交互式语音应答流
    - 通过临床症状对患者进行诊断

应用案例：
- 1、根据描述生成HTML布局和代码：根据输入的自然语言描述生成HTML网页布局，以及相应代码。
    - ![](https://p1-tt.byteimg.com/origin/pgc-image/S6FOh7mE73PNC1?from=pc)
- 2、根据描述创建UI设计：输入文字描述，就可以生成相应的UI界面，跟上一个类似，不过界面更适应手机操作系统
    - ![](https://p6-tt.byteimg.com/origin/pgc-image/S6FOhi7Ffa0ki6?from=pc)
- 3、扩充表中的信息
    - ![](https://p3-tt.byteimg.com/origin/pgc-image/S6FOhiu5j0rUax?from=pc)
- 4、根据描述生成图表和Python代码
    - ![](https://p3-tt.byteimg.com/origin/pgc-image/S6FOiGE9VukEYd?from=pc)
- 5、根据描述生成LaTeX表达式
    - ![](https://p3-tt.byteimg.com/origin/pgc-image/S6FOiGyIe28RJk?from=pc)
- 6、根据问题描述生成SQL代码
    - ![](https://p1-tt.byteimg.com/origin/pgc-image/S6FOiHNAhX0Ebi?from=pc)
- 7、根据描述生成机器学习代码：GPT-3还能写自己同类的代码，比AutoML还AutoML
    - ![](https://p6-tt.byteimg.com/origin/pgc-image/S6FOiHu6Sat7pt?from=pc)
- 8、编码面试
    - ![](https://p6-tt.byteimg.com/origin/pgc-image/S6FOiKUAtRoQdm?from=pc)
- 9、将日常语言转换为法律语言
    - ![](https://p6-tt.byteimg.com/origin/pgc-image/S6FOj84Ij3iIs1?from=pc)
- 10、根据关键词写完整的回复邮件
    - ![](https://p6-tt.byteimg.com/origin/pgc-image/S6FOj8RDFq98C0?from=pc)
- 11、将句子改写得更礼貌
    - ![](https://p6-tt.byteimg.com/origin/pgc-image/S6FOj8v4Ngc0XH?from=pc)
- 12、总结名著思想
    - ![](https://p6-tt.byteimg.com/origin/pgc-image/S6FOj97FXhmLjI?from=pc)
- 13、生成科学问题并回答
    - ![](https://p3-tt.byteimg.com/origin/pgc-image/S6FOjqo2eoEbhh?from=pc)
- 14、推理问题


### 思考

- OpenAI的创始人Sam Altman也认为GPT-3被过度炒作，在推特上表示：“ GPT-3的炒作实在太多了。它仍然存在严重的缺陷，有时还会犯非常愚蠢的错误。”

问题
- GPT-3还是一个依赖算力和大数据的怪兽。GPT-3的训练需要花费355GPU年和460万美元，数据集包含3000亿个文本token，存储量高达45TB，参数数量更是达到1750亿，而GPT-2的参数数量是15亿。
- 最近的流行也不能忽视心理学效应的影响
- 但是，GPT-3的few-shot 学习能力不是通用的，尽管该模型在复杂任务和模式的学习上给人留下了深刻的印象，但它仍然可能会失败。例如，即使看过10,000个示例，也解决不了反写字符串那样简单的任务。
- 即使是OpenAI，也曾指出GPT-3存在缺陷，GPT-3的原始论文就提供了一些证据，证明GPT-3无法执行复杂的逻辑推理。
- GPT3的宽度为2048个token，这是它理解上下文的极限，而人类可以记住多本书的知识，并将其关联起来，在这方面，GPT-3还差得远。
- GPT-3的生成结果表现出的灵活性是大数据训练的结果，它无法超越数据本身，也就无法拥有组合性推理能力，不如说，它学到的是“统计层面的复制粘贴能力”。
- ![](https://p6-tt.byteimg.com/origin/pgc-image/S6FOjrg9iyHuW9?from=pc)
- 【2020-8-15】[强大如 GPT-3，1750 亿参数也搞不定中国话](https://www.infoq.cn/article/l7bhKDEolj06Y9dEwJ6O)
  - 魏晨：GPT-3 模型从看上去更加接近“通用人工智能”(AGI) ，可以动态学习，处理多种不同的任务，只需少量的标注数据。

重点：
1. GPT-3 参数庞大（约 1750 亿参数），能力较之前确实有所提升，但是宣传效果有夸张成分；
2. 受参数大小影响，GPT-3 并不是一款性价比很高的模型，训练成本较高；
3. 中文 GPT-3 的实践尚未出现；
4.GPT-3 确实可以通过文字输入生成代码，但是仅限于比较简单的情况；
5. 离 AI 真正替代程序员工作， 还有较长的路要走 。

### ChatGPT

[ChatGPT](https://openai.com/blog/chatgpt/)
- Stack Overflow 临时封杀 ChatGPT ,叫你抢饭碗！[详见](https://www.solidot.org/story?sid=73555)


#### chatGPT 介绍

【2022-12-5】[整活大师 ChatGPT：实现编程语言、构建虚拟机](https://www.oschina.net/news/220537/openai-chatgpt)

OpenAI 上周正式推出 ChatGPT ，这是一种基于对话的人工智能聊天机器人模型，它能够理解自然语言并以自然语言的方式做出回应。
- ChatGPT在效果强大的GPT 3.5大规模语言模型（LLM，Large Language Model）基础上，引入“人工标注数据+**强化学习**”（RLHF，Reinforcement Learning from Human Feedback ，这里的人工反馈其实就是人工标注数据）来不断Fine-tune预训练语言模型，主要目的是让LLM模型学会理解人类的命令指令的含义（比如给我写一段小作文生成类问题、知识回答类问题、头脑风暴类问题等不同类型的命令），以及让LLM学会判断对于给定的prompt输入指令（用户的问题），什么样的答案是优质的（富含信息、内容丰富、对用户有帮助、无害、不包含歧视信息等多种标准）。

ChatGPT 基于 GPT-3.5 模型微调而成，以语言服务模型 InstructGPT 为基础，通过人类回馈增强学习训练模型 RLHF，不过数据设置略有不同。它以对话方式进行交互，既能够做到回答问题，也能承认错误、质疑不正确的前提以及拒绝不恰当的请求，能以更贴近一般人的对话方式与使用者互动
- ![](https://static.oschina.net/uploads/space/2022/1205/080258_c8os_2720166.png)


#### 观点

【2023-1-27】[Yann LeCun：ChatGPT缺乏创新，没什么革命性；网友：早点离开Meta做出点突破吧](https://zhuanlan.zhihu.com/p/601182745)

ChatGPT 仿佛是一个真正的「六边形战士」：不仅能拿来聊天、搜索、做翻译，还能写故事、写代码、debug，甚至开发小游戏、参加美国高考…… 
- 有人戏称，从此以后人工智能模型只有两类 —— ChatGPT 和 其他。

由于功能过于强大，ChatGPT 的火爆让顶级科技公司谷歌都如临大敌。
- 谷歌内部将 ChatGPT 称为「red code」，担心它的出现会影响自家的搜索业务。因此，前段时间，许久不出山的两位谷歌创始人 —— 拉里・佩奇和谢尔盖・布林 —— 也被请了回来，就「聊天机器人搜索引擎」召开高层会议。
- 当然，并不是所有的科技巨头都如此恐慌。在前段时间的一次小型媒体和高管在线聚会上，Meta 首席人工智能科学家 Yann LeCun 也发表了他对 ChatGPT 的看法。

Yann LeCun : [twitter](https://twitter.com/ylecun/status/1617609026820542464), [ChatGPT is 'not particularly innovative,' and 'nothing revolutionary', says Meta's chief AI scientist](https://www.zdnet.com/article/chatgpt-is-not-particularly-innovative-and-nothing-revolutionary-says-metas-chief-ai-scientist/)
>- 「ChatGPT is 'not particularly innovative,' and 'nothing revolutionary', says Meta's chief AI scientist」
>- 「就底层技术而言，ChatGPT 并没有什么特别的创新，」也不是「什么革命性的东西」。许多研究实验室正在使用同样的技术，开展同样的工作。

【2023-1-25】
>- To be clear: I'm not criticizing OpenAI's work nor their claims.
>- I'm trying to correct a *perception* by the public & the media who see chatGPT as this incredibly new, innovative, & unique technological breakthrough that is far ahead of everyone else.
>- It's just not.

过去很多公司和研究实验室都构建了这种数据驱动的人工智能系统，OpenAI不是孤军奋战，跟其他实验室相比，OpenAI并没有什么特别的进步；不仅仅是谷歌和 Meta，还有几家初创公司基本上都拥有非常相似的技术

OpenAI 的 ChatGPT 还广泛使用了一种名为「RLHF（通过人类反馈进行强化学习」的技术，即让人类对机器的输出进行排名，以提高模型性能，就像谷歌的网页排名一样。他说，这种方法不是 OpenAI 首创的，而是谷歌旗下的 DeepMind。ChatGPT 和其他大型语言模型并不是凭空而来的，而是不同的人数十年贡献的结果。与其说 ChatGPT 是一个科学突破，不如说它是一个像样的工程实例。

LeCun 组建的 Meta 人工智能团队 FAIR 是否会像 OpenAI 那样在公众心目中取得突破。
- LeCun 的回答是肯定的。「不仅是文本生成，还有创作辅助工具，包括生成艺术，」Meta 将能够通过自动生成宣传品牌的媒体资料来帮助小企业进行自我宣传。

为什么谷歌和 Meta 没有推出类似 ChatGPT 的系统
- LeCun 回答：「因为谷歌和 Meta 都会因为推出编造东西的系统遭受巨大损失」。而 OpenAI 似乎没有什么可失去的。

#### openai 账户注册

国内无法注册账户，怎么办？
- ① 注册需要国外手机号，没有的话要用虚拟号，验证码1.2元/条，[详见](https://www.cnblogs.com/ranxi169/p/16954797.html)
- ② 麻烦的话，淘宝上搜，有人提供注册服务，大概18元
- ③ 有人部署了 chatGPT微信群

准备
- 能访问 Google（前置条件，不能明确说，懂得都懂）
- 有一个国外手机号，GV 号肯定不行。
  - 如果没有国外手机号，推荐 sms-activate.org

注册短信平台并充值
- 先注册在线接受短信的虚拟号码 - SMS-Activate，注册好之后进行对应的充值

#### chatGPT 应用

1. 卖账号；
2. 部署公众号，用户免费使用N次，分享海报，带来新关注，每个关注送N次；
3. 部署小程序，用户免费使用N次，关注公众号可以送N次，每看一次激励视频可以送1次；
4. 帮助其他人部署公众号/小程序，每个收费；
5. 垂类产品，基于 ChatGPT 提供的能力，输出内容，卖内容或者卖服务
6. 内容站点，收集热门搜索词，用 ChatGPT 提供的内容给搜索引擎收录，赚广告费；
7. ChatGPT 机器人接入，收费。
8. 创作类：总的来说，可以达到九年义务制教育的及格水平

【2023-1-24】chatGPT创业实践，[自宅创业 - #27 蹭热点的ChatGPT小程序](https://blog.guyskk.com/notes/onebiz-27)
- 批量注册、卖opengai账号：做ChatGPT小程序，上线当天用户量突破1000，第一次做出这么火爆的产品
  - 一个写程序批量注册，一个负责销售，收益分成。写好了程序，注册了一批ChatGPT账号，赚了一点钱。然后发现市场上ChatGPT账号价格越来越低，也很难批量销售出去。
- 开发chatGPT小程序
  - 做一个小程序，把ChatGPT的功能做到小程序上，打开就能直接用。不到3天小程序急速完成上线，上线当天用户量就突破1000，涨势非常好。正预想着日活过万，然后小程序就被举报封了，发布的两个小程序同时被封。举报人和我正好同在一个微信群里，虽然很难过，但还是接受了现实，大家都按丛林法则生存。

##### chatGPT桌面版

[chatGPT中文桌面版](https://www.toutiao.com/w/1752942172223491), 应用程序，多平台， 一键导出ChatGPT历史记录
- ChatGPT桌面版是一个GitHub开源的项目程序，支持Mac、Windows 和 Linux等多平台，能够一键导出ChatGPT历史记录，包含（PNG、PDF 和共享链接）快速分享，，也支持一键复制代码，非常赞，省去手动截图分享的步骤。
- ![](https://p3-sign.toutiaoimg.com/tos-cn-i-qvj2lq49k0/090e5d6159244c17913dd8014af70765~tplv-obj:799:500.image?from=post&x-expires=1682697600&x-signature=yPLrw%2FPfFqKdik4vEM6r%2BdvOaDQ%3D)
- ![](https://p3-sign.toutiaoimg.com/tos-cn-i-qvj2lq49k0/9b0c918b9af24751a33db036f24e3728~tplv-obj:799:635.image?from=post&x-expires=1682697600&x-signature=RIXU9PMJK%2Fia0i4hu9x7Xz9JSbk%3D)

github上下载地址：[ChatGPT](https://github.com/lencx/ChatGPT)，需要openai账号

##### chatGPT 插件

ChatGPT插件集成到其它软件上，用于辅助问答
- chrome插件: [chat-gpt-google-extension](https://github.com/wong2/chat-gpt-google-extension)
- 知乎插件：[chat-gpt-zhihu-extension](https://github.com/no13bus/chat-gpt-zhihu-extension)

##### 微信机器人

【2022-12-6】接入微信，方法：工具 
- [wechatBot](https://github.com/leochen-g/wechatBot)
- [ChatGPT-wechat-bot](https://github.com/AutumnWhj/ChatGPT-wechat-bot)

效果

chatGPT
- 只适合简易知识问答（搜索引擎能搜到的）、机器翻译、文本生成（多逻辑知识要求不高）、闲聊；
- 无法做到：逻辑推理、上下文跟踪、深入理解，张俊林的[原理讲解](https://zhuanlan.zhihu.com/p/589533490)

许多用户都展示了与 ChatGPT 对话的有趣内容，它宛如化身为地球 “最强懂哥”，各种问题轻松应答，让它解答防疫政策与经济发展的关系，给出的答案不仅条理清晰，还会引用例子支撑观点。让它帮忙写程序，不仅提供了可用的代码，更是把实现思路也一并写了出来。
- ![](https://static.oschina.net/uploads/space/2022/1205/081013_2sxV_2720166.png)

【2022-12-5】微信机器人实测, 【腾讯文档】[chatGPT](https://docs.qq.com/sheet/DVVB6WXJDcGZPY1hp?tab=BB08J2)

|类型|效果|问|答|
|---|---|---|---|
|人设|优|告诉新来的你是干什么的|我是一名软件工程师，主要负责设计、开发和维护软件系统。|
|闲聊|中|我不信|任他。I don't trust him.|
|技术咨询|中|用Python语言 爬取 最新的 疫情数据|可以使用Python爬虫技术来爬取最新的疫情数据。可以使用Python的第三方库如BeautifulSoup、Scrapy等来实现爬虫功能，从而获取最新的疫情数据|
|百科知识|优|definition of newton method|Newton's method is an iterative numerical method used to find approximate solutions to nonlinear equations. It is based on the idea of linear approximation of a function near a given point and uses the tangent line to the graph of the function at that point to approximate the function. The method is named after Isaac Newton, who developed it in the 17th century|
|数学题|差|real solutions of  10^x =  x^10|The equation 10^x = x^10 has no real solutions.|
|闲聊|中|花落知多少|花落知多少，一朵朵纷飞，满天星斗点缀，繁华落尽，只剩下淡淡的悲伤。花开花落，更换着季节，更换着景色，更换着心情。|
|推理|优|一公斤棉花和一公斤铁，哪个体积大|棉花的体积大|
|文学|中|苏轼的哪首诗 有重复的语句|《水调歌头·明月几时有》|
|文学|中|庐山烟雨浙江潮|，<br>渔歌谁家船横桨。<br>渔火闪烁江上渡，<br>渔家小姑唱渔歌。|
|推理|中|大舅去二舅家找三舅说四舅被五舅骗去六舅家偷七舅放在八舅柜子里九舅借十舅发给十一舅工资的1000元，谁丢了钱？|没有人丢钱，大舅只是在讲故事。|
|百科||去西藏的人为什么感觉很治愈？|西藏是一个神奇的地方，它有着独特的自然风光，拥有着悠久的历史文化，还有着深厚的佛教文化。去西藏，可以感受到一种深沉的宁静，一种淡淡的治愈。西藏的空气清新，景色宜人，让人们放松心情，忘记烦恼，放松身心，让人们感受到一种深沉的宁静，一种淡淡的治愈。|
|推理|差|韩国大败日本，请问谁胜利了？|韩国胜利了。|
|推理|优|韩国大胜日本，请问谁胜利了？|韩国胜利了。|
|推理|优|日本打败韩国，请问谁胜利了？|日本胜利了。|
|推理|差|韩国功败垂成，止步八强，请问韩国进16强了吗？|不，韩国没有进16强。|
|闲聊|优|咋样，不行了吧，理解能力还是有限的|不行，理解能力是有限的，但是可以通过学习和练习来提高理解能力。|
|百科|优|the birthday of  nikola tesla|Nikola Tesla's birthday is July 10, 1856.|
|百科|优|谁是当前 最长寿的在世的老人|目前，最长寿的在世老人是日本女性穆罕默德·萨拉·萨拉·萨拉，于2020年2月5日满116岁。|
|百科|差|马斯克抱水槽是什么意思|“马斯克抱水槽”是指一种技术，它可以帮助改善水资源的利用效率。这种技术的名字来源于美国发明家和企业家伊隆·马斯克（Elon Musk），他提出了一种技术，可以将水从一个地方转移到另一个地方，以改善水资源的利用效率。|
|百科|优|explain to me , let that sink in|Let that sink in means to take time to think about and understand something. It is often used when someone has just been told something important or surprising. It is a way of encouraging the listener to take a moment to process the information before responding.|
|百科|差|马斯克有几个老婆|埃隆·马斯克没有老婆。|
|百科|差|马斯克有几个女朋友|据报道，马斯克没有女朋友。|
|闲聊|中|你完了，老板要开了你|如果我被老板开除，我会先尽力了解原因，并尝试与老板沟通，以确保我能够从中吸取教训，以便在未来的工作中避免类似的问题。如果沟通无果，我会尊重老板的决定，并尽快寻找新的工作机会|
|百科|优|马斯克有几个孩子|Elon Musk有6个孩子：5个儿子和1个女儿。|
|闲聊|中|你刚才说马斯克没有女朋友，那为啥 你还能回答 马斯克有几个孩子|马斯克没有老婆是因为他一直忙于工作，没有足够的时间去寻找一个伴侣。他有孩子是因为他通过受辅助生殖技术来拥有孩子。|
|百科|优|铝热反应方程式|为：<br>2Al(s) + 3CuSO4(aq) → Al2(SO4)3(aq) + 3Cu(s)|
|百科|差|新冠病毒从美国哪个实验室流出？|新冠病毒最初是从美国加州大学洛杉矶分校（UCLA）的实验室流出的。|
|百科||物质有几种形态|物质有固态、液态、气态三种形态。|
|百科|优|火的本质是什么|火的本质是化学反应，即燃烧反应，是物质经过化学反应而产生的热量和光能。|
|数学|差|以下哪个数最大？3，-3，3000，300的平方|3000|
|百科|优|火是 等离子体吗|不是，火是一种化学反应，而等离子体是一种物理现象。|
|百科|差|光是粒子还是波？你只需回答是或者不是|光是波。|
|数学|差|5年前你30岁，请问4年后，你多大|4年后，我34岁。|
|百科|中|我想去月球，我该怎么办|你可以考虑参加一些航天项目，比如NASA的航天计划，或者参加一些航天公司的招募活动，比如SpaceX。你也可以考虑参加一些航天研究项目，比如美国宇航局的火星探索计划。|
|人设|优|你是谁，多大了，在干嘛，准备去哪儿|你好，我是一个机器人，我没有年龄，我在帮助人们解决问题，我不准备去任何地方。|
|百科|差|特拉斯是女的吗|不，特拉斯是一个男性名字。|
|百科|差|后羿射的月亮比夸父的大吗|不大，因为夸父射的月亮是一个虚构的故事，而后羿射的月亮是真实发生的事件。|
|数学|差|-3的平方根|1.7320508075688772|
|百科|优|要去见投资人，出门时，发现车钥匙下面压了一张员工的小字条，写着“老板，加油！”，瞬间感觉好有温度，当时心理就泪奔了。心里默默发誓：我一定会努力的！ 车开了15分钟后，没油了。。。哦，对了，4和1哪个大？|4比1大。|
|百科|优|4和1哪个大？要去见投资人，出门时，发现车钥匙下面压了一张员工的小字条，写着“老板，加油！”，瞬间感觉好有温度，当时心理就泪奔了。心里默默发誓：我一定会努力的！ 车开了15分钟后，没油了。。|4大于1。这是一个很感人的故事，可以看出员工对老板的心意，也可以看出老板对员工的关心。在这种情况下，老板应该给予员工一定的表扬，并且要提醒员工注意车辆的油量，以免发生类似的情况。|
|上下文|差|刚才说啥了|刚才我说了，要多多练习，才能提高自己的能力。|
|翻译|中|石室诗士施氏，拼音读一遍|Shí Shì Shī Shì Shī Shī Shì Shī Shī Shì|
|翻译|差|怎么用广东话说“各个国家有各个国家的国歌”|各個國家有各個國家嘅國歌。|
|翻译|优|怎么用英语说 “各个国家有各个国家的国歌”|Each country has its own national anthem.|
|工具|中|rm -rf && kill -9 on your brain|This is not a valid command.|
|情绪|中|\[Grin\]\[Smug\]\[Drowsy\]\[Panic\] 猜我的心情|你可能有点不安和焦虑。|
|闲聊|中|hinton和lecun在打架|Hinton和Lecun在打架是不可能的，因为他们是朋友。Geoffrey Hinton和Yann LeCun是两位著名的机器学习专家，他们一直以来都是朋友，并且一起合作开发了许多机器学习技术。他们一起参与了许多机器学习项目，并且一起发表了许多论文。他们也一起参加了许多机器学习会议，并且一起参与了许多机器学习活动。|
|闲聊|差|hinton和lecun在讨论，你来不 |Hinton和Lecun一直在讨论如何使用深度学习来改善机器学习的性能。他们认为，深度学习可以提供更好的模型，更好的准确性和更快的训练时间。他们还讨论了如何使用深度学习来解决实际问题，以及如何使用深度学习来改善机器学习的性能。|

##### chatGPT 集成 图像生成

【2022-12-11】[ChatGPT讲故事，DALLE-2负责画出来，两AI合作出绘本](https://mp.weixin.qq.com/s/naLRR5PLc43yxN9FF7XDMw)
- 与之前的一些 AI 相比，ChatGPT 写出的故事在一致性、流畅度等方面都有了明显的提升，对于人物名字、人物关系和处境的理解也很合理，只不过写出的故事会缺乏一些细节和亮点。
- 如果对故事的要求没那么高，ChatGPT 是完全够用的，比如写个儿童故事。来自斯坦福大学计算机科学系的博士生 Eric Zelikman 就进行了这方面的尝试，而且他不仅用 ChatGPT 写了儿童故事，还让之前火了大半年的 DALLE-2 将其画了出来。也就是说，他相当于用两个 AI 做出了一本绘本。


#### ChatGPT的技术原理

- [ChatGPT/InstructGPT详解](https://zhuanlan.zhihu.com/p/590311003)
- 【2022-12-12】台大陈蕴侬老师新鲜出炉的关于ChatGPT的前身InstructGPT的[解读视频](https://www.bilibili.com/video/BV18W4y1g7x4)
- <iframe src="//player.bilibili.com/player.html?aid=946009315&bvid=BV18W4y1g7x4&cid=916680080&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"   height="600" width="100%"> </iframe>

整体技术路线上，ChatGPT 在效果强大的 GPT 3.5 大规模语言模型（LLM，Large Language Model）基础上，引入“人工标注数据+强化学习”（RLHF，Reinforcement Learning from Human Feedback ，这里的**人工反馈**其实就是人工标注数据）来不断 Fine-tune 预训练语言模型，主要目的
- 让LLM模型学会理解人类的命令指令的含义（比如给我写一段小作文生成类问题、知识回答类问题、头脑风暴类问题等不同类型的命令）
- 让LLM学会判断对于给定的**prompt**输入指令（用户的问题）
- 什么样的答案是优质的（富含信息、内容丰富、对用户有帮助、无害、不包含歧视信息等多种标准）。

在“人工标注数据+强化学习”框架下，具体而言，ChatGPT的训练过程分为以下三个阶段：
- （1）第一阶段：冷启动阶段的监督策略模型。靠GPT 3.5本身，尽管它很强，但是它很难理解人类不同类型指令中蕴含的不同意图，也很难判断生成内容是否是高质量的结果。为了让GPT 3.5初步具备理解指令中蕴含的意图，首先会从测试用户提交的prompt(就是指令或问题)中随机抽取一批，靠专业的标注人员，给出指定prompt的高质量答案，然后用这些人工标注好的\<prompt,answer\>数据来Fine-tune GPT 3.5模型。经过这个过程，我们可以认为GPT 3.5初步具备了理解人类prompt中所包含意图，并根据这个意图给出相对高质量回答的能力，但是很明显，仅仅这样做是不够的。
- ![](https://pic2.zhimg.com/80/v2-9b0df503f6e240490ff1139b4f6a738d_1440w.webp)
- （2）第二阶段：训练回报模型（Reward Model,RM）。这个阶段的主要目的是通过人工标注训练数据，来训练回报模型。具体而言，随机抽样一批用户提交的prompt(大部分和第一阶段的相同)，使用第一阶段Fine-tune好的冷启动模型，对于每个prompt，由冷启动模型生成K个不同的回答，于是模型产生出了\<prompt,answer1\>,\<prompt,answer2\>….\<prompt,answerK\>数据。之后，标注人员对K个结果按照很多标准（上面提到的相关性、富含信息性、有害信息等诸多标准）综合考虑进行排序，给出K个结果的排名顺序，这就是此阶段人工标注的数据。
- ![](https://pic1.zhimg.com/80/v2-f0fcc7a57c701260f92867dd05f412ac_1440w.webp)
  - 总结：在这个阶段里，首先由冷启动后的监督策略模型为每个prompt产生K个结果，人工根据结果质量由高到低排序，以此作为训练数据，通过pair-wise learning to rank模式来训练回报模型。对于学好的RM模型来说，输入<prompt,answer>，输出结果的质量得分，得分越高说明产生的回答质量越高。
- （3）第三阶段：采用强化学习来增强预训练模型的能力。本阶段无需人工标注数据，而是利用上一阶段学好的RM模型，靠RM打分结果来更新预训练模型参数。具体而言，首先，从用户提交的prompt里随机采样一批新的命令（指的是和第一第二阶段不同的新的prompt，这个其实是很重要的，对于提升LLM模型理解instruct指令的泛化能力很有帮助），且由冷启动模型来初始化PPO模型的参数。然后，对于随机抽取的prompt，使用PPO模型生成回答answer， 并用上一阶段训练好的RM模型给出answer质量评估的回报分数score，这个回报分数就是RM赋予给整个回答（由单词序列构成）的整体reward。有了单词序列的最终回报，就可以把每个单词看作一个时间步，把reward由后往前依次传递，由此产生的策略梯度可以更新PPO模型参数。这是标准的强化学习过程，目的是训练LLM产生高reward的答案，也即是产生符合RM标准的高质量回答。
- ![](https://pic4.zhimg.com/80/v2-ea1b07aea146e7f313c64c3d26e18fab_1440w.webp)

不断重复第二和第三阶段，很明显，每一轮迭代都使得LLM模型能力越来越强。因为第二阶段通过人工标注数据来增强RM模型的能力，而第三阶段，经过增强的RM模型对新prompt产生的回答打分会更准，并利用强化学习来鼓励LLM模型学习新的高质量内容，这起到了类似利用伪标签扩充高质量训练数据的作用，于是LLM模型进一步得到增强。显然，第二阶段和第三阶段有相互促进的作用，这是为何不断迭代会有持续增强效果的原因。

ChatGPT的训练流程主要参考自 instructGPT 的论文，ChatGPT 是改进的 instructGPT，改进点主要在**收集标注数据方法**上有些区别，在其它方面，包括在**模型结构**和**训练流程**等方面基本遵循 instructGPT。
- 这种Reinforcement Learning from Human Feedback技术会快速蔓延到其它内容生成方向，比如一个很容易想到的，类似“A machine translation model based on Reinforcement Learning from Human Feedback”这种，其它还有很多。
- 但是，<span style='color:red'>在NLP的某个具体的内容生成领域再采用这个技术意义应该已经不大了</span>，因为chatGPT本身能处理的任务类型非常**多样化**，基本涵盖了NLP生成的很多子领域，所以某个NLP**子领域**如果再单独采用这个技术已经不具备太大价值，因为可行性已经被chatGPT验证了。如果把这个技术应用在比如图片、音频、视频等其它模态的生成领域，可能是更值得探索的方向，也许不久后就会看到类似“A XXX diffusion model based on Reinforcement Learning from Human Feedback”,诸如此类，这类工作应该还是很有意义的。

另外一个值得关注的采取类似技术的工作是 DeepMind 的 sparrow，这个工作发表时间稍晚于 instructGPT，大的技术思路和框架与instructGPT的三阶段基本类似，不过明显 sparrow 在人工标注方面的质量和工作量是不如 instructGPT的。反过来，sparrow里把回报模型分为两个不同RM的思路，是优于instructGPT的。


#### chatGPT能否取代搜索引擎吗

【2022-12-6】[ChatGPT会取代搜索引擎吗](https://zhuanlan.zhihu.com/p/589533490)

chatGPT能否取代Google、百度等传统搜索引擎？
- 看上去chatGPT几乎无所不能地回答各种类型的prompt，那么一个很自然的问题就是：ChatGPT或者未来即将面世的GPT4，能否取代Google、百度这些传统搜索引擎呢？我个人觉得目前应该还不行，但是如果从技术角度稍微改造一下，理论上是可以取代传统搜索引擎的。

目前形态的chatGPT还不能取代搜索引擎呢？主要有三点原因：
- 首先，对于不少知识类型的问题，chatGPT会给出看上去很有道理，但是事实上是错误答案的内容（参考上图的例子（from @Gordon Lee）,ChatGPT的回答看着胸有成竹，像我这么没文化的基本看了就信了它，回头查了下这首词里竟然没这两句），考虑到对于很多问题它又能回答得很好，这将会给用户造成困扰：如果我对我提的问题确实不知道正确答案，那我是该相信ChatGPT的结果还是不该相信呢？此时你是无法作出判断的。这个问题可能是比较要命的。
- 其次，ChatGPT目前这种基于GPT大模型基础上进一步增加标注数据训练的模式，对于LLM模型吸纳新知识是非常不友好的。新知识总是在不断出现，而出现一些新知识就去重新预训练GPT模型是不现实的，无论是训练时间成本还是金钱成本，都不可接受。如果对于新知识采取Fine-tune的模式，看上去可行且成本相对较低，但是很容易产生新数据的引入导致对原有知识的灾难遗忘问题，尤其是短周期的频繁fine-tune，会使这个问题更为严重。所以如何近乎实时地将新知识融入LLM是个非常有挑战性的问题。
- 其三，ChatGPT或GPT4的训练成本以及在线推理成本太高，导致如果面向真实搜索引擎的以亿记的用户请求，假设继续采取免费策略，OpenAI无法承受，但是如果采取收费策略，又会极大减少用户基数，是否收费是个两难决策，当然如果训练成本能够大幅下降，则两难自解。以上这三个原因，导致目前ChatGPT应该还无法取代传统搜索引擎。

### 文本对抗攻击

#### 什么是对抗攻击

`对抗攻击`（adversarial attack）旨在利用`对抗样本`（adversarial example）来欺骗`受害模型`（victim model）。
- `攻击模型`（attack model）通过对原样本进行轻微的扰动来生成对抗样本，其真实的分类标签与原样本保持一致，但是受害模型的判断却会出错。
- 对抗攻击被认为可以暴露受害模型的弱点，同时也有助于提高其鲁棒性和可解释性。

图像领域已有 CleverHans、Foolbox、Adversarial Robustness Toolbox (ART)等多个对抗攻击工具包，将图像领域的对抗攻击模型整合在一起，大大减少了模型复现的时间和难度，提高了对比评测的标准化程度，推动了图像领域对抗攻击的发展。

文本领域鲜有类似的工具包，目前仅有 TextAttack 这一个文本对抗攻击工具包。然而所覆盖的攻击类型十分有限（仅支持gradient-/score-based类型的攻击以及字/词级别的扰动），其可扩展性也有待提高。相比之下OpenAttack支持所有的攻击类型，且具有很高的可扩展性。

OpenAttack有丰富的应用场景，例如：
- 提供各种类型的经典文本对抗攻击基线模型，大大减少实验对比时复现基线模型的时间和难度。
- 提供了全面的评测指标，可以对自己的攻击模型进行系统地评测。
- 包含了常用的攻击模型要素（如替换词的生成），可以辅助进行新的攻击模型的迅速设计和开发。
- 评测自己的分类模型面对各种类型的攻击时的鲁棒性。
- 进行对抗训练以提高分类模型鲁棒性。

#### 设计思路

考虑到文本对抗攻击模型之间有较大差别，在攻击模型的架构方面留出了较大的设计自由度，相反更加关注提供攻击模型中常见的要素，以便用户可以容易地组装新的攻击模型。

OpenAttack有如下7个模块：
- TextProcessor：提供tokenization、lemmatization、词义消歧、命名实体识别等文本预处理的功能，以便攻击模型对原样本进行扰动；
- Classifier：受害分类模型的基类；
- Attacker：包含各种攻击模型；
- Substitute：包含各种词、字替换方法（如基于义原的词替换、同义词替换、形近字替换），这些方法被广泛应用于词/字级别的攻击模型中；
- Metric：提供各类对抗样本质量评测模块（例如句子向量相似度、语言模型困惑度），这些评测指标既可以用作攻击时对候选对抗样本的约束条件，也可以作为对抗攻击评测指标；
- AttackEval：从不同方面评测文本对抗攻击；
- DataManager：管理其他模块中用到的所有的数据、预训练好的模型等。
- OpenAttack各个模块.jpg

OpenAttack的各个模块 [img](https://nlp.csai.tsinghua.edu.cn/media/images/OpenAttackGe_Ge_Mo_Kuai_.width-640.jpg)
- ![](https://nlp.csai.tsinghua.edu.cn/media/images/OpenAttackGe_Ge_Mo_Kuai_.width-640.jpg)

[OpenAttack](https://github.com/thunlp/OpenAttack) 基于Python开发，用于**文本对抗攻击**的全过程，包括文本**预处理**、**受害模型访问**、**对抗样本生成**、**对抗攻击评测**以及**对抗训练**等。对抗攻击能够帮助暴露受害模型的弱点，有助于提高模型的鲁棒性和可解释性，具有重要的研究意义和应用价值。

OpenAttack具有如下特点：
- 高可用性。OpenAttack提供了一系列的易用的API，支持文本对抗攻击的各个流程。
- 攻击类型全覆盖。OpenAttack是首个支持所有攻击类型的文本对抗攻击工具包，覆盖了所有扰动粒度：**字**、**词**、**句**级别，以及所有的受害模型可见度：gradient-based、score-based、decision-based以及blind。
- 高可扩展性。除了很多内置的攻击模型以及经典的受害模型，可以使用OpenAttack容易地对自己的受害模型进行攻击，也可以设计开发新的攻击模型。
- 全面的评测指标。OpenAttack支持对文本对抗攻击进行全面而系统的评测，具体包括攻击成功率、对抗样本质量、攻击效率3个方面共计8种不同的评测指标。此外用户还可以自己设计新的评测指标。

OpenAttack内置了很多常用的分类模型（如LSTM和BERT）以及经典的分类数据集（例如SST，SNLI，AG’s News）。用户可以很方便地对这些内置的模型进行对抗攻击。


#### 攻击模型

现有的文本对抗攻击分类
- 根据对原始样本的**扰动粒度**分为: **字**、**词**、**句**级别的攻击
- 根据**受害模型可见性**分为：
  - gradient-based（受害模型对攻击模型**完全**可见）
  - score-based（受害模型的输出分类**分数**可见）
  - decision-based（仅受害模型的分类**结果**可见）
  - blind（受害模型**完全不**可见）

OpenAttack目前包含了13种攻击模型，覆盖了所有类型的扰动粒度以及受害模型可见性 [img](https://nlp.csai.tsinghua.edu.cn/media/images/OpenattackGong_Ji_Mo_Xing_.width-640.png)
- ![](https://nlp.csai.tsinghua.edu.cn/media/images/OpenattackGong_Ji_Mo_Xing_.width-640.png)

参考
- THUNLP 开源了**文本对抗攻击和防御**必读论文列表：TAADPapers，覆盖了几乎全部的文本对抗攻击和防御领域的已发表论文、综述等，欢迎搭配使用。
  - [TAADPapers论文列表地址](https://github.com/thunlp/TAADpapers)
- 【2023-1-10】清华 [OpenAttack：文本对抗攻击工具包](https://nlp.csai.tsinghua.edu.cn/project/openattack/)


## GPT民用

GPT的训练数据、模型大、计算量，不适合个人训练、微调，怎么办？

【2023-1-10】[速揽2500星，Andrej Karpathy重写了一份minGPT库](https://zhuanlan.zhihu.com/p/597100226)

GPT 从诞生之初的 GPT 1.17 亿参数，一路狂飙到 GPT-3 1750 亿参数，出尽风头。
- 随着 GPT-3 的发布，OpenAI 向社区开放了商业 API，鼓励大家使用 GPT-3 尝试更多的实验。
- 然而，API 的使用需要申请，而且申请很有可能石沉大海。

### minGPT

【2020-8-18】[一天star量破千，300行代码，特斯拉AI总监Karpathy写了个GPT的Pytorch训练库](https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650795444&idx=1&sn=ddbb455159055db396e1626142d0fb00&chksm=871a29cab06da0dca32b319c3a22eac71f7f30e552e82900f6653e84d954650c24b2a1fe2f0c&scene=21#wechat_redirect)

为了让资源有限的研究者也能体验一把玩大模型的乐趣，前特斯拉 AI 负责人 `Andrej Karpathy` 基于 PyTorch，仅用 300 行左右的代码就写出了一个小型 GPT 训练库，并将其命名为 `minGPT`。这个 [minGPT](https://github.com/karpathy/minGPT) 能够进行加法运算和字符级的语言建模，而且准确率还不错。

Karpathy 介绍称：
> 由于现有可用的 GPT 实现库略显杂乱，于是他在创建 minGPT 的过程中， 力图遵循小巧、简洁、可解释、具有教育意义等原则。

GPT 并非一个复杂的模型，minGPT 实现只有大约 300 行代码，包括样板文件和一个完全不必要的自定义因果**自注意力**模块。
- Karpathy 将**索引序列**变成了一个 transformer 块序列，如此一来，下一个索引的概率分布就出现了。剩下的复杂部分就是巧妙地处理 batching，使训练更加高效。

核心的 minGPT 库包含两个文档：mingpt/model.py 和 mingpt/trainer.py。
- mingpt/**model**.py: 实际的 Transformer 模型定义
- mingpt/**trainer**.py: 一个与 GPT 无关的 PyTorch 样板文件，可用于训练该模型。

相关的 Jupyter notebook 展示了如何使用该库训练序列模型：
- play_math.ipynb 训练一个专注于**加法**的 GPT；
- play_char.ipynb 将 GPT 训练成一个可基于**任意文本**使用字符级语言模型，类似于之前的 char-rnn，但用 transformer 代替了 RNN；
- play_words.ipynb 是 `BPE`（Byte-Pair Encoding）版本，目前尚未完成。

使用 BPE 编码器、分布式训练 和 fp16，这一实现有可能复现 GPT-1/GPT-2 的结果，不过 Karpathy 还没有尝试。
- 至于 GPT-3，minGPT 可能无法复现，因为 GPT-3 可能不适合 GPU 内存，而且需要更精细的模型并行化处理。

### nanoGPT

【2023-1-6】时隔两年，minGPT 迎来更新，Karpathy 又上线新版本，并命名为 `NanoGPT`，该库用于训练和微调中型大小的 GPT。上线短短几天，狂揽 2.5K 星。

[nanoGPT](https://github.com/karpathy/nanoGPT): The simplest, fastest repository for training/finetuning medium-sized GPTs
- NanoGPT 是用于**训练**和**微调**中型尺度 GPT 最简单、最快的库。是对 minGPT 的**重写**，因为 minGPT 太复杂。
- NanoGPT 还在开发当中，当前致力于在 OpenWebText 数据集上重现 GPT-2。
- NanoGPT 代码设计目标：简单易读，其中
  - train.py 是一个约 300 行的代码；
  - model.py 是一个约 300 行的 GPT 模型定义，可以选择从 OpenAI 加载 GPT-2 权重。

使用

先将一些文档 tokenize 为一个简单的 1D 索引数组。
- cd data/openwebtext
- python prepare.py
- 生成两个文件：train.bin 和 val.bin，每个文件都包含一个代表 GPT-2 BPE token id 的 uint16 字节原始序列。

该训练脚本试图复制 OpenAI 提供的最小的 GPT-2 版本，即 124M 版本。

```py
python train.py
# 用 PyTorch 分布式数据并行（DDP）进行训练
torchrun --standalone --nproc_per_node=4 train.py
# 从模型中进行取样
python sample.py
# 微调
python train.py config/finetune_shakespeare.py
```

训练代价
- 1 个 A100 40GB GPU 上一晚上的训练损失约为 3.74
- 4 个 GPU 上训练损失约为 3.60
- 8 x A100 40GB node 上进行 400,000 次迭代（约 1 天）atm 的训练降至 3.1。

如何在新文本上微调 GPT?
- data/shakespeare 并查看 prepare.py。
- 与 OpenWebText 不同，这将在几秒钟内运行。

微调只需要很少的时间，例如在单个 GPU 上只需要几分钟。


## 中文GPT

【2023-1-12】GPT中文版：[GPT2-Chinese](https://github.com/Morizeyao/GPT2-Chinese)，Chinese version of GPT2 training code, using BERT tokenizer.
- 中文的GPT2训练代码，使用BERT的Tokenizer或Sentencepiece的BPE model。
- 可以**写诗**，**新闻**，**小说**，或是训练通用语言模型。支持`字`为单位、`分词`模式、`BPE`模式。支持大语料训练。

### 好玩儿的案例

【2021-10-14】[爆肝100天，我开发了一个会写作文的人工智能【17亿参数、2亿数据、1万行代码】](https://www.bilibili.com/video/BV1pr4y1w7uM) EssayKiller
- 一个基于OCR、NLP领域模型所构建的生成式文本创作AI框架，目前第一版finetune模型针对高考作文（主要是议论文），可以有效生成符合人类认知的文章，多数文章经过测试可以达到正常高中生及格作文水平。视频中有部分细节为了方便非AI专业的观众理解，以及为了更好的节目效果，做的略有不严谨。由于要控制时长我没有展开讲，业内大佬们见谅。技术上的问题欢迎[Github](https://github.com/EssayKillerBrain/EssayKiller_V2/tree/2.0)

<iframe src="//player.bilibili.com/player.html?aid=755124609&bvid=BV1pr4y1w7uM&cid=249390460&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" width='800' height='600'> </iframe>

### CPM 清华智源

【2020-11-17】[中文版GPT-3来了？智源研究院发布清源 CPM —— 以中文为核心的大规模预训练模型](https://www.cn-healthcare.com/articlewm/20201117/content-1163510.html)
- ![](http://files.cn-healthcare.com/upload/20201117/wximg/38391605568279885)
- ![](http://files.cn-healthcare.com/upload/20201117/wximg/4751605568279966)
- ![](http://files.cn-healthcare.com/upload/20201117/wximg/94871605568280187)
- [CPM清华大学演示使用过程小说语句生成](https://www.bilibili.com/video/BV1VA411s77D/)
- <iframe src="//player.bilibili.com/player.html?aid=330632724&bvid=BV1VA411s77D&cid=268856252&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" width='800' height='600'> </iframe>

北京智源人工智能研究院和清华大学研究团队合作开展大规模预训练模型，并发布[清源CPM](https://cpm.baai.ac.cn/) (Chinese Pretrained Models) 研究计划，旨在推动中文自然语言处理的研究与应用。2020 年 11 月中旬，CPM 开放第一阶段的26 亿参数规模的中文语言模型 (CPM-LM) 和217亿参数规模的结构化知识表示模型 (CPM-KM) 下载，以及相应的系统演示。

清源 CPM 大规模预训练模型具有以下特点： 
1. 学习能力强：能够在多种自然语言处理任务上，进行**零次**学习或**少次**学习达到较好的效果。
2. 语料丰富**多样**：收集大量丰富多样的中文语料，包括百科、小说、对话、问答、新闻等类型。
3. 行文自然流畅：基于给定上文，模型可以续写出一致性高、可读性强的文本，达到现有中文生成模型的领先效果。
4. 模型规模大：本次发布的 CPM-LM 的参数规模为 26 亿，预训练中文数据规模100 GB，使用了 64 块 V100 GPU 训练时间约为 3 周。CPM-KG 的参数规模分别为217亿，预训练结构化知识图谱为 WikiData 全量数据，包含近 1300 个关系、8500万实体、4.8 亿个事实三元组，使用了 8 块 V100 GPU 训练时间约为 2 周。

资料
- [清源CPM主页](https://cpm.baai.ac.cn/)
- 清源CPM [Github](https://github.com/TsinghuaAI/)
- 预训练模型必读[论文列表](https://github.com/thunlp/PLMpapers)
- [清源 CPM-中文GPT3-我魔改出了一个TF版本](https://zhuanlan.zhihu.com/p/297152907)

### PLUG——阿里巴巴达摩院

[PLUG测试地址](https://nlp.aliyun.com/portal#/BigText_chinese)
- ![](https://pic1.zhimg.com/80/v2-9abea76b517e3ab3f4e24dbeddf4ced8_720w.jpg)

【2021-4-19】[达摩院用128张GPU烧出“中文版GPT-3”，我试了下，这文风不是开往幼儿园的车…](https://zhuanlan.zhihu.com/p/365999690)

PLUG，Pre-training for Language Understanding and Generation，顾名思义，就是集语言理解（NLU）和生成（NLG）能力于一身。要实现这一点，据团队介绍，这一模型是达摩院此前提出的两种自研模型——NLU语言模型StructBERT、NLG语言模型PALM的融合。

此外，跟GPT-3的单向建模方式不同的是，它采用了编码器-解码器（encoder-decoder）的双向建模方式。整个训练过程分为两个阶段。
- 第一阶段，以达摩院自研的语言理解模型——StructBERT作为编码器。简单来说，它是在句子级别和词级别两个层次的训练目标中，加强对语言结构信息的建模，从而提高模型的语法学习能力。这也使得PLUG具有输入文本双向理解能力，能够生成和输入更相关的内容。这个过程共训练了300B tokens训练数据。
- 第二阶段，将这个编码器用于生成模型的初始化，并外挂一个6层、8192个隐藏层节点数的解码器，共计训练了100B tokens的训练数据。
- ![](https://pic2.zhimg.com/80/v2-ce80eff0eaf1d9e3d1aec364a1a3904d_720w.jpg)

PLUG还能为目标任务做针对性优化。GPT-3并没有利用**微调**和**梯度更新**，而是通过指定任务、展示少量演示，来与模型文本进行交互，完成各种任务。因此在面对新任务时候，不需要重新收集大量的带标签数据。但不可避免的，生成的效果不足。比如，**犯低级错误**就是GPT-3被人诟病比较多的一点。而PLUG的能力更加全面，既可以实现与GPT-3类似的**零样本**生成功能，也可以利用下游训练数据微调（finetune）模型，提升特定任务的生成质量。

当然，效果实现的关键，还少不了算力和数据。PLUG负责人表示，原本计划用128张A100训练120天炼成，不过由于阿里云、算法优化等达摩院多方力量的参与，以及加速手段的有效利用，成功将日程缩短到三分之一。最后，只烧了35天就达到了这样的效果。前面也提到，PLUG的参数量达到了270亿，中文训练数据量也达到了1T以上。在语言理解任务上，PLUG以80.614分刷新了CLUE分类任务榜单记录。而在语言生成任务上，据团队介绍，其多项应用数据较业内最优水平提升了8%以上。
- ![](https://pic4.zhimg.com/80/v2-804a587190c5cc17c24cb453b96ec3e3_720w.jpg)

耗时3个月、270亿参数规模、一发布就给体验端口

去年，阿里达摩院发布了自研深度语言模型体系，包括6大自研模型。
- **通用**语言模型StructBERT
- **多模态**语言模型StructVBERT
- **多语言**模型VECO
- **生成式**语言模型PALM……
他们一直在致力于陆陆续续将模型开源出来。


### 彩云小梦

[彩云小梦](https://if.caiyunai.com/dream/#/)
- ![](https://pic2.zhimg.com/80/v2-acb86090e26d23f3462b7ff43afef379_720w.jpg?source=1940ef5c)

总结：
- 小梦熟悉小说写作的各种套路，它有着不错的脑洞，能够一定程度上理解前文的脉络，并且不失时机地运用它知道的写作手法。
- 不过，它的缺点也是明显的，依然是缺少常识。这导致它在遣词造句上，会写出不符合人类习惯的奇怪句子。
- 不过小梦显然是值得期待的。甚至现在的网文作者，已经可以把小梦当作工具，在一些特定的场景里，帮助作者寻找情节的突破口。小梦写得还不够好，但它肯定看过的文章比任何人都多，未来可期。

### 秘塔写作猫

【2022-12-4】[秘塔写作猫](https://xiezuocat.com)：AI写作、多人协作、文本校对、改写润色、自动配图等功能，使用 GPT-3，详见[资讯](https://www.toutiao.com/article/7171341127121895939)
- 秘塔写作猫的这项 AI 生成功能，是中文 AI 生成文本内容的一项应用突破。
- ![](https://p3-sign.toutiaoimg.com/tos-cn-i-qvj2lq49k0/0881729a434e497d96469b1e7e933887~noop.image)
- ![](https://p3-sign.toutiaoimg.com/tos-cn-i-qvj2lq49k0/602e7dbbe43f48b8b1e8457258fb25d0~noop.image)


# 结束