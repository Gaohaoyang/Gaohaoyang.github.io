---
layout: post
title:  "贝叶斯概率系列-Bayes"
date:   2020-07-26 15:02:00
categories: 数学基础 机器学习
tags: 贝叶斯 频率学派 贝叶斯学派 点估计 分布估计 参数估计 区间估计 MLE MAP HMM CRF 概率分布
excerpt: 贝叶斯理论及相关知识点
author: 鹤啸九天
mathjax: true
---

* content
{:toc}


# 资料

- [六大概率分布](http://www.csuldw.com/2016/08/19/2016-08-19-probability-distributions/)
- 【2018-11-15】数说工作室：[概率论-上帝的赌术](https://mp.weixin.qq.com/s?__biz=MjM5MDEzNDAyNQ==&mid=200838250&idx=1&sn=d0efd4cfa4a79112ebd43009c57c22eb&scene=19#wechat_redirect)，协和八：说人话的统计学，[做统计，多少数据才算够](https://mp.weixin.qq.com/s?__biz=MzAxMDA4NjU3OA==&mid=207643438&idx=1&sn=20fbf90250185008f841fffe28bf4e9b&scene=1&srcid=0710wI1mdNbPso36OkPa9LXY#rd)，【2018-5-20】【精华】[说人话的统计学-合集](https://mp.weixin.qq.com/s?__biz=MzAxMDA4NjU3OA==&mid=401048178&idx=1&sn=b8e79ee70f99fcc6a6c3f7c51c3c3f96&chksm=094693af3e311ab915b1a0675cdd05eec8a04125a12dd6f568105a1685438c7e3e9c8015ea6f&mpshare=1&scene=23&srcid=0710wjvWkpUbrUlCJqkEZQvd&sharer_sharetime=1587351730042&sharer_shareid=b8d409494a5439418f4a89712efcd92a#rd)，【2019-08-30】[统计之都](https://cosx.org/)，【2020-3-11】[概率统计思维的建立](https://zhuanlan.zhihu.com/p/112172344)

- 【2018-8-13】[贝叶斯概率模型一览](https://www.toutiao.com/a6585030992715579911/?tt_from=mobile_qq&utm_campaign=client_share&timestamp=1534170408&app=news_article&utm_source=mobile_qq&iid=39213720304&utm_medium=toutiao_android&group_id=6585030992715579911)，[贝叶斯概率-精简讲解](https://www.toutiao.com/a6696083742659707400/?tt_from=android_share&utm_campaign=client_share&timestamp=1559173967&app=news_article_lite&utm_medium=toutiao_android&req_id=20190530075247010023072207214183F&group_id=6696083742659707400)
  - ![](http://p6-tt.byteimg.com/large/a06d00076f15d3682ffe)
  - 统计学习根据任务类型可以分为监督学习、半监督学习、无监督学习、增强学习等
  - 每类任务中，又可以将各类模型归结为概率模型和非概率模型，以监督学习为例
    - `概率模型`(`生成模型`)通过函数 F 来描述 X 和 Y 的联合概率或者条件概率分布，如 P(X\|Y)；
    - `非概率模型`（`判别模型`）通过函数 F 来直接描述 X 到 Y 的映射，如 Y = f(X)。
- 【2018-8-16】[可视化讲解贝叶斯推断](http://students.brown.edu/seeing-theory/bayesian-inference/index.html#section3)
- 【2018-9-7】深度学习与贝叶斯暑期培训教程[DeepBayes](http://deepbayes.ru/),含ppt、视频，github地址：[Seminars DeepBayes Summer School 2018](https://github.com/bayesgroup/deepbayes-2018)，[video视频](https://www.youtube.com/playlist?list=PLe5rNUydzV9Q01vWCP9BV7NhJG3j7mz62),[slides资料](https://drive.google.com/drive/folders/1rJ-HTN3sNTvhJXPoXEEhfGlZWtjNY26C)
- 【2018-11-30】[透彻理解马尔科夫蒙特卡洛](https://www.toutiao.com/a6627436642611233294/?tt_from=mobile_qq&utm_campaign=client_share&timestamp=1543535821&app=news_article&utm_source=mobile_qq&iid=52365900690&utm_medium=toutiao_android&group_id=6627436642611233294)，马尔科夫链可视化讲解[Markov Chains](http://setosa.io/ev/markov-chains/)
- 【2018-11-29】[透彻理解最大似然估计](https://www.toutiao.com/a6625581290538140167/?tt_from=mobile_qq&utm_campaign=client_share&timestamp=1543506091&app=news_article&utm_source=mobile_qq&iid=52365900690&utm_medium=toutiao_android&group_id=6625581290538140167)
- [统计学基础知识【脑图笔记】](http://www.cnblogs.com/xiaofeng1234/p/5987845.html)


# 基本概念


## 概率与统计

- 概率论和统计学的区别
  - 简单来说，概率论和统计学解决的问题是互逆的。假设有一个具有不确定性过程（process），然后这个过程可以随机的产生不同的结果（outcomes）。则概率论和统计学的区别可以描述为：
    - `概率论`（probability theory）中，已知该过程的概率模型，该模型的不确定性由相应的概率分布来描述；概率论要回答的问题是该过程产生某个结果的可能性有多大这类问题。
    - `统计学`（statistics）中，该过程的概率模型是未知的，但是有一系列该过程产生的结果的观测值；希望通过这些观测值来推断出这个过程中的不确定性是什么样的。
  - 总结来说就是：（类似归纳与演绎）
    - 通过已知的概率模型来精确的计算各种结果的可能性就是`概率论`；——`演绎`
    - 根据观测的结果来推断模型的不确定性就是`统计学`。——`归纳`

## 概率分布

六大分布：**伯努利**分布、**二项**分布、**多项式**分布、**Beta**分布、Dirichlet分布、**高斯**分布

### 正态分布

- 【2021-5-2】[高中就开始学的正态分布，原来如此重要](https://www.toutiao.com/i6711190906105496077/)

机器学习的世界是以概率分布为中心的，而概率分布的核心是正态分布。正态分布也被称为高斯分布, 以天才卡尔·弗里德里希·高斯（Carl Friedrich Gauss）的名字命名的。简单的预测模型一般都是最常用的模型，因为易于解释，也易于理解。现在补充一点：正态分布因为简单而流行。

正态分布是一条倒钟形曲线，样本的平均值、众数以及中位数是相等
![](https://p3-tt.byteimg.com/origin/pgc-image/5a44734ccb0949ebbffc76311c76935a?from=pc)

概率密度函数
![](https://p6-tt.byteimg.com/origin/pgc-image/28d2088a7e86416e8cd9253c23d31f3c?from=pc)

示例，非常接近正态分布：人群的身高、成年人的血压、扩散后的粒子的位置、测量误差、人群的鞋码、员工回家所需时间。周围的大部分变量都呈置信度为 x% 的正态分布（x<100）

两个参数分别是：样本的平均值和标准差。
- 平均值——样本中所有点的平均值。
- 标准差——表示数据集与样本均值的偏离程度。
分布的这一特性让统计人员省事不少，因此预测任何呈正态分布的变量准确率通常都很高。

为什么这么多变量近似正态分布？
- 大量随机变量上多次重复一个实验时，它们的分布总和将非常接近正态性（normality）—— **中心极限定理**
- 把大量分布不同的随机变量加在一起，新变量最终也服从正态分布

分布转换
- 如果样本满足某个未知的分布，那么通过一系列操作，总是能变成正态分布。
- 相反，标准正态分布的叠加与转换，也一定能变化为任意未知分布。从标准正态转换到未知分布，就是很多机器学习模型希望做到的，不论是视觉中的 VAE 或 GAN，还是其它领域的模型。

但对于传统统计学，更希望将特征的分布转换成正态分布，因为正态分布简单又好算呀

转换为标准正态
- 线性变换
  - ![](https://p6-tt.byteimg.com/origin/pgc-image/b5dddc3d5cb54e6e86b718a741e3cd83?from=pc)
  - x 可能服从某个未知分布，但是归一化后的 Z 是服从正态分布
- Box-cox 变换: 用 Python 的 SciPy 包将数据转换成正态分布
  - scipy.stats.boxcox(x, lmbda=None, alpha=None)
- YEO-JOHBSON 变换
  - Python 的 sci-kit learn 提供了合适的函数：
  - sklearn.preprocessing.PowerTransformer(method=’yeo-johnson’, standardize=True, copy=True)

注意：没有做任何分析的情况下不能轻易假设变量服从正态分布，以泊松分布（Poisson distribution）、t 分布（student-t 分布）或二项分布（Binomial distribution）的样本为例，如果错误地假设变量服从正态分布可能会得到错误的结果。


## 频率学派与贝叶斯学派


- [马同学高等数学](https://www.matongxue.com/lessons/686/)

|维度|频率派|古典派|主观派|
|---|---|----|---|
|理论基础|过往事实的归纳总结|不充分理由原则|知识和直觉|
|概率定义|频率稳定性|等概率|信念强度|

- 注：`不充分理由原则`
>雅各布·伯努利（1654－1705）提出，如果因为无知而没办法判断哪一个结果更容易出现，那么应该给予它们相同的概率

- 以不充分理由原则为基础，经由皮埃尔-西蒙·拉普拉斯侯爵（1749－1827）之手，确立了 古典概率 的定义，即：
>未知的概率都为等概率

- 整个19世纪的人们都广泛接受这个定义，并发展出了一系列的定义和定理。
- `贝特朗悖论`：法国数学家贝特朗（也翻译为“伯特兰”）于1888年在他的著作《Calcul des probabilités》中提到了这个悖论，锯木厂的木头，人们开始反思古典概率中的不合理之处：**“等概率”的描述实在是太模糊了，存在歧义**。
  - 边长的分布是未知的，所以是等概率的
  - 面积的分布是未知的，所以是等概率的
- 现代概率论通过分布来描述边长的随机性后，这种模糊性消失了，贝特朗悖论解决

**古典统计学和贝叶斯统计学**

- 统计学领域中有两大学派：`古典统计学`（classical）和`贝叶斯统计学`（Bayesian，以英国数学家托马斯•贝叶斯命名）。古典统计学又称为`频率论`（frequentist）。
- 关于这俩大学派孰优孰劣已有一个世纪的争论。它们的本质区别在于对待未知模型或者参数的方法是不同的：
  - ![](http://p1-tt.byteimg.com/large/a06f00023320ec9a1b61)
  - 古典统计学认为，未知的模型或者参数是**确定**的，只不过我们不知道它确切的形式或者取值。
    - 古典统计学通过进行大量重复实验并统计某个特定结果出现的频率作为对未知参数的估计。
    - 大数定律
  - 贝叶斯统计学认为，未知的模型或者参数变量是**不确定**的，但是这种不确定性可以由一个概率分布来描述。
    - 贝叶斯统计学“使用概率的方法来解决统计学问题”——贝叶斯公式+主观概率
      - `先验分布`（prior distribution）：根据主观判断或者过去的经验，猜测概率分布
      - `后验分布`（posterior distribution）：根据越来越多的观测值（new data 或者 new evidence）来修正猜测得到的概率分布
    - 贝叶斯统计学中的“概率”的概念可以被解释为对未知变量不同取值的信心程度的测度（measure of confidence）
    - 贝叶斯统计学派被古典统计学派诟病的核心问题是对于未知变量的先验分布是非常主观的。
    - 适合场景：选举、疾病诊断
      - 无法大量重复试验
      - 合理的先验分布对未知量的估计是非常有益的
    - 关于贝叶斯统计的一个笑话，代表着很多吃瓜群众对贝叶斯统计的看法，以及贝叶斯统计学派的自嘲：
>- A Bayesian is one who, vaguely expecting a horse, and catching a glimpse of a donkey, strongly believes he has seen a mule.
>- 一个贝叶斯学派的学者是这样的：模糊的期待着一匹马（**先验**），却看到了一头驴（**证据**），于是便自信的认为那是一头骡子（**后验**）。

- [频率学派与贝叶斯学派之争](http://www.cnblogs.com/549294286/archive/2013/04/08/3009073.html)：[知乎网友解释](https://www.zhihu.com/question/20587681/answer/21294468),频率学派最先出现，疯狂打压新生的贝叶斯学派，贝叶斯很凄惨，就跟艺术圈的梵高一样，死后的论文才被自己的学生发表，经过拉普拉斯之手发扬光大，目前二派就像华山派的剑宗和气宗。频率学派挺煞笔的，非得做大量实验才能给出结论，比如你今年高考考上北大的概率是多少啊？频率学派就让你考100次，然后用考上的次数除以100。而贝叶斯学派会找几个高考特级教师对你进行一下考前测验和评估，然后让这几个教师给出一个主观的可能性，比如说：你有9成的把握考上北大。这个区别说大也大，说小也小。
  - （1）往大了说，**世界观**就不同，频率派认为参数是客观存在，不会改变，虽然未知，但却是固定值；贝叶斯派则认为参数是随机值，因为没有观察到，那么和是一个随机数也没有什么区别，因此参数也可以有分布，个人认为这个和量子力学某些观点不谋而合。
  - （2） 往小处说，频率派最常关心的是**似然函数**，而贝叶斯派最常关心的是**后验分布**。我们会发现，后验分布其实就是似然函数乘以先验分布再normalize一下使其积分到1。因此两者的很多方法都是相通的。贝叶斯派因为所有的参数都是随机变量，都有分布，因此可以使用一些基于采样的方法（如MCMC）使得我们更容易构建复杂模型。频率派的优点则是没有假设一个先验分布，因此更加客观，也更加无偏，在一些保守的领域（比如制药业、法律）比贝叶斯方法更受到信任。
   - 频率 vs 贝叶斯 =   P(X;w)  vs  P(X\|w) 或 P(X,w)
   - 频率学派认为参数固定，通过无数字实验可以估计出参数值——客观；
   - 贝叶斯学派认为参数和数据都是随机的，参数也服从一定的分布，需要借助经验——主观

## 参数估计

- 【2020-8-1】[三种参数估计方法](https://learning.snssdk.com/feoffline/toutiao_wallet_bundles/toutiao_learning_wap/online/article.html?item_id=6772498016138953223&app_name=news_article)

两大流派：
- `频率学派`：参数未知但固定。`极大似然估计`MLE（`点估计`）
- `贝叶斯学派`：参数未知且变化。`最大后验估计`MAP（`点估计`）、`贝叶斯估计`（`分布估计`）
- 注：频率是贝叶斯的一个特例，隐含了先验知识

方法
- `极大似然估计`：用样本数据进行点估计
- `最大后验估计`：用样本数据+先验知识进行点估计
- `贝叶斯估计`：用样本数据+先验知识进行分布估计

![](https://p3.pstatp.com/large/pgc-image/8b8e3f3de8a84894a52ccbe1877bfb7e)
![](https://p3.pstatp.com/large/pgc-image/8d065abba83141e3aca9f7f4e60c4b85)


## 矩

【2020-5-29】用4个数来概括一个分布
- **均值** mean: 第一矩。表位置
- **方差** variance: 第二矩。表胖瘦
- **偏度** skewness: 第三矩。表歪斜
- **峰度** kurtosis: 第四矩。表尾巴胖瘦

- 力学上的矩，质量（mass）：
  - 0 阶矩：是总**质量**；
  - 1 阶矩除以总质量是**质心**（质量中心）；
  - 2 阶矩是**惯性矩**（moment of inertia）

- 统计学上的矩
  - 0 阶矩：`总概率`，也就是 1；
  - 1 阶矩：`均值`，
  - 2 阶中心矩：`方差`；如果是多元，对应协方差
  - 3 阶中心矩：`偏度`（skewness）
  - 4 阶中心矩（归一化和平移）：`峰态`（kurtosis）

# HMM

- [NLP硬核入门-隐马尔科夫模型HMM](https://zhuanlan.zhihu.com/p/87632700)

## 基本概念

### 1.1 定义、假设和应用

一个例子了解隐马尔科夫模型HMM。假设：
- （1）小明所在城市的天气有{**晴天**，**阴天**，**雨天**}三种情况，小明每天的活动有{**宅**，**打球**}两种选项。
- （2）我们只知道他每天参与了什么活动，而不知道他所在城市的天气是什么样的。
- （3）这个城市每天的天气情况，会和前一天的天气情况**有点关系**。譬如说，如果前一天是晴天，那么后一天是晴天的概率，就大于后一天是雨天的概率。
- （4）小明所在的城市，一年四季的**天气情况都差不多**。
- （5）小明每天会根据当天天气情况决定今天进行什么样的活动。
- （6）我们想通过小明的活动，猜测他所在城市的天气情况。

那么，城市天气情况和小明的活动选择，就构成了一个**隐马尔科夫模型** `HMM`，我们下面用学术语言描述下：
- （1）HMM的基本定义： 
> HMM是用于描述由隐藏的**状态序列**和显性的**观测序列**组合而成的**双重随机过程**。

在前面的例子中，城市天气就是隐藏的状态序列，这个序列是观测不到的。小明的活动就是观测序列，这个序列是能够观测到的。这两个序列都是随机序列。
- （2）HMM的假设一：**马尔科夫性**假设
>当前时刻的状态值，仅依赖于**前一时刻**的状态值，而不依赖于更早时刻的状态值。
>每天的天气情况，会和前一天的天气情况有点关系。

马尔科夫性：随机过程中某事件的发生**只**取决于它的上一事件，是“**无记忆**”过程。
- （3）HMM的假设二：**齐次性**假设。
> 状态转移概率矩阵**与时间无关**。即所有时刻共享同一个状态转移矩阵。
> 小明所在的城市，一年四季的天气情况都差不多。
- （4）HMM的假设三：**观测独立性**假设。
> 当前时刻的观察值，仅依赖于当前时刻的状态值。
> 小明每天会根据当天的天气情况，决定今天进行什么样的活动。
- （5）HMM的应用目的：通过可观测到的数据，预测不可观测的数据。想通过小明的活动，猜测他所在城市的天气情况。

- HMM被广泛应用于**标注**任务。在标注任务中，状态值对应着标记，任务会给定观测序列，以预测其对应的标记序列。
- HMM属于**生成模型**，是**有向图**。
 
### 1.2 三个基本问题

在学习HMM的三个问题：
- （1）**概率计算**问题：给定模型参数和观测序列，计算该观测序列的概率。是后面两个问题的基础。
- （2）学习训练问题：给定观测序列，估计模型参数。
- （3）**解码**预测问题：给定模型参数和观测序列，求概率最大的状态序列。
这三个问题会贯穿我们学习HMM的整个过程

## 2.概率计算问题

### 2.1 标记符号和参数
 
先约定一下HMM的标记符号，并通过套用上文的例子来理解：
 
（1）状态值集合（一共有N种状态值）：![](https://pic3.zhimg.com/80/v2-916988fbfc7f9adede2d190e4e433702_1440w.jpg)
 
天气的状态值集合为{晴天，阴天，雨天}。
 
（2）观测值集合（一共有M种观测值）：![](https://pic2.zhimg.com/80/v2-ae7b265245bd82f35637399d30be36a5_1440w.jpg)
 
小明活动的观测值集合为{宅，打球}。
 
（3）状态值序列：![](https://pic4.zhimg.com/80/v2-64942caa26d69040bc80091c236982f7_1440w.jpg)
 
每一天的城市天气状态值构成的序列。
 
（4）观测值序列：![](https://pic4.zhimg.com/80/v2-2fb5e320486472fb9be5c5661459f1e3_1440w.jpg)
 
每一天的小明活动的观测值构成的序列。
 
（5）序列状态值、观测值下标：idx(t)，表示t时刻的状态值或观测值，取的是状态值、观测值集合中的第idx(t)个。

再给出HMM模型的三个参数：A，B，π。
- A：状态转移概率矩阵。表征转移概率，维度为N*N。
- B：观测概率矩阵。表征发射概率，维度为N*M。
- π：初始状态概率向量。维度为N*1。
- λ=(A,B,π)，表示模型的所有参数。
 
同样通过上文的例子来理解这三个参数。
 
状态转移概率矩阵A：
 
![](https://pic4.zhimg.com/80/v2-281005e58f29765b253d82632957b173_1440w.jpg)

 
观测概率矩阵B：

![](https://pic4.zhimg.com/80/v2-736397689e8650c7839f234994680c1f_1440w.jpg)

初始概率状态向量π：

![](https://pic1.zhimg.com/80/v2-469efe7a1869cead41eb91aadd016528_1440w.jpg)
 
### 2.2 前向后向算法
 
我们通过前向后向算法，来计算特定观测序列出现的概率。
 
_（2.2节和2.3节涉及大量的公式推导，没有兴趣的童鞋可以跳过不看，不影响文章其它部分的理解）_
 
#### 2.2.1前向算法模型
 
定义：![](https://pic4.zhimg.com/80/v2-1c40e0fb01b8713620292350e90f4133_1440w.jpg)
 
表示在模型参数已知的条件下，预测1~t时刻观测值为特定序列以及t时刻状态值为特定值的概率。
 
前向算法模型的思路是：利用t时刻的α值，去预测t+1时刻的α值。使用的是迭代的思路。
 
（1）模型初值：![](https://pic1.zhimg.com/80/v2-1939d924ae746f5f8b7d8e6c359ddedc_1440w.jpg)
 
（2）迭代公式：
![](https://pic4.zhimg.com/80/v2-a73b53eb08c4b80d6552944f3aeec087_1440w.jpg)
 
（3）模型终值：
 
![](https://pic1.zhimg.com/80/v2-54ca9da0acb7faecfeb9522528e1c3ec_1440w.jpg)
 
#### 2.2.2后向算法模型
 
定义：![](https://pic2.zhimg.com/80/v2-e79ee77dc940ea7b0c5ba6a31d42c29d_1440w.jpg)
 
。表示在模型参数和t时刻状态值已知的条件下，预测t+1之后所有时刻观测值为特定序列的概率。
 
后算法模型的思路是：利用t+1时刻的β值，去预测t时刻的β值。使用的是递归的思路。
 
（1）模型初值：人为地定义，![](https://pic3.zhimg.com/80/v2-de5a92a575fc1d61a224039f052df9ea_1440w.jpg)
 
（2）递归公式：
 
![](https://pic3.zhimg.com/80/v2-3610d4a38fe79b08e5c148b92e632662_1440w.jpg)
 
（3）模型终值：
 
![](https://pic2.zhimg.com/80/v2-98aaf41d104bcfa52640bcdf0b1dd2e5_1440w.jpg)
 
  
 
#### 2.2.3前向后向算法模型公式
 
![](https://pic4.zhimg.com/80/v2-15a1990d44f8a231370c9d09f48cd12b_1440w.jpg)
 
这个公式比李航教授书里的公式更容易理解些，《统计学习方法》书里的公式是：
 
![](https://pic4.zhimg.com/80/v2-be22517e9cd44c2da7d03e616f3d0ff7_1440w.jpg)
 
这两个公式都是正确的，这两个公式在推导过程中获取的以下两个公式，在2.3节有不同的应用：
 
![](https://pic1.zhimg.com/80/v2-f4fbfad8be9029f4be9f1326cb28bb84_1440w.jpg)
 
![](https://pic4.zhimg.com/80/v2-623e4fce7f775e4cde9c9ecf6277e757_1440w.jpg)
 
### 2.3 一些概率和期望的计算
 
#### 2.3.1两个常用的概率公式
 
（1）![](https://pic2.zhimg.com/80/v2-8444d45db770ab9136cc2b6745022425_1440w.jpg)
 
给定模型λ和观测序列，求时刻t处于指定状态的概率。
 
![](https://pic3.zhimg.com/80/v2-1e2e90a90feae438dc5950ca9f94181a_1440w.jpg)
 
（2）![](https://pic4.zhimg.com/80/v2-d77b7d7704a6a2a96b6006d206fe2b9f_1440w.jpg)
 
给定模型λ和观测序列，求时刻t和t+1处于指定状态的概率。
 
![](https://pic4.zhimg.com/80/v2-9de2c9e23bffa10a6bf85942be117ad7_1440w.jpg)
 
#### 2.3.2 三个常用的期望公式
 
在观测序列O的条件下，状态i出现的期望值：![](https://pic4.zhimg.com/80/v2-8ed8d813051d0e6e868100e754abb887_1440w.jpg)

在观测序列O的条件下，由状态i转移的期望值：![](https://pic4.zhimg.com/80/v2-c11123f03db45290dc7fb4474982cc93_1440w.jpg)
 
在观测序列O的条件下，由状态i转移到状态j的期望值：![](https://pic2.zhimg.com/80/v2-aa4308a9b08ab8db85bbc73f3ddd8531_1440w.jpg)

## 3.学习训练问题
 
模型参数的学习，可以根据有无标注数据，分为有监督和无监督两类。
 
在HMM里，如果训练数据包含观测序列和状态序列，即为有监督学习。如果训练数据只包含观测序列，即为无监督学习。
 
在有监督学习中，可以直接通过统计方法，求得模型的状态转移概率矩阵A、观测概率矩阵B、初始状态概率向量π。
 
在无监督学习中，模型使用Baum-Welch算法来求得模型参数。Baum-Welch算法是HMM模型中最难的部分，由于本文是“入门”文章，这里就不展开推导了。
 
注一：其实Baum-Welch算法和EM算法是一样的，只是Baum-Welch算法提出得比较早，当时EM算法还没有被归纳出来。
 
## 4 解码预测问题
 
### 4.1 贪心算法
 
贪心算法使用了条件独立性假设，认为不同时刻出现的状态是互相孤立的，是没有相关性的。所以，可以分别求取序列每个时刻最有可能出现的状态，再将各个时刻的状态按顺序组合成状态序列，作为预测结果。
 
贪心算法使用的公式，是2.3.1节推导出的
 
![](https://pic4.zhimg.com/80/v2-1b6fecda5d731a85fb3c16461fff17fb_1440w.jpg)

贪心算法的优点是计算简单。缺点是不同时刻的状态之间，实际上是概率相关的，这种算法可能导致陷入局部最优点，无法取到全局整体序列的最优值。
 
实际上，业界极少用贪心算法来解码预测HMM模型的状态序列。

### 4.2 维特比算法
 
维特比Viterbi算法使用了动态规划DP的思路，在这一节，我们不列举公式，而是通过一个例子，来呈现维特比算法是怎么工作的。
 
依旧使用前文的天气和活动的例子，模型参数和2.1节一样：
 
状态转移概率矩阵A：
 
![](https://pic4.zhimg.com/80/v2-281005e58f29765b253d82632957b173_1440w.jpg)
 
观测概率矩阵B：
 
![](https://pic4.zhimg.com/80/v2-736397689e8650c7839f234994680c1f_1440w.jpg)
 
初始概率状态向量π：
 
![](https://pic1.zhimg.com/80/v2-469efe7a1869cead41eb91aadd016528_1440w.jpg)
 
现在已知小明在3天里的活动序列为：[宅，打球，宅]，求最有可能的天气序列情况。
 
维特比算法预测过程：
 
步骤1：根据模型参数π和B，求得第1天天气的概率分布。
- P(D1，晴天，宅)=0.2*0.5=0.1
- P(D1，阴天，宅)=0.4*0.4=0.16
- P(D1，雨天，宅)=0.4*0.7=0.28
 
此时我们保存3个序列：
- 序列最后时刻为晴天的最大概率序列：[晴天：0.1]
- 序列最后时刻为阴天的最大概率序列：[阴天：0.16]
- 序列最后时刻为雨天的最大概率序列：[雨天：0.28]

步骤2：根据步骤1的3个序列，模型参数A和B，求得第2天天气的概率分布。
 
P(D2，晴天，打球)
= max(P(D1，晴天) * P(D2，晴天|D1，晴天) * P(打球|D2，晴天),P(D1，阴天) * P(D2，晴天|D1，阴天) * P(打球|D2，晴天),P(D1，雨天) * P(D2，晴天|D1，雨天) * P(打球|D2，晴天))
= max(0.1 * 0.5 * 0.5, 0.16 * 0.3 * 0.5, 0.28 * 0.2 * 0.5) （前一时刻为雨天时，序列概率最大）= 0.028

序列最后时刻为晴天的最大概率序列：[雨天，晴天：0.028]
 
P(D2，阴天，打球) = max(0.1 * 0.2 * 0.6, 0.16 * 0.5 * 0.6, 0.28 * 0.3 * 0.6) （前一时刻为雨天时，序列概率最大）= 0.0504
 
序列最后时刻为阴天的最大概率序列：[雨天，阴天：0.0504]
 
P(D2，雨天，打球) = max(0.1 * 0.3 * 0.3, 0.16 * 0.2 * 0.3, 0.28 * 0.5 * 0.3) （前一时刻为雨天时，序列概率最大）= 0.042
 
序列最后时刻为雨天的最大概率序列：[雨天，雨天：0.042]
 
步骤3：根据步骤2的3个序列，模型参数A和B，求得第3天天气的概率分布。
 
P(D3，晴天，宅) = max(0.028 * 0.5 * 0.5, 0.0504 * 0.3 * 0.5, 0.042 * 0.2 * 0.5) （前一时刻为阴天时，序列概率最大）= 0.00756
 
序列最后时刻为晴天的最大概率序列：[雨天，阴天，晴天：0.00756]
 
P(D3，阴天，宅) = max(0.028 * 0.2 * 0.4, 0.0504 * 0.5 * 0.4, 0.042 * 0.3 * 0.4) （前一时刻为阴天时，序列概率最大）= 0.01008
 
序列最后时刻为阴天的最大概率序列：[雨天，阴天，阴天：0.01008]
 
P(D3，雨天，宅) = max(0.028 * 0.3 * 0.7, 0.0504 * 0.2 * 0.7, 0.042 * 0.5 * 0.7) （前一时刻为雨天时，序列概率最大）= 0.0147
 
序列最后时刻为雨天的最大概率序列：[雨天，雨天，雨天：0.0147]
 
步骤4：对比步骤3的3个序列，选出其中概率最大的那个状态序列。
 
[雨天，阴天，晴天：0.00756]，[雨天，阴天，阴天：0.01008]，[雨天，雨天，雨天：0.0147]中，概率最大的序列为[雨天，雨天，雨天]，概率值为0.0147。
 
所以最优可能的天气序列情况为：[雨天，雨天，雨天]
 
最后，简单地描述下维特比算法的思路：
- （1）若状态值集合有N个取值，则需维护N个状态序列，以及N个状态序列对应的概率。每个状态序列存储的是：序列最后一个时刻取值为特定状态（共N个状态）时，概率最大的状态序列。本节案例中，就维护晴天、阴天、雨天三个状态序列，及其概率。
- （2）自第1个时刻开始，根据状态子序列和模型参数，计算和更新N个状态序列及其概率值。本节的案例中，对于当前时刻的晴天、阴天、雨天3个状态值，分别拼接上一时刻的状态序列和当前时刻的状态。例如D3晴天分别拼接D2状态序列得到：[雨天，晴天]+[晴天]，[雨天，阴天]+[晴天]，[雨天，雨天]+[晴天]3个新序列。通过计算求得其中概率最大的序列为[雨天，阴天，晴天]，最后更新晴天状态序列为[雨天，阴天，晴天]，丢弃另外两个序列，并保存概率值0.00756。D3状态为阴天和雨天的状态序列更新流程与此相同。
- （3）一直迭代到最后一个时刻，对比所有状态序列的概率值，概率值最大的状态序列即为最大状态概率序列。
 
注：4.2小节的例子数据，引用自李航《统计学习方法》。
 
## 5 HMM的局限性
 
- 马尔科夫性（有限历史性）：实际上在NLP领域的文本数据，很多词语都是长依赖的。
- 齐次性：序列不同位置的状态转移矩阵可能会有所变化，即位置信息会影响预测结果。
- 观测独立性：观测值和观测值（字与字）之间是有相关性的。
- 单向图：只与前序状态有关，和后续状态无关。在NLP任务中，上下文的信息都是必须的。
- 标记偏置LabelBias：若状态A能够向N种状态转移，状态B能够向M种状态转移。若N<<M，则预测序列更有可能选择状态A，因为A的转移概率较大。
 
## 6 工程实现
 
在具体实现过程中，为避免多个很小的数相乘导致浮点数下溢，所以可以取对数，使相乘变为相加。
 
由于HMM的EM算法中存在求和操作（Viterbi算法只有乘积操作），所以不能简单地使用取对数来避免浮点数下溢，采用的方式是设置一个比例系数，将概率值乘以它来放大；当每次迭代结束后，再把比例系数取消掉，继续下一次迭代。
 
## 7 jieba分词源码解读
 
分词任务是NLP的几个基础任务之一，jieba分词是一个应用得很广泛的开源项目。jieba分词使用了好几个不同的分词算法，我们在这一节对其中的HMM分词代码进行解读。
 
### 7.1 HMM分词基本概念
 
利用HMM算法进行分词，实际上就是执行一个序列标注任务。
 
我们看到的词语是观测序列，我们要预测一个状态序列，最后通过状态序列，来执行分词操作。
 
状态序列包括B、M、E、S四种状态。单个字构成的词，被标注为S。多个字构成的词，词语的第一个字被标注为B，最后一个字被标注为E，中间的若干个子被标注为M。
 
### 7.2 模型参数
 
jieba分词通过有监督的方式，获取模型参数A，B，π。
 
状态转移概率矩阵A被保存在文件prob_trans中：
 
![](https://pic2.zhimg.com/80/v2-2d80780a0beb10ad64e6e4f92243f129_1440w.jpg)

观测概率矩阵B被保存在文件prob_emit中：

![](https://pic1.zhimg.com/80/v2-a2d99195e2c598ddbd4c7152b9610398_1440w.jpg)

初始状态概率向量π被保存在文件prob_start中：

![](https://pic1.zhimg.com/80/v2-b7b50545aa8cdd971eb40c0ae7ca6970_1440w.jpg)
 
 
### 7.3 维特比算法
 
如截图所示，维特比解码流程同4.2节的描述完全一致，关键的部分我已经添加了注释。
 
唯一需要注意的是，HMM的工程实践中，大部分概率值都被取了对数，所以乘法操作在代码里体现为加法操作。
 
![](https://pic2.zhimg.com/80/v2-b1a9d2efe4b6e7aa0fde0a3dd4448ccd_1440w.jpg)

 
### 7.4 分词
 
根据维特比算法输出的标注状态序列，输出分词结果。
 
![](https://pic4.zhimg.com/80/v2-56df9f4f742ed59b32e296f61d805b7f_1440w.jpg)


# CRF

[NLP硬核入门-条件随机场CRF](https://zhuanlan.zhihu.com/p/87638630)

【2020-8-14】**条件随机场**（CRF）与**隐马尔可夫模型**（HMM）的区别是显而易见的。虽然这两种方法都用于对**顺序数据**建模，但它们是不同的算法。
- 隐马尔可夫模型具有**生成性**，通过对**联合概率**分布建模给出了输出。
- 条件随机场具有**判别性**，并对**条件概率**分布进行了建模。CRFs不依赖于独立假设(即标签彼此独立)，并且避免了标签偏差。
- 隐马尔可夫模型是条件随机场的特例，转移概率使用了常数。
- HMMs基于朴素贝叶斯，从逻辑回归得到，CRFs就是从逻辑回归得到的。

## 1. CRF概述
 
### 1.1 随机场的定义
 
在这一小节，我们将会由泛化到特例，依次介绍随机场、马尔科夫随机场、条件随机场、线性链条件随机场的概念。
 
（1）随机场是一个图模型，是由若干个结点（随机变量）和边（依赖关系）组成的图模型，当给每一个结点按照某种分布随机赋予一个值之后，其全体就叫做随机场。
（2）马尔科夫随机场是随机场的特例，它假设随机场中任意一个结点的赋值，仅仅和它的邻结点的取值有关，和不相邻的结点的取值无关。用学术语言表示是：满足成对、局部或全局马尔科夫性。
（3）条件随机场CRF是马尔科夫随机场的特例，它假设模型中只有X（输入变量，观测值）和Y（输出变量，状态值）两种变量。输出变量Y构成马尔可夫随机场，输入变量X不具有马尔科夫性。
（4）线性链条件随机场，是状态序列是线性链的条件随机场。

_注1：马尔科夫性：随机过程中某事件的发生只取决于它的上一事件，是“无记忆”过程。_
 
我们的应用领域是NLP，所以本文只针对线性链条件随机场进行讨论。
 
线性链条件随机场有以下性质：
（1）对于状态序列y，y的值只与相邻的y有关系，体现马尔科夫性。
（2）任意位置的y与所有位置的x都有关系。
（3）我们研究的线性链条件随机场，假设状态序列Y和观测序列X有相同的结构，但是实际上后文公式的推导，对于状态序列Y和观测序列X结构不同的条件随机场也适用。
（4）观测序列X是作为一个整体，去影响状态序列Y的值，而不是只影响相同或邻近位置（时刻）的Y。
（5）线性链条件随机场的示意图如下：
 
![](https://pic1.zhimg.com/80/v2-495c02385b7ec049e9a0aea2b2769c44_1440w.jpg)

注二：李航老师的《统计学习方法》里，使用了两种示意图来描述线性链条件随机场，一种是上文所呈现的，这张图更能够体现性质（4），另一种如下图，关注点是X和Y同结构：

![](https://pic4.zhimg.com/80/v2-e38b08dc8f82f1c02f5b4333aa91b3d7_1440w.jpg)
 
### 1.2 CRF的应用
 
线性链条件随机场CRF是在给定一组随机变量X（观测值）的条件下，获取另一组随机变量Y（状态值）的条件概率分布模型。在NLP领域，线性链条件随机场被广泛用于标注任务（NER、分词等）。

### 1.3 构建CRF的思路（重要）
 
我们先给出构建CRF模型的核心思路，现在暂不需要读懂这些思路的本质思想，但是我们要带着这些思路去阅读后续的内容。
（1）CRF是判别模型，是黑箱模型，不关心概率分布情况，只关心输出结果。
（2）CRF最重要的工作，是提取特征，构建特征函数。
（3）CRF使用特征函数给不同的标注网络打分，根据分数选出可能性最高的标注网络。
（4）CRF模型的计算过程，使用的是以e为底的指数。这个建模思路和深度学习输出层的softmax是一致的。先计算各个可能情况的分数，再进行softmax归一化操作。
 
## 2.CRF模型的概率计算
 
_（对数学公式推导没兴趣的童鞋，只需要看2.1和2.2）_
 
2.1标记符号和参数
 
先约定一下CRF的标记符号：
 
观测值序列：
 
![](https://pic4.zhimg.com/80/v2-2fb5e320486472fb9be5c5661459f1e3_1440w.jpg)
 
状态值序列：
 
![](https://pic4.zhimg.com/80/v2-64942caa26d69040bc80091c236982f7_1440w.jpg)
 
转移（共现）特征函数及其权重：
 
![](https://pic2.zhimg.com/80/v2-dce23c054dd2bce439fa1672f612f451_1440w.jpg)
 
状态（发射）特征函数及其权重：
 
![](https://pic4.zhimg.com/80/v2-df7c869b7d58e7bd6cd0184200692743_1440w.jpg)
 
简化后的特征函数及其权重：
 
![](https://pic4.zhimg.com/80/v2-b5e4d07fd011e66b6a796b8cfa727d8b_1440w.jpg)
 
特征函数t的下标：k1
 
特征函数s的下标：k2
 
简化后的特征函数f的下标：k

## 2.2 一个栗子
 
在进行公式推导前，我们先通过一个直观的例子，初步了解下CRF。
 
例：输入观测序列为X=(x1,x2,x3)，输出状态序列为Y=(y1,y2,y3)，状态值集合为{1,2}。在已知观测序列后，得到的特征函数如下。求状态序列为Y=(y1,y2,y3)=(1,2,2)的非规范化条件概率。
 
![](https://pic2.zhimg.com/80/v2-2fd6c11d0e485b4583683d58e679ef3d_1440w.jpg)

解：参照状态序列取值和特征函数定义，可得特征函数t1，t5，s1，s2，s4取值为1，其余特征函数取值为0。乘上权重后，可得状态序列(1,2,2)的非规范化条件概率为：1+0.2+1+0.5+0.5=3.2

## 2.3 特征函数
 
在这一小节，我们描述下特征函数，以及它的简化形式和矩阵形式。
 
（1）线性链条件随机场的原始参数化形式
 
分数：
 
![](https://pic3.zhimg.com/80/v2-3468fb9322711a30eb6be51ee64f28be_1440w.jpg)
 
归一化概率：
 
![](https://pic1.zhimg.com/80/v2-3b7a714c2624d6290d943286b8775348_1440w.jpg)
 
其中，归一项为：
 
![](https://pic3.zhimg.com/80/v2-335762d5b82504eed2d52feb48d1f11e_1440w.jpg)
 
_t_为定义在边上的特征函数，通常取值0或1，依赖于两个相邻结点的状态，_λ_为其权重。_t_有时被称为转移特征，其实称为共现特征更合适些。因为图模型更强调位置关系而不是时序关系。
 
_s_为定义在节点上的特征函数，通常取值0或1，依赖于单个结点的状态，_μ_为其权重。_s_有时被称为状态特征。
 
需要强调的是：CRF模型中涉及的条件概率，不是真实的概率，而是通过分值softmax归一化成的概率。
 
（2）线性链条件随机场的简化形式
 
特征函数：
 
![](https://pic2.zhimg.com/80/v2-3d46c4333770265e6ea0098a1ab80d49_1440w.jpg)
 
权重：
 
![](https://pic4.zhimg.com/80/v2-d12cbfde1fc055e210283cc8a937ff8f_1440w.jpg)
 
对特征函数在各个位置求和，将局部特征函数转化为全局特征函数：
 
![](https://pic2.zhimg.com/80/v2-cd2c33f5f9a17916177c4254cf66e62d_1440w.jpg)
 
归一化概率：
 
![](https://pic4.zhimg.com/80/v2-a6424857211a7d5483bc9efeaa5b3297_1440w.jpg)
 
向量化：
 
![](https://pic1.zhimg.com/80/v2-5d6d7237a21579bccb6c060f121df554_1440w.jpg)

（3）线性链条件随机场的矩阵形式
 
构建矩阵_Mi(x)_。位置_i_和观测值序列_x_是矩阵的自变量。
 
矩阵的维度是m*m，m为状态值y的集合的元素个数，矩阵的行表示的是位置i-1的状态，矩阵的列表示的是位置i的状态，矩阵各个位置的值表示位置i-1状态和位置i状态的共现分数，并以e为底取指数。
 
![](https://pic1.zhimg.com/80/v2-1eaf53d526e5ca4e2e7066e36f5f1c84_1440w.jpg)
 
## 2.4 前向后向算法
 
（1）前向算法模型
 
（a）_αi(yi=s|x)_表示状态序列y在位置i取值s，在位置1~i取值为任意值的可能性分数的非规范化概率。
 
定义：
 
![](https://pic4.zhimg.com/80/v2-749fca17cd93567c80df41627cee5407_1440w.jpg)
 
（b）递归公式：
 
![](https://pic3.zhimg.com/80/v2-355f9970f633bf9022bd317b99a124ee_1440w.jpg)
 
（c）人为定义：
 
![](https://pic1.zhimg.com/80/v2-2485d6314346d35e4cd5784b7acfbaec_1440w.jpg)
 
（d）归一项：
 
![](https://pic2.zhimg.com/80/v2-9639f05f69a7d3851825aecdaa183525_1440w.jpg)
 
（2）后向算法模型
 
（a）_βi(yi=s|x)_表示状态序列y在位置i取值s，在位置i+1~n取值为任意值的可能性分数的非规范化概率。
 
定义：
 
![](https://pic4.zhimg.com/80/v2-88840e160a09dea22e50ada12eb9f607_1440w.jpg)
 
（b）递归公式：
 
![](https://pic2.zhimg.com/80/v2-2c9e7869dcf569f0bbfbcabf8842d035_1440w.jpg)
 
（c）人为定义：
 
![](https://pic4.zhimg.com/80/v2-501f31bf135c5d5a0f81ed748a2739eb_1440w.jpg)
 
（d）归一化项：
 
![](https://pic1.zhimg.com/80/v2-0e8d940063021a78eb6d05c257100aa0_1440w.jpg)
 
注：在前向算法和后向算法中，人为地定义了α(0)和β(n+1)，采用的是李航老师书里的定义方法。但是，我认为采用先验概率（类似HMM中的初始概率分布）或者全部定义成1更合适。因为这里的概率模型应该表现得更通用一点，而不要引入实际预测序列的第一项和最后一项的信息。

## 2.5 一些概率和期望的计算
 
（1）两个常用的概率公式
 
状态序列y，位置i的取值为特定值，其余位置为任意值的可能性分数的归一化条件概率：
![](https://pic1.zhimg.com/80/v2-a295122d8b112bf4278ce6a2f49d35cc_1440w.jpg)

状态序列y，位置i-1，i的取值为特定值，其余位置为任意值的可能性分数的归一化条件概率：

![](https://pic1.zhimg.com/80/v2-33c5d9f52410f087b53b64d229f83b14_1440w.jpg)
 
（1）两个常用的期望公式
 
特征函数_f_关于条件分布P(Y|X)的数学期望：
 
![](https://pic4.zhimg.com/80/v2-0f9128ce0c60672094c3ef0952b2744b_1440w.jpg)
 
特征函数_f_关于联合分布P(X,Y)的数学期望：
 
![](https://pic3.zhimg.com/80/v2-366880fe34a917820bfa1a210aea8212_1440w.jpg)

 
## 3. CRF模型的训练和预测

 
### 3.1 学习训练问题
 
CRF模型采用正则化的极大似然估计最大化概率。
 
采用的最优化算法可以是：迭代尺度法IIS，梯度下降法，拟牛顿法。
 
相应的知识可以通过最优化方法的资料进行学习，本文篇幅有限，就不作展开了。

### 3.2 预测解码问题
 
和HMM完全一样，采用维特比算法进行预测解码，这里不作展开。
 
## 4.CRF的优缺点（重要）
 
### 4.1 CRF相对于HMM的优点
 
（1）规避了马尔科夫性（有限历史性），能够获取长文本的远距离依赖的信息。
（2）规避了齐次性，模型能够获取序列的位置信息，并且序列的位置信息会影响预测出的状态序列。
（3）规避了观测独立性，观测值之间的相关性信息能够被提取。
（4）不是单向图，而是无向图，能够充分提取上下文信息作为特征。
（5）改善了标记偏置LabelBias问题，因为CRF相对于HMM能够更多地获取序列的全局概率信息。
（6）CRF的思路是利用多个特征，对状态序列进行预测。HMM的表现形式使他无法使用多个复杂特征。
 
### 4.2 条件随机场CRF的缺点
 
（1）CRF训练代价大、复杂度高。
（2）每个特征的权重固定，特征函数只有0和1两个取值。
（3）模型过于复杂，在海量数据的情况下，业界多用神经网络。
（4）需要人为构造特征函数，特征工程对CRF模型的影响很大。
（5）转移特征函数的自变量只涉及两个相邻位置，而CRF定义中的马尔科夫性，应该涉及三个相邻位置。

### 4.3 标记偏置LabelBias
 
在HMM中的体现：对于某一时刻的任一状态，当它向后一时进行状态刻转移时，会对转移到的所有状态的概率做归一化，这是一种局部的归一化。即使某个转移概率特别高，其转移概率也不超过1。即使某个转移概率特别低，如果其它几个转移概率同样低，那么归一化后的转移概率也不会接近0。
 
在CRF被规避的原因：CRF使用了全局的归一化。在进行归一化之前，使用分数来标记状态路径的可能性大小。待所有路径所有位置的分数都计算完成后，再进行归一化。某些某个状态转移的子路径有很高的分数，会对整条路径的概率产生很大的影响。

## 5. 基于TensorFlow的BiLSTM-CRF

 
BiLSTM-CRF是当前用得比较广泛的序列标注模型。
 
BiLSTM-CRF模型由BiLSTM和CRF两个部分组成。
 
BiLSTM使用的是分类任务的配置，最终输出一个标注好的序列。也就是说，即使没有CRF，BiLSTM也能独立完成标注任务。
 
CRF接收BiLSTM输出的标注序列，进行计算，最后输出修正后的标注序列。
 
TensorFlow提供了CRF的开发包，路径为：tf.contrib.crf。需要强调的是，TensorFlow的CRF，提供的是一个严重简化后的CRF，和原始CRF差异较大。虽然减小了模型复杂度，但是在准确率上也一定会有所损失。
 
下面简要介绍下TensorFlow中CRF模块的几个关键函数。
 
（1）crf\_log\_likelihood

![](https://pic1.zhimg.com/80/v2-254649cc74ca993545d0fe4fbacb80a0_1440w.jpg)

BiLSTM模块输出的序列，通过参数inputs输入CRF模块。
 
CRF模块通过crf\_sequence\_score计算状态序列可能性分数，通过crf\_log\_norm计算归一化项。
 
最后返回log_likelihood对数似然。
 
（2）crf\_sequence\_score
 
![](https://pic3.zhimg.com/80/v2-d331b9beea3f5f481ec05043b64989e2_1440w.jpg)
 
crf\_sequence\_score通过crf\_unary\_score计算状态特征分数，通过crf\_binary\_score计算共现特征分数。
 
（3）crf\_unary\_score

![](https://pic3.zhimg.com/80/v2-457e5ec6efa03a53015011cf48f4e84a_1440w.jpg)

crf\_unary\_score利用掩码的方式，计算得出一个类似交叉熵的值。
 
（4）crf\_binary\_score

 
![](https://pic4.zhimg.com/80/v2-b3d33acc4009a1a945128abadb06a66f_1440w.jpg)
 
crf\_binary\_score构造了一个共现矩阵transition_params，表示不同状态共现的概率，这个矩阵是可训练的。最后通过共现矩阵返回共现特征分数。
 
（5）crf\_log\_norm
 
![](https://pic1.zhimg.com/80/v2-5206d73e57ae7432bc72fd022551a834_1440w.jpg)
 
归一化项。


# [深入讲解贝叶斯背后的哲学与数学思想](https://www.toutiao.com/i6646199526229017102/?tt_from=mobile_qq&utm_campaign=client_share&timestamp=1595428970&app=news_article&utm_source=mobile_qq&utm_medium=toutiao_android&use_new_style=1&req_id=20200722224250010027057086161C5AB3&group_id=6646199526229017102)

 
- 2019-01-14 12:13:57
 
数理统计中有**频率学派**和**贝叶斯学派**之分。关于两者的差异，众说纷纭，网上博客、知乎有专门的讨论。
 
> 然而，从更高的哲学上看待这个问题，就会发觉，贝叶斯论和频率论的真正区别在于**人们如何解释概率之间的哲学差异**。

本文将透彻分析贝叶斯背后的哲学与数学思想。让大家从一个更高的视角来把应用贝叶斯思想及推理，不光是应用于机器学习算法，还能指导工作生活。
 
![深入讲解贝叶斯背后的哲学与数学思想](http://p3-tt.byteimg.com/large/pgc-image/cf9c27bd305741ae847ede8e7e95a563)
 
## 贝叶斯定理的证据思想
 
![深入讲解贝叶斯背后的哲学与数学思想](http://p3-tt.byteimg.com/large/pgc-image/36a3553db51c419e8825afda37836ce0)
 
将A视为关于世界的一些命题，将B视为一些数据或证据。例如，A代表今天下雨的命题，B代表外面人行道潮湿的证据，那么分析一下这个贝叶斯推理过程的思想。
 
p（雨\|湿） 问道，"外面潮湿，下雨的几率是多少？" 为了评估这个问题，让我们来看看方程式的右侧。在看地面之前，下雨的概率是多少， p（下雨）？将此视为对世界的假设的合理性。然后我们问在这个假设下，外面潮湿的观察有多少可能性， 即p（潮湿\|下雨）？根据证据，这个过程有效地更新了我们对一个命题的初步信念，在一些观察的支持下最终衡量了降雨的合理性。
 
> 我们的初始信念由先验分布p（下雨）表示，我们的最终信念由后验分布p（雨\|湿）表示。分母只是问："证据的总合理性是多少？"，我们必须考虑所有假设，以确保后验是一个合适的概率分布。
 
这种思维方式可以帮助你摆脱对世界的黑白解释，而不是通过概率镜头来观察事物和解释。
 
从一个基于证据的世界观开始，如果引入新证据，你的初始世界观的概率会发生变化。
 
## 贝叶斯哲学本质：动态的看待世界
 
贝叶斯定理本质：
 
> 贝叶斯定理是一种基于最佳可用证据（观察，数据，信息）计算信念（假设，主张，命题）的有效性的方法。最本真的描述：最初的信念加上新的证据=新的和改进的信念。
 
所以你对自己信仰的确定性并不是固定的，而是流动的、可塑的。您应该能够根据新证据修改您的意见。
 
辩证法强调不要静止的看问题，要动态的看问题。所以为突出强调动态看问题的哲学思想，进一步的描述为：
 
我们用客观信息修改我们的观点：初始信念+最近的客观数据=新的和改进的信念。每次重新计算系统时，后验都成为新迭代的先验。这是一个不断发展的系统，每一点新信息都越来越接近于确定性。
 
这种思维方式可以帮助人们减少确认偏差的影响，从而开启对新可能性的看法。
 
贝叶斯推理过程，是一个不断修正的趋近于真理的过程。
 
贝叶斯定理的另一个用法是判断一个假设发生在另一个假设上的可能性。
 
中心前提是第一原则，即这个世界上大多数事物都是不确定的。很多时候你没有完美的信息，你不知道一切，你需要做出推论。
 
贝叶斯定理，在一个充满不确定性的世界中，为我们的决策提供信息。随着新信息的出现，需要反思这些新证据如何改变对事物的看法，然后根据它进行修正。
 
## 贝叶斯哲学精神：科学的客观性和精确性
 
伯茨麦格雷恩有一个对贝叶斯的经典陈述：
 
> 贝叶斯坚信，现代科学需要客观性和精确性。贝叶斯是信仰的衡量标准。它说我们甚至可以从缺失和不充分的数据，近似和无知中学到东西。
 
随着人们开始认识到人类思考和决策方式的固有不完善性，贝叶斯思想的应用正在不断增长。
 
很长一段时间，经典的经济学模型将人视为理性行为者，在开明的自我利益的基础上做出决策是完美的。现在我们开始意识到这种观点是有缺陷的，相反，人类行为经济学作为认知偏见的牺牲品的观点正变得越来越普遍。
 
贝叶斯思维也是我们学习方法的一个很好的近似。纳特•西尔弗在《信号与噪音》中说：
 
" 相反，它（贝叶斯定理）是一种在数学和哲学上表达我们如何了解宇宙的声明：我们通过近似来了解它，在我们收集更多证据时越来越接近真相。"
 
![深入讲解贝叶斯背后的哲学与数学思想](http://p3-tt.byteimg.com/large/pgc-image/28f881b2ea8a46e6bbc4ea29dcaafcdd)
 
## 贝叶斯数学思想：用数据调整先验
 
贝叶斯推理是非常强大的工具，可用于对任何随机变量进行建模，例如回归参数的值、人口统计数据、业务KPI或单词的词性。对于在机器学习建模中当数据有限、担心过拟合等情况下更有非常有用。
 
> 接下来通过高斯分布估计来讲解贝叶斯在应用于参数估计中的数学思想与方法。
 
在分布参数前提下的数据的概率：条件概率分布
 
假设我们给出了高斯随机变量X的样本数据集，D = {x1，...，xN}，并且给出数据的方差是σ2
 
我们对μ的最佳猜测是什么？这里假设数据是独立的并且分布相同。
 
![深入讲解贝叶斯背后的哲学与数学思想](http://p3-tt.byteimg.com/large/pgc-image/877d431ec6dc449da4d9330dca78ca45?from=pc)
 
把高斯分布写成似然函数的形式如下，就是在当前参数下数据发生的概率密度函数:
 
![深入讲解贝叶斯背后的哲学与数学思想](http://p3-tt.byteimg.com/large/pgc-image/c9d5f1a7743c48e4af1276d993478846?from=pc)
 
我们希望选择最大化此表达式的μ。
 
### 贝叶斯概率
 
对于上边高斯分布参数估计，我们用贝叶斯定理的思想解决，我们的目的是求得参数，换作概率的表达就是，求在参数d的概率条件下的θ的概率，即p(θ\|d)：
 
![深入讲解贝叶斯背后的哲学与数学思想](http://p3-tt.byteimg.com/large/pgc-image/e285a88d27d44ed28ec33a4af20a2ab3)
 
p(d|θ)是似然函数，概率的形式，实质上就是上文中写成条件概率形式的概率密度函数。p(θ)
 
是先验概率（先前的信念）。
 
![深入讲解贝叶斯背后的哲学与数学思想](http://p1-tt.byteimg.com/large/pgc-image/a3aff297edea4c02995c23a2f890e14f)
 
归一化常数，也就是证据的总合理性量度，必须考虑所有假设。p(θ|d)是后验分布，在面对数据时重新调整我们先前的信念（先验概率）。
 
> 这样，我们就把一个求取参数的过程转化为贝叶斯定理的求解过程。
 
### 最大后验概率估计MAP
 
在高斯分布估计中，假设我们事先认为某个随机变量X的平均值是μ0，我们的信念的方差是σ02，然后我们给出X的样本数据集，d = {x1，...，xN}，如下图所示，并且以某种方式知道数据的方差是σ2，本文只给出求取一个参数的情况。
 
现在求后验分布参数μ？
 
![深入讲解贝叶斯背后的哲学与数学思想](http://p1-tt.byteimg.com/large/pgc-image/30c2ec2be2c54961a7c168cd2291c511?from=pc)
 
上面的假设，已知知道高斯分布两个参数如下，即先验。
 
![深入讲解贝叶斯背后的哲学与数学思想](http://p1-tt.byteimg.com/large/pgc-image/66ee3636792944b1ab991823cfee9624?from=pc)
 
根据贝叶斯概率，我们所求即为：
 
![深入讲解贝叶斯背后的哲学与数学思想](http://p3-tt.byteimg.com/large/pgc-image/296c80b35db64a6e82b89ef269e82c30?from=pc)
 
p(d|u)是似然函数，如下图所示：
 
![深入讲解贝叶斯背后的哲学与数学思想](http://p6-tt.byteimg.com/large/pgc-image/00392efe032a44139237fbe0b23990f5?from=pc)
 
![深入讲解贝叶斯背后的哲学与数学思想](http://p6-tt.byteimg.com/large/pgc-image/1013e77550fc4039a493d8ad82389955?from=pc)
 
p(u)是μ的先验概率：
 
![深入讲解贝叶斯背后的哲学与数学思想](http://p3-tt.byteimg.com/large/pgc-image/0782d6b47d424ada910dbc2ad7c02094?from=pc)
 
后验概率可写为：
 
![深入讲解贝叶斯背后的哲学与数学思想](http://p1-tt.byteimg.com/large/pgc-image/f826d50cd9ca4e119d542fdb003b3cc7?from=pc)
 
根据两个高斯分布的乘积也是高斯分布，后验概率也是高斯：
 
![深入讲解贝叶斯背后的哲学与数学思想](http://p6-tt.byteimg.com/large/pgc-image/f4fe91a851554cd89c7739d62ce2501a?from=pc)
 
![深入讲解贝叶斯背后的哲学与数学思想](http://p1-tt.byteimg.com/large/pgc-image/5220941b697f444f9a353d903f08e46b?from=pc)
 
通过变换形式，最后得到：
 
![深入讲解贝叶斯背后的哲学与数学思想](http://p3-tt.byteimg.com/large/pgc-image/c4c2709365a24ad79f90dfa8faaa92af?from=pc)
 
![深入讲解贝叶斯背后的哲学与数学思想](http://p3-tt.byteimg.com/large/pgc-image/e6d504cc172144b0b789b3d3be8ce2f9?from=pc)


# 资料


## 说人话的统计学合辑

- 优质文章：[说人话的统计学合辑](http://www.360doc.com/content/17/1104/08/41417155_700749710.shtml)
 
###  第1章  高屋建瓴看统计
*   [你真的懂p值吗？](http://mp.weixin.qq.com/s?__biz=MzAxMDA4NjU3OA==&mid=207134405&idx=1&sn=8a4e661a0cd0fad97d869845f2e4b1a2&scene=21#wechat_redirect)
*   [做统计，多少数据才算够？（上）](http://mp.weixin.qq.com/s?__biz=MzAxMDA4NjU3OA==&mid=207643438&idx=1&sn=20fbf90250185008f841fffe28bf4e9b&scene=21#wechat_redirect)
*   [做统计，多少数据才算够？（下）](http://mp.weixin.qq.com/s?__biz=MzAxMDA4NjU3OA==&mid=207981601&idx=1&sn=ec4235c0df795e858ed99020381473c0&scene=21#wechat_redirect)
*   [提升统计功效，让评审心服口服！](http://mp.weixin.qq.com/s?__biz=MzAxMDA4NjU3OA==&mid=208048284&idx=1&sn=ea3e00da596826b6c0b267bca46e4306&scene=21#wechat_redirect)
*   [你的科研成果都是真的吗？](http://mp.weixin.qq.com/s?__biz=MzAxMDA4NjU3OA==&mid=208129350&idx=1&sn=734fa50cf19fec17afb7103c11fd6439&scene=21#wechat_redirect)
*   [见识数据分析的「独孤九剑」](http://mp.weixin.qq.com/s?__biz=MzAxMDA4NjU3OA==&mid=208295028&idx=1&sn=d22dea627fff86bf0daded79959bd019&scene=21#wechat_redirect)
*   [贝叶斯vs频率派：武功到底哪家强？](http://mp.weixin.qq.com/s?__biz=MzAxMDA4NjU3OA==&mid=208453473&idx=1&sn=8d16e540580c3aced266a6c9041f996c&scene=21#wechat_redirect)
### 第2章  算术平均数与正态分布
*   [数据到手了，第一件事先干啥？](http://mp.weixin.qq.com/s?__biz=MzAxMDA4NjU3OA==&mid=400430409&idx=1&sn=03b30d4122d177650543f50649195ebd&scene=21#wechat_redirect)
*   [算术平均数：简单背后有乾坤](http://mp.weixin.qq.com/s?__biz=MzAxMDA4NjU3OA==&mid=400735492&idx=1&sn=dc2b5dae73740cd2841dabf2c420f842&scene=21#wechat_redirect)
*   [正态分布到底是怎么来的？](http://mp.weixin.qq.com/s?__biz=MzAxMDA4NjU3OA==&mid=401781634&idx=1&sn=4cbabdb7191b8d49df95f0988943e18b&scene=21#wechat_redirect)
 
### 第3章  t检验：两组平均数的比较
*   [想玩转t检验？你得从这一篇看起](http://mp.weixin.qq.com/s?__biz=MzAxMDA4NjU3OA==&mid=402713181&idx=1&sn=eafb0bd061c6d22fba9582ba230a942c&scene=21#wechat_redirect)
*   [就是要实用！t 检验的七十二变](http://mp.weixin.qq.com/s?__biz=MzAxMDA4NjU3OA==&mid=403019527&idx=1&sn=9d279713517f96a204d4541e3ff68023&scene=21#wechat_redirect)
*   [不是正态分布，t 检验还能用吗？](http://mp.weixin.qq.com/s?__biz=MzAxMDA4NjU3OA==&mid=403375449&idx=1&sn=2fb2c79f8b272686d3908c38ad03b6b1&scene=21#wechat_redirect)
*   [只有15个标本，也能指望 t 检验吗？](http://mp.weixin.qq.com/s?__biz=MzAxMDA4NjU3OA==&mid=403973660&idx=1&sn=e6c513cfde7b47f1c195d401d142f0f2&scene=21#wechat_redirect)
*   [样本分布不正态？数据变换来救场！](http://mp.weixin.qq.com/s?__biz=MzAxMDA4NjU3OA==&mid=2652548058&idx=1&sn=35f73ef5a627b20c1fd29e3eb3ed8b33&scene=21#wechat_redirect)
*   [数据变换的万能钥匙：Box-Cox变换](http://mp.weixin.qq.com/s?__biz=MzAxMDA4NjU3OA==&mid=2652548109&idx=1&sn=0fdd23615447ee8ec27900dbb33a0026&scene=21#wechat_redirect)
*   [t 检验用不了？别慌，还有神奇的非参数检验](http://mp.weixin.qq.com/s?__biz=MzAxMDA4NjU3OA==&mid=2652548283&idx=1&sn=bc958997ddb65c2e11d1b78d2f1b06aa&scene=21#wechat_redirect)
*   [只讲 p 值，不讲效应大小，都是耍流氓！](http://mp.weixin.qq.com/s?__biz=MzAxMDA4NjU3OA==&mid=2652548670&idx=1&sn=93eb1ce6a6b97c21247108db2a868361&scene=21#wechat_redirect)
*   [找出 t 检验的效应大小，对耍流氓 say no！](http://mp.weixin.qq.com/s?__biz=MzAxMDA4NjU3OA==&mid=2652548856&idx=1&sn=f4d2d21a3bce3f6e34a7d7a99315c004&scene=21#wechat_redirect)
*   [用置信区间，就是这么（不）自信！](http://mp.weixin.qq.com/s?__biz=MzAxMDA4NjU3OA==&mid=2652549146&idx=1&sn=94f80df33a0ff425c9884971645a33be&scene=21#wechat_redirect)
*   [如何确定 t 检验的置信区间](http://mp.weixin.qq.com/s?__biz=MzAxMDA4NjU3OA==&mid=2652549198&idx=2&sn=b27598a5f93c9d4957c1287be799b374&scene=21#wechat_redirect)
*   [优雅秀出你的 t 检验，提升Paper逼格！](http://mp.weixin.qq.com/s?__biz=MzAxMDA4NjU3OA==&mid=2652549367&idx=1&sn=6a32c3a96bbf885ebd81c7dd4c52783e&scene=21#wechat_redirect)
*   [要做 t 检验，这两口毒奶可喝不得！](http://mp.weixin.qq.com/s?__biz=MzAxMDA4NjU3OA==&mid=2652549476&idx=1&sn=d571ebf331f5ad08413f9e9a57c73b3c&scene=21#wechat_redirect)
 
###  第4章  方差分析(ANOVA)：多组平均数的比较
*   [要比较三组数据，t 检验还能用吗？](http://mp.weixin.qq.com/s?__biz=MzAxMDA4NjU3OA==&mid=2652549639&idx=1&sn=877daad6e64e689dfb72b8ab7b95bb18&scene=21#wechat_redirect)
*   [ANOVA在手，多组比较不犯愁](http://mp.weixin.qq.com/s?__biz=MzAxMDA4NjU3OA==&mid=2652549791&idx=1&sn=e7079f101ccc4ca5a2f9899d163d2e60&scene=21#wechat_redirect)
*   [ANOVA的基本招式你掌握了吗？](http://mp.weixin.qq.com/s?__biz=MzAxMDA4NjU3OA==&mid=2652549926&idx=1&sn=7dc7d10bd57a8833ebe67a2e33f7f0dd&chksm=80bba2fbb7cc2bedc1d37f5b35e2b581479c327bf0edb1a5b39392027c7ee977c8644c8eca7d&scene=21#wechat_redirect)
*   [ANOVA做出了显著性？事儿还没完呢！](http://mp.weixin.qq.com/s?__biz=MzAxMDA4NjU3OA==&mid=2652550143&idx=1&sn=c3ee5aafdf9404bba3abeb3386ef9f83&chksm=80bba222b7cc2b3436e6e9b5509055d1387ad21eb89c24b26d571452fdfb56cfbeb0e93c011e&scene=21#wechat_redirect)
*   [听说，成对t检验还有ANOVA进阶版？](http://mp.weixin.qq.com/s?__biz=MzAxMDA4NjU3OA==&mid=2652550306&idx=1&sn=394f7597b8e80f5a20877923094b7663&chksm=80bba57fb7cc2c6979d94982d4e4b10a24c66434ba6ece4deb5a22fe4047720d21f6c0a8cdbc&scene=21#wechat_redirect)
*   [重复测量ANOVA：你要知道的事儿都在这里啦](http://mp.weixin.qq.com/s?__biz=MzAxMDA4NjU3OA==&mid=2652550550&idx=1&sn=f86f766ec2b5b883232317fabfb8055b&chksm=80bba44bb7cc2d5dc6b6e222f24c4d63f65cb4570771b30b9c4aee8cae6a9abb5ecd39e39008&scene=21#wechat_redirect)
*   [没听说过多因素 ANOVA ？那你就可就 OUT 了！](http://mp.weixin.qq.com/s?__biz=MzAxMDA4NjU3OA==&mid=2652550743&idx=1&sn=189408e5db94d3242b599596dd4130cc&chksm=80bba78ab7cc2e9cae8f64f2a69e617efb807899656c1c1156de645ae7a36cb5e2d643b355e4&scene=21#wechat_redirect)
*   [多因素ANOVA＝好几个单因素ANOVA？可没这么简单！](http://mp.weixin.qq.com/s?__biz=MzAxMDA4NjU3OA==&mid=2652550964&idx=1&sn=1cda6ec54d40aa21c992e7ea61e661ff&chksm=80bba6e9b7cc2fff85ed20d3d7ca637deb2db4c7f8c663f61b6bfeac84aaae0d843395ca3473&scene=21#wechat_redirect)
*   [两个因素相互影响，ANOVA结果该如何判读？](http://mp.weixin.qq.com/s?__biz=MzAxMDA4NjU3OA==&mid=2652551172&idx=1&sn=4dd852c9460b84e19ccd127ebf34c9ec&chksm=80bba9d9b7cc20cf9726b22576744dc6685065377942977b61a6145fe0952600f8f3c71d8b68&scene=21#wechat_redirect)
*   [ANOVA还能搞三四五因素？等等，我头有点儿晕](http://mp.weixin.qq.com/s?__biz=MzAxMDA4NjU3OA==&mid=2652551457&idx=1&sn=be0f338c2e815b770a59f5448416b072&chksm=80bba8fcb7cc21ea085e4c83164b8cd6025f46bf74ab950c861d9fcf3c3b994f04d404b5e2d3&scene=21#wechat_redirect)
*   [要做ANOVA，样本量多大才够用](http://mp.weixin.qq.com/s?__biz=MzAxMDA4NjU3OA==&mid=2652551560&idx=1&sn=3d30bf3068bc9fe0f26692d50c038a5a&chksm=80bba855b7cc2143ec7575a46dff0dda8e773e040bc1b8f37fc2c4c47fd19e6f25acc9aa873a&scene=21#wechat_redirect)
 
###  第5章  线性回归：统计建模初步

*   [车模航模你玩过，统计学模型你会玩吗？](http://mp.weixin.qq.com/s?__biz=MzAxMDA4NjU3OA==&mid=2652551691&idx=1&sn=ae8993277c68a1f660c0fbeb81f1b7ef&chksm=80bbabd6b7cc22c0b63ef0ef1e541a59003d10241cba7687ea26eb2b8640371782e514ed927e&scene=21#wechat_redirect)
*   [如果只能学习一种统计方法，我选择线性回归](http://mp.weixin.qq.com/s?__biz=MzAxMDA4NjU3OA==&mid=2652551811&idx=1&sn=d441953a14d4a09be4c62f982924f3bb&chksm=80bbab5eb7cc224832626ff58860f72a3fd93e7407d5c04285d2c9574306183667a779f22a1c&scene=21#wechat_redirect)
*   [回归线三千，我只取这一条](http://mp.weixin.qq.com/s?__biz=MzAxMDA4NjU3OA==&mid=2652552010&idx=1&sn=cdaf7103bb6bdb81d3e65ce5a5d65610&chksm=80bbaa97b7cc2381a7d17dd2b878df07809507377f60093dde7f90533391840d6083c0274b6f&scene=21#wechat_redirect)
*   [三千回归线里选中了你，你靠谱吗？](http://mp.weixin.qq.com/s?__biz=MzAxMDA4NjU3OA==&mid=2652552220&idx=1&sn=717e5c741d6e9ce30c255975bb94cfd8&chksm=80bbadc1b7cc24d75a4d57539d0da20e1c23963b49622d8fa4cfc27d3b595cebc4812a7b03a2&scene=21#wechat_redirect)
*   [自变量不止一个，线性回归该怎么做？](http://mp.weixin.qq.com/s?__biz=MzAxMDA4NjU3OA==&mid=2652552402&idx=1&sn=e2096d4e2763e019d7d735efa9e010f7&chksm=80bbad0fb7cc2419afc931720e3abc48ce608a0c11e978163983f95542e54377a5f4ff1ba467&scene=21#wechat_redirect)
*   [找出「交互效应」，让线性模型更万能](http://mp.weixin.qq.com/s?__biz=MzAxMDA4NjU3OA==&mid=2652552648&idx=1&sn=aa0dfdf3adac2e5ff4a7c7ffad0bfaee&chksm=80bbac15b7cc2503a308fd8827a83b55ec94db24a67265e7907f08a2fccf2f09e33ec88018a7&scene=21#wechat_redirect)
*   [天啦噜！没考虑到混杂因素，后果会这么严重？](http://mp.weixin.qq.com/s?__biz=MzAxMDA4NjU3OA==&mid=2652552738&idx=1&sn=da201510bafb95b2f4156b04694a0cc3&chksm=80bbafffb7cc26e95bd08befdc70f2f5e245a7de84df55bffdb23a2d6b2c49789a6531549aa4&scene=21#wechat_redirect)
*   [回归系数不显著？也许是打开方式不对！](http://mp.weixin.qq.com/s?__biz=MzAxMDA4NjU3OA==&mid=2652552927&idx=1&sn=243075854a7d9c428a9c81ff196d005c&chksm=80bbaf02b7cc2614607329bcfa335e9eeb01b708a074a35270474fde64ed83c1002b761083e6&scene=21#wechat_redirect)
*   [评价线性模型，R平方是个好裁判吗？](http://mp.weixin.qq.com/s?__biz=MzAxMDA4NjU3OA==&mid=2652553119&idx=1&sn=d7ed15b0516269b74457afb05fe92ae1&chksm=80bbae42b7cc275469485a52d8b1a7daf94f09aa319dd138fc2732eadf72be9eb3e0a478f044&scene=21#wechat_redirect)
*   [如果R平方是砒霜，本文教你三种解药！](http://mp.weixin.qq.com/s?__biz=MzAxMDA4NjU3OA==&mid=2652553213&idx=1&sn=e3b41220fd001f33964168cc9b0aebe4&chksm=80bbae20b7cc2736c231d9409ff2f8b33397d9df773356ef38ed1d2e07b96fb8bd63e2d9b607&scene=21#wechat_redirect)
*   [线性模型生病了，你懂得怎样诊断吗？](https://mp.weixin.qq.com/s?__biz=MzAxMDA4NjU3OA==&mid=2652553278&idx=1&sn=911872deb951e3cb2df16f5a422a1517&chksm=80bbd1e3b7cc58f580c4e4aa9c66054ad7baa841f8222fc3ab99ba24d1798d300936de1c24a9&scene=21#wechat_redirect)
*   [「脱离群众」的数据点，是「春风化雨」还是「秋风扫落叶」](https://mp.weixin.qq.com/s?__biz=MzAxMDA4NjU3OA==&mid=2652553348&idx=1&sn=e48000f41aad9e8b011cd142a6d90adb&chksm=80bbd159b7cc584f50c1379ce244c48a57028dfa4e8e2c99045c5c47da8efe2e0e0492c4c7fb&scene=21#wechat_redirect)

###  第6章  广义线性模型：统计建模进阶
    
*   [你在 或者不在 需要逻辑回归来算](https://mp.weixin.qq.com/s?__biz=MzAxMDA4NjU3OA==&mid=2652553458&idx=1&sn=cd3eafdf82243346642fe57234d64d73&chksm=80bbd12fb7cc58394bea5b4ca24dead48d9def51a9f0ad20aa89458d14be944c83ceb51ab72e&scene=21#wechat_redirect)
*   [逻辑回归的袅娜曲线，你是否会过目难忘？](https://mp.weixin.qq.com/s?__biz=MzAxMDA4NjU3OA==&mid=2652553605&idx=1&sn=048729536703ad7ec08032b0a7d15ff4&scene=21#wechat_redirect)
*   [逻辑回归的统计检验，原来招数辣么多？](http://mp.weixin.qq.com/s?__biz=MzAxMDA4NjU3OA==&mid=2652554118&idx=2&sn=e14d82806e74fb37f3acfdb6c6d13aee&chksm=80bbd25bb7cc5b4d692c18ba181060b7e59f2a80b71683478da797a1314add1949a48486daad&scene=21#wechat_redirect)
*   [线性回归能玩多变量，逻辑回归当然也能! ](http://mp.weixin.qq.com/s?__biz=MzAxMDA4NjU3OA==&mid=2652554118&idx=1&sn=422b68cd453032109bea73a37496793b&chksm=80bbd25bb7cc5b4d7e6cd9c6aad28e180f721fa65c1dd1b21858e4c46c0c854d4f22d9442f2b&scene=21#wechat_redirect)
*   [喂，你的逻辑回归模型该做个体检啦](http://mp.weixin.qq.com/s?__biz=MzAxMDA4NjU3OA==&mid=2652554302&idx=1&sn=085df8a05c5f51847ae94151e85e2d25&chksm=80bbd5e3b7cc5cf55fcfa83d3584869d3c32c633885af53f59d029e2fe636c15b225d3455f93&scene=21#wechat_redirect)！
*   [逻辑回归能摆平二分类因变量，那……不止二分类呢？](http://mp.weixin.qq.com/s?__biz=MzAxMDA4NjU3OA==&mid=2652554469&idx=1&sn=6f06c3485f31bbacf66616848bbf4295&chksm=80bbd538b7cc5c2e7f118601387af4d7606f1b92df0979e338abb339d20a0743c799d8846d9f&scene=21#wechat_redirect)
*   [让人眼花缭乱的多项逻辑回归，原来是这么用的](http://mp.weixin.qq.com/s?__biz=MzAxMDA4NjU3OA==&mid=2652554542&idx=1&sn=c28757c48aecb04b099fcedc229cc7a9&chksm=80bbd4f3b7cc5de5cfe95901d8142925f32baccc1624dc10c884ed79fa7dd1c8992dbbbd72ae&scene=21#wechat_redirect)
*   [只问方向，无问远近，定序回归的执念你懂吗？](http://mp.weixin.qq.com/s?__biz=MzAxMDA4NjU3OA==&mid=2652554613&idx=1&sn=e7ad742318c22bb7a251880768f7e4c1&chksm=80bbd4a8b7cc5dbee17bbfe90c850b5bafa28ea89ea2beb010af590dc66c75e88d99367753af&scene=21#wechat_redirect)
*   [包教包会：定序回归实战](http://mp.weixin.qq.com/s?__biz=MzAxMDA4NjU3OA==&mid=2652554707&idx=1&sn=d5103ca376456d79d233c5526182e6e0&chksm=80bbd40eb7cc5d186992f93842d8d94d968da5715d82047a3158b13fe595c7d5ef9998fa5550&scene=21#wechat_redirect)
*   [「数」风流人物，还靠泊松回归](http://mp.weixin.qq.com/s?__biz=MzAxMDA4NjU3OA==&mid=2652554788&idx=1&sn=14d43bcf154646a2f2f1483b73ce104d&chksm=80bbd7f9b7cc5eefe1acea860cb7568be0d12d770b0cc1896996717aa9fc5a773e115c99076a&scene=21#wechat_redirect)
*   [广义线性模型到底是个什么鬼？](http://mp.weixin.qq.com/s?__biz=MzAxMDA4NjU3OA==&mid=2652554925&idx=1&sn=c8ee808dfcca76afb9f39178fabfbf66&chksm=80bbd770b7cc5e6697f9aed47680ec241cbaa0c9028e7327a19c939562b6e3cd55b4b1a34463&scene=21#wechat_redirect)


# 结束


