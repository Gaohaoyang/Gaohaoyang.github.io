---
layout: post
title:  "流形学习&降维-Manifold Learning&Dimention Reduction"
date:   2020-04-29 14:21:00
categories: 深度学习
tags: 深度学习 流形学习 降维 无监督学习 维数灾难
excerpt: 机器学习无监督学习中的降维技术，线性（pca/lda），非线性（t-sne/isomap/mds），及背后的流形学习原理
author: 鹤啸九天
mathjax: true
---

* content
{:toc}

# 总结

- 【2020-9-9】[流形学习t-SNE，LLE，Isomap](https://www.toutiao.com/i6870113106081612292)
- 【2020-9-19】流形学习前沿方向：隐图学习，[Latent graph neural networks: Manifold learning 2.0?](https://towardsdatascience.com/manifold-learning-2-99a25eeb677d)
![](https://wx2.sinaimg.cn/mw690/5396ee05ly1ginkjd7x4ij20d40aetdm.jpg)


# Embedding

- Embedding（嵌入）是拓扑学里面的词，在深度学习领域经常和`Manifold`（流形）搭配使用。
  - 三维空间的球体是一个二维流形嵌入在三维空间（2D manifold embedded in 3D space）。球上的任意一个点只需要用一个二维的经纬度来表达就可以了。
  - 一个二维空间的旋转矩阵是2x2的矩阵，其实只需要一个角度就能表达了，这是一维流形嵌入在2x2的矩阵空间。

作者：刘斯坦
链接：https://www.zhihu.com/question/38002635/answer/1382442522


# 流形学习

- [Neural Networks, Manifolds, and Topology](https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/)
  - ![](https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/img/simple2_1.png)
  - ![](https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/img/spiral.1-2.2-2-2-2-2-2.gif)
  - ![](https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/img/topology_2D-2D_train.gif)
- **Manifold Hypothesis**（`流形假设`）
  - 流形假设：“自然的原始数据是低维的流形嵌入于(embedded in)原始数据所在的高维空间”
  - 深度学习就是把高维原始数据（图像，句子）映射到低维流形，使得高维的原始数据被映射到低维流形之后变得可分，而这个映射就叫嵌入（Embedding）。如Word Embedding把单词组成的句子映射到一个表征向量。但后来把低维流形的表征向量叫做Embedding，其实是一种误用。
  - Embedding就是从原始数据提取出来的Feature，也就是那个通过神经网络映射之后的低维向量。
- 流形学习假设所有处于高维空间的数据点都分布在一个低维的流形上。流形学习的目的就在于寻找一种映射，从高维空间中恢复出低维流形来，从而利用流形的低维坐标表示高位空间的数据点，实现数据降维的目的。常用的算法有`Isomap`, `LLE`（Locally Linear Embedding）, `LE`（Laplacian Eigenmaps），`LLP`（Locality Preserving Projection）等

- 虽然有一些维度缩减的变体是有监督的（例如线性/二次判别分析），**流形学习通常指的是无监督的降维**，其中类别没有提供给算法（虽然可能存在）

![](https://p1-tt.byteimg.com/origin/pgc-image/18fd608d47914f9c90c2227d2dd56a9e)
![](https://p3-tt.byteimg.com/origin/pgc-image/c0574ea8081f49af95604bb350657bd2)


- 降维的目的在于寻找数据的“内在变量”,如图，丢弃掉数据之间你的公共信息（“A”的形状），发掘数据之间的变化信息（缩放尺度及旋转角度）。由于缩放尺度与旋转角度并非是线性分布的，因此更适合采用非线性降维方法。
![](https://img-blog.csdnimg.cn/20190401202159198.jpg)

- [什么是流形？manifold](https://www.bilibili.com/video/BV145411x7vJ)
<iframe src="//player.bilibili.com/player.html?aid=455350252&bvid=BV145411x7vJ&cid=181172271&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" height="600" width="100%"> </iframe>
- [用t-SNE进行数据可视化-GoogleTechTalks出品](https://www.bilibili.com/video/BV1Ax411v7z5)
<iframe src="//player.bilibili.com/player.html?aid=10560557&bvid=BV1Ax411v7z5&cid=17434638&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" height="600" width="100%"> </iframe>


# 降维

## VC维

- [VC维与模型复杂度、样本复杂度](https://blog.csdn.net/JasonDing1354/article/details/42009157)
- 物理意义：将假设集合的数量\|H\|比作假设集合的自由度，那么VC维就是假设集合在做二元分类的有效的自由度，即这个假设空间能够产生多少Dichotomies的能力（VC维说的是，到什么时候，假设集合还能shatter，还能产生最多的Dichotomies）
- 假设空间的容量越大，VC维越大，那么模型就越难学习

### 基本概念

- [VC维来龙去脉](http://www.flickering.cn/machine_learning/2015/04/vc%E7%BB%B4%E7%9A%84%E6%9D%A5%E9%BE%99%E5%8E%BB%E8%84%89/)
- [如何通俗的理解机器学习中的VC维、shatter和break point？](https://www.zhihu.com/question/38607822/answer/149407083)
- 学习VC维要先知道的概念有：`增长函数`（growth function）、`对分`（dichotomy）、`打散`（shattering）和`断点`（break point）
  - 1.增长函数
    - 增长函数表示假设空间H对m个示例所能赋予标记的最大可能结果数。
    - 比如说现在数据集有两个数据点，考虑一种二分类的情况，可以将其分类成A或者B，则可能的值有：AA、AB、BA和BB，所以这里增长函数的值为4.
    - 增长函数值越大则假设空间H的表示能力越强，复杂度也越高，学习任务的适应能力越强。不过尽管H中可以有无穷多的假设h，但是增长函数却不是无穷大的：对于m个示例的数据集，最多只能有2^m个标记结果，而且很多情况下也达不到2^m的情况。
  - 2.对分
    - 对于二分类问题来说，H中的假设对D中m个示例赋予标记的每种可能结果称为对D的一种对分（dichotomy）。对分也是增长函数的一种上限。
  - 3.打散
    - 打散指的是假设空间H能实现数据集D上全部示例的对分，即增长函数=2^m。但是认识到不打散是什么则更加重要
  - 4. 断点
    - 假设空间H的VC维数就是最大的非break point值，也就是break point-1
  - Vapink-Chervonenkis Dimension
    - 引出VC维的定义了：假设空间H的VC维是能被H打散的最大的示例集（数据集）的大小
    - 或：
      - 对于一个假设空间H，如果存在m个数据样本能够被假设空间H中的函数按所有可能的2^h种形式分开 ，则称假设空间H能够把m个数据样本打散（shatter）。假设空间H的VC维就是能打散的最大数据样本数目m。若对任意数目的数据样本都有函数能将它们shatter，则假设空间H的VC维为无穷大

## [理解维度诅咒](https://blog.csdn.net/z13653662052/article/details/87936713)
- 原文[The Curse of Dimensionality in classification](http://www.visiondummy.com/2014/04/curse-dimensionality-affect-classification/)

### 介绍

在本文中，我们将讨论所谓的“维度诅咒”，并解释在设计分类器时它的重要性。在下面的章节中，我将提供这个概念的直观解释，由一个由于维数诅咒而过度拟合的明显例子说明。

考虑一个例子，其中我们有一组图像，每个图像描绘一只猫或一只狗。我们想创建一个能够自动区分狗和猫的分类器。为此，我们首先需要考虑可以用数字表示的每个对象类的描述符，这样数学算法（即分类器）可以使用这些数字来识别对象。例如，我们可以说猫和狗的颜色通常不同。区分这两个类的可能描述符可以由三个数组成; 正在考虑的图像的平均红色，平均绿色和平均蓝色。例如，一个简单的线性分类器可以线性地组合这些特征来决定类标签：

> If 0.5*red + 0.3*green + 0.2*blue > 0.6 : return cat;
> else return dog;

然而，这三种颜色描述数字，称为特征，显然不足以获得完美的分类。因此，我们可以决定添加一些描述图像纹理的特征，例如通过计算X和Y方向的平均边缘或梯度强度。我们现在有5个特征组合在一起，可以通过分类算法来区分猫和狗。

为了获得更准确的分类，我们可以根据颜色或纹理直方图，统计矩等添加更多功能。也许我们可以通过仔细定义几百个这些功能来获得完美的分类？这个问题的答案可能听起来有点违反直觉：不，我们不能！。事实上，在某一点之后，通过添加新功能来增加问题的维度实际上会降低分类器的性能。这由图1说明，并且通常被称为“维度的诅咒”。

![](http://www.visiondummy.com/wp-content/uploads/2014/04/dimensionality_vs_performance.png)

特征维度与分类器性能

图1.随着维度的增加，分类器的性能会提高，直到达到最佳要素数。进一步增加维度而不增加训练样本的数量导致分类器性能的降低。

在接下来的部分中，我们将回顾上述原因是什么，以及如何避免维度的诅咒。

### 维度和过度拟合的诅咒
在早先介绍的猫和狗的例子中，我们假设有无数的猫和狗生活在我们的星球上。然而，由于我们有限的时间和处理能力，我们只能获得10张猫狗照片。然后，分类的最终目标是基于这10个训练实例训练分类器，该分类器能够正确地分类我们不了解的无限数量的狗和猫实例。

现在让我们使用一个简单的线性分类器，并尝试获得一个完美的分类。我们可以从一个特征开始，例如图像中的平均“红色”颜色：

![](http://www.visiondummy.com/wp-content/uploads/2014/04/1Dproblem.png)

一维分类问题，图2.单个功能不会导致我们的训练数据完美区分。

图2显示，如果仅使用单个特征，则无法获得完美的分类结果。因此，我们可能决定添加另一个特征，例如图像中的平均“绿色”颜色：

![](http://www.visiondummy.com/wp-content/uploads/2014/04/2Dproblem.png)

二维分类问题

图3.添加第二个特征仍然不会导致线性可分的分类问题：在此示例中，没有一条线可以将所有猫与所有狗分开。

最后，我们决定添加第三个特征，例如图像中的平均“蓝色”颜色，从而产生三维特征空间：

![](http://www.visiondummy.com/wp-content/uploads/2014/04/3Dproblem.png)

3D分类问题

图4.在我们的示例中，添加第三个特征会导致线性可分的分类问题。存在一种将狗与猫完美分开的平面。

在三维特征空间中，我们现在可以找到一个完美地将狗与猫分开的平面。这意味着可以使用这三个特征的线性组合来获得10幅图像的训练数据的完美分类结果：

![](http://www.visiondummy.com/wp-content/uploads/2014/04/3Dproblem_separated.png)

线性可分的分类问题

图5.我们使用的特征越多，我们成功区分类的可能性就越高。

上面的插图似乎表明，在获得完美的分类结果之前增加特征的数量是训练分类器的最佳方式，而在引言中，如图1所示，我们认为情况并非如此。但是，请注意当我们增加问题的维数时，训练样本的密度如何呈指数下降。

在1D情况下（图2），10个训练实例覆盖了完整的1D特征空间，其宽度为5个单位间隔。因此，在1D情况下，样品密度为10/5 = 2个样品/间隔。然而，在二维情况下（图3），我们仍然有10个训练实例，现在覆盖了一个面积为5×5 = 25个单位正方形的2D特征空间。因此，在2D情况下，样品密度为10/25 = 0.4个样品/间隔。最后，在3D情况下，10个样本必须覆盖5x5x5 = 125个单位立方体的特征空间体积。因此，在3D情况下，样品密度为10/125 = 0.08个样品/间隔。

如果我们继续添加特征，则特征空间的维度会增长，并变得更稀疏和稀疏。由于这种稀疏性，找到可分离的超平面变得更加容易，因为当特征的数量变得无限大时，训练样本位于最佳超平面的错误侧的可能性变得无限小。但是，如果我们将高维分类结果投影回较低维空间，则与此方法相关的严重问题变得明显：

![](http://www.visiondummy.com/wp-content/uploads/2014/04/overfitting.png)

过度拟合

图6.使用太多特征会导致过度拟合。分类器开始学习特定于训练数据的异常，并且在遇到新数据时不能很好地概括。

图6显示了投影到2D特征空间的3D分类结果。尽管数据在3D空间中是线性可分的，但在较低维度的特征空间中却不是这种情况。实际上，添加第三维以获得完美的分类结果，简单地对应于在较低维特征空间中使用复杂的非线性分类器。因此，分类器学习我们的训练数据集的特例和异常。因此，生成的分类器将在真实世界数据上失败，包括通常不遵守这些异常的无限量的看不见的猫和狗。

这个概念被称为过度拟合，是维度诅咒的直接结果。图7显示了仅使用2个特征而不是3个特征训练的线性分类器的结果：

![](http://www.visiondummy.com/wp-content/uploads/2014/04/no_overfitting.png)

线性分类器,图7.尽管训练数据未被完美分类，但该分类器在看不见的数据上比图5中的数据获得更好的结果。

虽然图7中显示的具有决策边界的简单线性分类器似乎比图5中的非线性分类器表现更差，但是这个简单的分类器更好地概括了看不见的数据，因为它没有学习仅在我们的训练数据中的特定异常。巧合。换句话说，通过使用较少的特征，避免了维数的诅咒，使得分类器不会过度拟合训练数据。

下面的解释非常经典

图8以不同的方式说明了上述内容。假设我们想要仅使用一个值为0到1的单个特征来训练分类器。让我们假设这个特征对于每只猫和狗都是唯一的。如果我们希望我们的训练数据覆盖此范围的20％，那么所需的训练数据量将占整个猫狗数量的20％。现在，如果我们添加另一个特征，生成2D特征空间，事情会发生变化; 为了覆盖20％的2D特征范围，我们现在需要在每个维度中获得猫和狗总数的45％（0.45 ^ 2 = 0.2）。在3D情况下，这变得更糟：要覆盖20％的3D特征范围，我们需要在每个维度中获得总数的58％（0.58 ^ 3 = 0.2）。

![](http://www.visiondummy.com/wp-content/uploads/2014/04/curseofdimensionality.png)

训练数据量随着维度的数量呈指数增长

图8.覆盖20％特征范围所需的训练数据量随着维度的数量呈指数增长。

换句话说，如果可用的训练数据量是固定的，那么如果我们继续添加维度就会发生过度拟合。另一方面，如果我们不断增加维度，训练数据量需要以指数级增长，以保持相同的覆盖范围并避免过度拟合。

在上面的例子中，我们展示了维度的诅咒引入了训练数据的稀疏性。我们使用的特征越多，数据就越稀疏，因此准确估计分类器的参数（即其决策边界）变得更加困难。维度诅咒的另一个影响是，这种稀疏性不是均匀分布在搜索空间上。实际上，原点周围的数据（在超立方体的中心）比搜索空间的角落中的数据要稀疏得多。这可以理解如下：

想象一个代表2D特征空间的单位正方形。特征空间的平均值是该单位正方形的中心，距离该中心单位距离内的所有点都在一个单位圆内，该单位圆内接单位正方形。不属于该单位圆的训练样本更靠近搜索空间的角落而不是其中心。这些样本难以分类，因为它们的特征值差异很大（例如，单位正方形的相对角上的样本）。因此，如果大多数样本落在内接单位圆内，则分类更容易，如图9所示：

![](http://www.visiondummy.com/wp-content/uploads/2014/04/inscribed_circle.png)

单位距离平均单位圆内的特征

图9.位于单位圆外的训练样本位于特征空间的角落，并且比特征空间中心附近的样本更难分类。

现在一个有趣的问题是，当我们增加特征空间的维数时，圆（超球面）的体积如何相对于正方形（超立方体）的体积发生变化。尺寸d的单位超立方体的体积总是1 ^ d = 1. 尺寸d和半径0.5 的内切超球体的体积可以计算为：

![](http://www.visiondummy.com/wp-content/ql-cache/quicklatex.com-3472e58fe7837e68dc4f98a8516cc5bc_l3.png)

（1） $$ \ begin {equation *} V（d）= \ frac {\ pi ^ {d / 2}} {\ Gamma（\ frac {d} {2} + 1）} 0.5 ^ d。 \ {端方程*} $$

图10显示了当维度增加时，这个超球体的体积如何变化：
![](http://www.visiondummy.com/wp-content/uploads/2014/04/hypersphere.png)
随着维度的增加，超球体的体积趋向于零

图10.随着维数的增加，超球面的体积趋向于零。

则
![](https://img-blog.csdnimg.cn/20190226160056763.png)

这表明，当维数趋于无穷大时，超球体的体积倾向于零，而周围超立方体的体积保持不变。这种令人惊讶且相当反直觉的观察部分地解释了与分类中的维度诅咒相关的问题：在高维空间中，大多数训练数据驻留在定义特征空间的超立方体的角落中。如前所述，特征空间角落中的实例比超球面质心周围的实例更难分类。这由图11示出，其示出了2D单位正方形，3D单位立方体以及具有2 ^ 8 = 256个角的8D超立方体的创造性可视化：

![](http://www.visiondummy.com/wp-content/uploads/2014/04/sparseness.png)

高维特征空间在其原点周围稀疏

图11.随着维度的增加，更大比例的训练数据驻留在要素空间的角落中。

对于8维超立方体，大约98％的数据集中在其256个角上。因此，当特征空间的维数变为无穷大时，从样本点到质心的最小和最大欧几里得距离的差值与最小距离本身的比率趋向于零：

![](http://www.visiondummy.com/wp-content/ql-cache/quicklatex.com-7ffb60f75669300ffbcf8768471ca99d_l3.png)

$$ \ begin {equation *} \ lim_ {d \ to \ infty} \ frac {\ operatorname {dist} _ {\ _max}  -  \ operatorname {dist} _ {\ min}} {\ operatorname {dist} _ {\ min }到\ 0 \ end {equation *} $$

因此，距离测量开始失去其在高维空间中测量不相似性的有效性。由于分类器依赖于这些距离测量（例如欧几里德距离，马哈拉诺比斯距离，曼哈顿距离），因此在较低维空间中分类通常更容易，其中较少特征用于描述感兴趣对象。类似地，高斯似然性在高维空间中变为平坦且重尾的分布，使得最小和最大似然之间的差异与最小似然本身的比率趋于零。

### 如何避免维数的诅咒？

图1显示，当问题的维数变得太大时，分类器的性能会降低。那么问题是“太大”意味着什么，以及如何避免过度拟合。遗憾的是，没有固定的规则来定义在分类问题中应该使用多少特征。实际上，这取决于可用的训练数据量（特征的数量和样本数量有关），决策边界的复杂性以及所使用的分类器的类型。

如果理论无限数量的训练样本可用，则维度的诅咒不适用，我们可以简单地使用无数个特征来获得完美的分类。训练数据的大小越小，应使用的特征越少。如果N个训练样本足以覆盖单位区间大小的1D特征空间，则需要N ^ 2个样本来覆盖具有相同密度的2D特征空间，并且在3D特征空间中需要N ^ 3个样本。换句话说，所需的训练实例数量随着使用的维度数量呈指数增长。

此外，倾向于非常准确地模拟非线性决策边界的分类器（例如，神经网络，KNN分类器，决策树）不能很好地推广并且易于过度拟合。因此，当使用这些分类器时，维度应该保持相对较低。如果使用易于推广的分类器（例如朴素贝叶斯线性分类器），那么所使用的特征的数量可以更高，因为分类器本身不那么具有表现力（less expressive）。图6显示在高维空间中使用简单分类器模型对应于在较低维空间中使用复杂分类器模型。

因此，当在高维空间中估计相对较少的参数时，以及在较低维空间中估计大量参数时，都会发生过度拟合。例如，考虑[高斯密度函数](http://www.visiondummy.com/2014/03/divide-variance-n-1/)，由其均值和协方差矩阵参数化。假设我们在3D空间中操作，使得协方差矩阵是由3个独特元素组成的3×3对称矩阵（对角线上的3个方差和非对角线上的3个协方差）。与分布的三维均值一起，这意味着我们需要根据训练数据估计9个参数，以获得表示数据可能性的高斯密度。在1D情况下，仅需要估计2个参数（均值和方差），而在2D情况下需要5个参数（2D均值，两个方差和协方差）。我们再次可以看到，要估计的参数数量随着维度的数量而增长。

在[前面的文章](http://www.visiondummy.com/2014/03/divide-variance-n-1/)中，我们表明，如果要估计的参数数量增加（并且如果估计的偏差和训练数据的数量保持不变），参数估计的方差会增加。这意味着，由于方差的增加，如果维数上升，我们的参数估计的质量会降低。分类器方差的增加对应于过度拟合。

另一个有趣的问题是应该使用哪些特征。给定一组N个特征; 我们如何选择M个特征的最佳子集，使得M < N？一种方法是在图1所示的曲线中搜索最优值。由于为所有特征的所有可能组合训练和测试分类器通常是难以处理的，因此存在几种尝试以不同方式找到该最佳值的方法。这些方法称为特征选择算法，并且通常采用启发式（贪婪方法，最佳优先方法等）来定位最佳数量和特征组合。

另一种方法是用一组M个特征替换N个特征的集合，每个特征是原始特征值的组合。试图找到原始特征的最佳线性或非线性组合以减少最终问题的维度的算法称为特征提取方法。一种众所周知的降维技术是[主成分分析](http://www.visiondummy.com/2014/05/feature-extraction-using-pca/)（PCA），它产生原始N特征的不相关的线性组合。PCA试图找到较低维度的线性子空间，以便保持原始数据的最大方差。但是，请注意，数据的最大差异不一定代表最具辨别力的信息。

最后，在分类器训练期间用于检测和避免过度拟合的宝贵技术是交叉验证。交叉验证方法将原始训练数据分成一个或多个训练子集。在分类器训练期间，一个子集用于测试所得分类器的准确度和精度，而其他子集用于参数估计。如果用于训练的子集的分类结果与用于测试的子集的结果大不相同，则过度拟合正在发挥作用。如果只有有限数量的训练数据可用，则可以使用几种类型的交叉验证，例如k折交叉验证和留一交叉验证。

![](https://upload.wikimedia.org/wikipedia/commons/1/1c/K-fold_cross_validation_EN.jpg)

Diagram of k-fold cross-validation with k=4

### 结论

在本文中，我们讨论了特征选择，特征提取和交叉验证的重要性，以避免由于维度的诅咒而过度拟合。通过一个简单的例子，我们回顾了维度诅咒在分类器训练中的重要影响，即过度拟合。





## 资料



# 结束


