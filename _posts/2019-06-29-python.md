---
layout: post
title:  "Python技能"
date:   2019-06-29 19:17:00
categories: 技术工具
tags: Python Numpy Pandas pyecharts virtualenv
author : 鹤啸九天
excerpt: Web开发相关技术知识点
mathjax: true
---

* content
{:toc}

# 总结

- 【2021-1-23】python代码调用过程可视化工具，[ryven](https://ryven.org/index)
  - ![](https://ryven.org/img/examples/ryven1.png)


- aadict包：在dict的基础上封装了一层，使其能够像操作属性一样使用字典。使得后续操作更加简便且易读，还可以预防不必要的异常出现。
- 代码示例

```python
from addict import Dict

configs = Dict()

configs.platform.status = "on"
configs.platform.web.task.name = "测试-1204"
configs.platform.web.periods.start = "2020-10-01"
configs.platform.web.periods.end = "2020-10-31"
# 获取不存在的key时会返回一个空字典，不用担心报KeyError，对返回值做判空处理即可
app_configs = configs.platform.app
assert app_configs == {}
```

## 安装

### 自动编译安装python

- 【2020-7-3】脚本如下：

```shell

#-----自定义区------
# 下载python3
src_file='https://www.python.org/ftp/python/3.8.3/Python-3.8.3.tgz'
file_name="${src_file##*/}"
install_dir=~/bin
#-------------------
 
[ -e ${file_name} ]||{
        wget ${src_file}
        echo "下载完毕..."
}&& echo "文件已存在, $file_name"
# 解压
tar zxvf ${file_name}
echo "安装目录: $install_dir"
# 安装
new_dir=${file_name%.*}
cd $new_dir
./configure --prefix=${install_dir}/python38
# 如果不设置安装目录prefix, 就会提示sudo权限
make && make install
echo "安装完毕，请设置环境变量"
 
# 设置环境变量
#vim ~/.bash_profile
echo "
alias python3='${install_dir}/python38/bin/python3.8'
alias pip3='${install_dir}/python38/bin/pip3'
" >> ~/.bash_profile
 
echo '生效'
source ~/.bash_profile


echo '修改pip源'
mkdir ~/.pip
echo "
[global]
index-url = https://pypi.tuna.tsinghua.edu.cn/simple
[install]
trusted-host=mirrors.aliyun.com
" > ~/.pip/pip.conf

# 或者一行命令设置
#pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple

```


## anaconda

- 下载

```shell
# 官方地址，慢
wget https://repo.anaconda.com/archive/Anaconda3-2019.03-Linux-x86_64.sh
# 清华地址: https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/
wget https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/Anaconda3-5.3.1-Linux-x86_64.sh
bash Anaconda3-2019.03-Linux-x86_64.sh
```

- 安装完成之后会多几个应用
   - Anaconda Navigtor ：用于管理工具包和环境的图形用户界面，后续涉及的众多管理命令也可以在 Navigator 中手工实现。
   - Jupyter notebook ：基于web的交互式计算环境，可以编辑易于人们阅读的文档，用于展示数据分析的过程。
   - qtconsole ：一个可执行 IPython 的仿终端图形界面程序，相比 Python Shell 界面，qtconsole 可以直接显示代码生成的图形，实现多行代码输入执行，以及内置许多有用的功能和函数。
   - spyder ：一个使用Python语言、跨平台的、科学运算集成开发环境。
- 加入环境变量
   - 安装器若提示“`Do you wish the installer to prepend the Anaconda install location to PATH in your /home/<user>/.bash_profile ?`，建议输入“yes”。
   - 如果输入“no”，则需要手动添加路径。添加 export PATH="/<anaconda_path>/bin:$PATH" 在 .bashrc 或者 .bash_profile 中。

- 注意：
   - 不要擅自在bash_profile中添加alias别名！会导致虚拟环境切换后python、pip转换失效

```shell
# anaconda 环境
export PATH="~/anaconda3/bin:$PATH"
# 以下语句不要添加！
alias python='/home/wangqiwen004/anaconda3/bin/python'
alias pip='/home/wangqiwen004/anaconda3/bin/pip'
# 如果仍然失效，强制使用变量切换
sra(){
    CONDA_ROOT="~/anaconda3"
    env=$1
    conda activate $env
    export LD_LIBRARY_PATH="$CONDA_ROOT/envs/$env/lib:$LD_LIBRARY_PATH" 
    export PATH=$CONDA_ROOT/envs/$env/bin:$PATH
}
# 【2020-7-10】以上方法不支持默认环境base的切换，优化如下：
sra(){
    CONDA_ROOT="~/anaconda3"
    # 获取当前虚拟环境名称列表(不含base)
    env_list=(`conda info -e | awk '{if($1!~/#|base/)printf $1" "}'`)
    env=$1
    conda activate $env
    echo "env=$env, str=${env_list[@]}"
    # 判断是否匹配已有环境名称
    # echo "${env_list[@]}" | grep $env && echo "yes" || echo "no"
    #[[ "$1" =~ "${env_str}" ]] && echo "yes" || echo "no"
    #[[ ${env_list[@]/${env}/} != ${env_list[@]} ]] && {
    res="no"
    for i in ${env_list[@]}
    do
         [ "$i" == "$env" ] && res="yes"
    done
    [ $res == "yes" ] && {
        echo "找到目标环境$env"
        export LD_LIBRARY_PATH="$CONDA_ROOT/envs/$env/lib:$LD_LIBRARY_PATH"
        export PATH=$CONDA_ROOT/envs/$env/bin:$PATH
    }||{
        echo "启用默认环境base"
        env="base"
        export LD_LIBRARY_PATH="$CONDA_ROOT/lib:$LD_LIBRARY_PATH"
        export PATH=$CONDA_ROOT/bin:$PATH
    }
    echo "环境切换完毕: --> $env"
}
alias srd='conda deactivate'
# 激活的使用方法
sra learn
```

- 注意：不要这样加！

### 常用命令

- 汇总如下：
   -  [anaconda完全手册](https://www.jianshu.com/p/eaee1fadc1e9)
   - [Anaconda介绍、安装及使用教程](https://zhuanlan.zhihu.com/p/32925500)

```shell
conda --version # 查看版本
activate # 切换到base环境
activate learn # 切换到learn环境
conda create -n learn python=3 # 创建一个名为learn的环境并指定python版本为3(的最新版本，也可以是2.7、3.6等))
conda create -n learn numpy matplotlib python=2.7 # 创建环境同时安装必要的包
conda create -n py36_tf1 python=3.6 tensorflow==1.11 # [2020-7-21]
conda create --prefix="D:\\my_python\\envs\\my_py_env"  python=3.6.3 # 自定义虚拟环境
conda create --name env_name --clone learn # 克隆环境 learn -> env_name
conda env list # 列出conda管理的所有环境, conda info -e 
source activate learn # linux下激活虚拟环境conda activate，windows下为：activate learn
source deactivate # linux下关闭虚拟环境conda deactivate，windows下为：deactivate
conda list # 列出当前环境的所有包
conda list -n learn # 列出某环境下的所有包
conda install requests #安装requests包, 同pip install
conda install -n your_env_name [package] # 即可安装package到your_env_name中
conda remove requests #卸载requets包
conda remove -n learn --all # 删除learn环境及下属所有包
conda remove --name learn  package_name  # 删除learn环境下某个包
conda update requests # 更新requests包
conda search pyqtgraph # 搜索包(模糊查找)
conda search --full-name pyqtgraph # 精确查找
conda env export > environment.yaml # 导出当前环境的包信息
conda env create -f environment.yaml # 用配置文件创建新的虚拟环境

# 添加Anaconda的TUNA镜像
conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/
# TUNA的help中镜像地址加有引号，需要去掉
 
# 设置搜索时显示通道地址
conda config --set show_channel_urls yes

```

### 问题解决

- 使用conda install 安装各种包的时候速度很慢，参考：[conda install速度慢](https://blog.csdn.net/mojiewangday/article/details/105583026)
- 解决
   - 修改conda镜像路径
   - 执行如下命令，更换仓库径路为清华镜像路径

```
conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/
conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/
```
   - 在自己用户目录C:\Users<你的用户名>下生成一个文件，名字为：~/.condarc

```
conda config --set show_channel_urls yes
```
   - 修改.condarc文件为如下:

```
channels:
  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/
show_channel_urls: true
ssl_verify: true
```

   - 执行完上述三步，conda的镜像路径就更换完毕，如不放心可以 conda info 查看 channel URLs 信息已经更改。

- 【2020-8-22】执行conda install flask-restplus时，pip没问题
   - Solving environment: failed with initial frozen solve. Retrying with flexible solve
   - 解决：执行
      - conda config --add channels conda-forge
      - conda config --set channel_priority flexible

## yaml

- YAML是一种直观的能够被电脑识别的的数据序列化格式，容易被人类阅读，并且容易和脚本语言交互。YAML类似于XML，但是语法比XML简单得多，对于转化成数组或可以hash的数据时是很简单有效的。
   1. 大小写敏感
   2. 使用缩进表示层级关系
   3. 缩进时不允许使用Tab，只允许使用空格
   4. 缩进的空格数目不重要，只要相同层级的元素左对齐即可
   5. \# 表示注释，从它开始到行尾都被忽略
- 支持的数据类型
   - 字符串
   - 整型
   - 浮点型
   - 布尔型
   - null
   - 时间
   - 日期
- 主要特性，更多：[Python YAML用法详解](https://blog.csdn.net/lmj19851117/article/details/78843486)
   - & 和 * 用于引用
   - 强制转换，用!!实现
   - 同一个yaml文件中，可以用 --- 来分段
   - 构造器(constructors)、表示器(representers)、解析器(resolvers )



- config.yaml内容：

```yaml
name: Tom Smith
age: 37
spouse:
    name: Jane Smith
    age: 25
children:
 - name: Jimmy Smith
   age: 15
 - name1: Jenny Smith
   age1: 12
---
# 这个例子输出一个字典，其中value包括所有基本类型
str: "Hello World!"
int: 110
float: 3.141
boolean: true  # or false
None: null  # 也可以用 ~ 号来表示 null
time: 2016-09-22t11:43:30.20+08:00  # ISO8601，写法百度
date: 2016-09-22  # 同样ISO8601
name: &name 灰蓝 # 设置被引用字段名
tester: *name # * 取引用内容
a: !!str 3.14 # 转字符串
b: !!int "123" # 转int
```

- 操作方法

```python
import yaml

# 文件
f = open(r'config.yml')
# 字符串
f = '''
---
name: James
age: 20
---
name: Lily
age: 19
'''
y = yaml.load(f) 
y = yaml.load_all(f) # 多个yaml区域
for data in y:
    print(data)
# 转成yaml文档
obj1 = {'name': 'Silenthand Olleander',
            'race': 'Human',
            'traits': ['ONE_HAND', 'ONE_EYE']
}
obj2 = {"name": "James", "age": 20}
print(yaml.dump(obj1，))
# 中文输出
import json
print(json.dumps(obj1, ensure_ascii=False, indent=2))
print(yaml.dump(d,default_flow_style=False, indent=2, allow_unicode=True))
f = open(r'out_config.yml','w')
print(yaml.dump(obj2,f))
yaml.dump_all([obj1, obj2], f) # 一次输出多个片区
```

## [logging](https://docs.python.org/3/library/logging.html)模块

- [python日志基于时间切分和基于文件大小切分](https://www.jianshu.com/p/5a4e226444bd)
- 代码：

```python
#！coding:utf-8
import logging
import logging.handlers
import datetime, time

#logging    初始化工作
logger = logging.getLogger("zjlogger")
logger.setLevel(logging.DEBUG)

# 添加TimedRotatingFileHandler
# (1) 定义一个1秒换一次log文件的handler, 保留3个旧log文件
rf_handler = logging.handlers.TimedRotatingFileHandler(filename="all.log",when='S',interval=1, backupCount=3)
# (2) 写入文件，如果文件超过100个Bytes，仅保留5个文件。
handler = logging.handlers.RotatingFileHandler('logs/myapp.log', maxBytes=100, backupCount=5)

rf_handler.setFormatter(logging.Formatter("%(asctime)s - %(levelname)s - %(filename)s[:%(lineno)d] - %(message)s"))

#在控制台打印日志
handler = logging.StreamHandler()
handler.setLevel(logging.DEBUG)
handler.setFormatter(logging.Formatter("%(asctime)s - %(levelname)s - %(message)s"))

logger.addHandler(rf_handler)
logger.addHandler(handler)

while True:
    logger.debug('debug message')
    logger.info('info message')
    logger.warning('warning message')
    logger.error('error message')
    logger.critical('critical message')
    time.sleep(1)

```


## Python版本转换

- 【2020-8-24】Python代码版本转换

```shell
2to3 -help # 帮助
2to3 -w .　　#将当前整个文件夹代码从python2转到python3
# python2文件会在.py后面再加上一个后缀.bak，而新生成的python3文件使用之前python2文件的命名

```

### virtualenv

- 【2018-1-3】[Python--Virtualenv简明教程](http://python.jobbole.com/85398/)，[virtualenv官方文档](http://virtualenv.readthedocs.org/en/latest/virtualenv.html)
- python沙盒环境，virtualenv创建一个拥有自己安装目录的环境, 这个环境不与其他虚拟环境共享库, 能够方便的管理python版本和管理python库
- （1）安装：

```shell
pip install virtualenv
#或者由于权限问题使用sudo临时提升权限
sudo pip install virtualenv
virtualenv -h #获得帮助
```
- （2）创建：
    - 用virtualenv管理python环境
    - virtualenv ENV  # 创建一个名为ENV的目录, 并且安装了ENV/bin/python
    - 生成ENV目录，包含bin（解释器）、include和lib（python库）文件夹
    - 运行 virtualenv --system-site-packages ENV, 会继承/usr/lib/python2.7/site-packages下的所有库, 最新版本virtualenv把把访问全局site-packages作为默认行为
- （3）激活虚拟环境：

```shell
cd ENV
source ./bin/activate  #激活当前virtualenv
(ENV)➜  ENV git:(master) ✗ #注意终端发生了变化
```

【2020-5-14】注意：windows下没有bin目录，参考，执行方法：.\scripts\activate.bat

- （4）关闭
    - deactivate
- （5）指定python版本
    - 可以使用-p PYTHON_EXE选项在创建虚拟环境的时候指定python版本

```shell
#创建python2.7虚拟环境
➜  Test git:(master) ✗ virtualenv -p /usr/bin/python2.7 ENV2.7
virtualenv -p "C:\Program Files (x86)\Python\Python27" py2
```


## Python依赖包

- 【2020-8-23】python项目依赖包管理
- 安装：
   - pip install -r requirements.txt
- 自动生成requirements.txt
   - pip install pipreqs 
   - pipreqs /path/to/project
- 生成requirements.txt文件

```shell
pip freeze  #显示所有依赖
pip freeze > requirement.txt  #生成requirement.txt文件
pip install -r requirement.txt  #根据requirement.txt生成相同的环境
```

- 对比分析

|工具包|优点|缺点|
|---|---|---|
|pip freeze|包含列表完全|不相关的依赖包也会包含进来|
|pipreqs|只会包含项目 imports 的包|包含列表不是很完全|
|pip-compile|精准控制项目依赖包|需要手动操作，不方便|

## 单元测试

- [2020-11-6] 经验总结, 提交代码前，执行单测，确保代码功能无误
- 主目录下执行命令: [pytest使用笔记（一）](https://www.pianshen.com/article/4986199865/)
   - pytest # 整体报告
   - pytest -q # 运行简单模式测试报告
   - pytest -x # 在第一个测试用例发生错误时就停止运行
   - pytest --maxfail=2 # 在第2个测试用例发生错误时就停止运行
   - pytest test_mod.py # 执行具体文件
   - pytest test # 运行单个文件的用例
   - pytest -k "TestClass" # 运行某些包含关键字的用例，如包含TestClass的用例
   - pytest test_class.py::TestClass # 运行某一文件内特定模块的用例
   - pytest -m slow # 运行用@ pytest.mark.slow装饰器修饰的用例


# python技能


## uuid生成

- [python生成并处理uuid的方法](https://blog.csdn.net/yl416306434/article/details/80569688)

- uuid是128位的全局唯一标识符（univeral unique identifier），通常用32位的一个字符串的形式来表现。有时也称guid(global unique identifier)。python中自带了uuid模块来进行uuid的生成和管理工作。（具体从哪个版本开始有的不清楚。。）
- python中的uuid模块基于信息如MAC地址、时间戳、命名空间、随机数、伪随机数来uuid。具体方法有如下几个：　　
   - uuid.uuid1()　　基于MAC地址，时间戳，随机数来生成唯一的uuid，可以保证全球范围内的唯一性。
   - uuid.uuid2()　　算法与uuid1相同，不同的是把时间戳的前4位置换为POSIX的UID。不过需要注意的是python中没有基于DCE的算法，所以python的uuid模块中没有uuid2这个方法。
   - uuid.uuid3(namespace,name)　　通过计算一个命名空间和名字的md5散列值来给出一个uuid，所以可以保证命名空间中的不同名字具有不同的uuid，但是相同的名字就是相同的uuid了。【感谢评论区大佬指出】namespace并不是一个自己手动指定的字符串或其他量，而是在uuid模块中本身给出的一些值。比如uuid.NAMESPACE_DNS，uuid.NAMESPACE_OID，uuid.NAMESPACE_OID这些值。这些值本身也是UUID对象，根据一定的规则计算得出。
   - uuid.uuid4()　　通过伪随机数得到uuid，是有一定概率重复的
   - uuid.uuid5(namespace,name)　　和uuid3基本相同，只不过采用的散列算法是sha1

一般而言，在对uuid的需求不是很复杂的时候，uuid1方法就已经够用了

```python
import uuid

name = 'test_name'
# namespace = 'test_namespace'
namespace = uuid.NAMESPACE_URL

print uuid.uuid1()
print uuid.uuid3(namespace,name)
print uuid.uuid4()
print uuid.uuid5(namespace,name)
```

## 性能优化

- 【2020-9-10】[Python语言优化](https://www.cnblogs.com/pypypy/p/11995237.html)
- 原生的python通常都是由cpython实现，而cpython的运行效率，确实让人不敢恭维，比较好的解决方案有cython、numba、pypy等等

### cython

![](https://yqfile.alicdn.com/d0a10d313a638bc6a18eb6ef4b1632920a2b133a.png)


### numba

![](https://yqfile.alicdn.com/94af3cf79a6a076262e647cf491247928939ff79.jpeg)

### pypy

![](https://yqfile.alicdn.com/ae64deb6d0ce804914e023290429be7812e62a16.jpeg)


## 多进程

- 【2021-1-21】多年前写的多任务调度框架，每天处理20T上网数据

```python
# ***************************************************************************
# *
# * Copyright (c) 2012 Baidu.com, Inc. All Rights Reserved
# *
# **************************************************************************/
  
#/**
# * @file start.py
# * @author wangqiwen@baidu.com(zhanglun@baidu.com)
# * @date 2013/04/26 16:32
# * detect if data is ready
# * process data use codes defined in conf
# * output data to path in conf
# * multi thread , multi job commit
# * log is added
# **/
 
#import packages:
import ConfigParser
import string
import os
import sys
import threading
import time
import datetime
import logging
import urllib, re
import commands
import json
 
sys.path.append('.')
 
from optparse import OptionParser
 
#define class:
#job spec conf
class JobSpecConf:
    def __init__(self):
        self.job_name = None
        self.input_ready = []
        self.inputs = []
        self.must_input = []
        self.code_path = None
        self.detect_start_time = 0
        self.detect_end_time = 0
        self.delay_before_start = 0
        self.output_ready = None
        self.map_con_num = 0
        self.reduce_num = 0
 
#load jon conf
def loadJobConf(conf_file,date):
    config = ConfigParser.ConfigParser()
    config.read(conf_file)
    global_conf.map_dict['$date'] = date
    global_conf.map_dict['$date_pre'] = getPreDate(date=date)
    # tranform job conf info
    job_conf_list = []
    try:
        for section in config.sections():
            # all job session name starts with 'job' in configre file
            if not section.startswith('job'):
                continue
            # new job conf object
            job_spec_conf = JobSpecConf()
            job_spec_conf.job_name = config.get(section,"job_name")
            # update job_name in map_dict
            global_conf.map_dict['$job_name'] = job_spec_conf.job_name
            # get input path & ready
            item_list = config.options(section)
            path_list = [multiReplace(config.get(section,i),global_conf.map_dict) for i in item_list if i.startswith('input_path_')]
            ready_list = [multiReplace(config.get(section,i),global_conf.map_dict) for i in item_list if i.startswith('input_ready_')]
            job_spec_conf.input_number = len(path_list)
            if len(path_list) != len(ready_list):
                logger.error("configure file(%s) error: input path & ready in section %s doesn't match !" %(conf_file,section))
                return job_conf_list
            job_spec_conf.inputs = path_list
            job_spec_conf.input_ready = ready_list
            if 'must_input' in item_list:
                job_spec_conf.must_input = [multiReplace(config.get(section,'input_ready_%s'%(i)),global_conf.map_dict) for i in config.get(section,"must_input").split('&')]
                logger.info('[%s] must_input = %s'%(job_spec_conf.job_name,repr(job_spec_conf.must_input)))
            # get other info
            job_spec_conf.output = multiReplace(config.get(section,"output"),global_conf.map_dict)
            job_spec_conf.output_ready = multiReplace(config.get(section,"output_ready"),global_conf.map_dict)
            job_spec_conf.code_path = multiReplace(config.get(section,"code_path"),global_conf.map_dict)
            job_spec_conf.detect_start_time = config.get(section,"detect_start_time")
            job_spec_conf.detect_end_time = config.get(section,"detect_end_time")
            job_spec_conf.delay_before_start = config.getint(section,"delay_before_start")
            job_spec_conf.map_con_num = config.get(section,"map_con_num")
            job_spec_conf.reduce_num = config.get(section,"reduce_num")
            job_conf_list.append(job_spec_conf)
        logger.info("job conf info: %s" %(repr(job_conf_list)))
    except Exception,err:
        logger.error("error:(%s) when loading configure file(file:%s,section:%s)" %(err,conf_file,section))
    return job_conf_list
 
def multiReplace(str,dict):
    rx = re.compile('|'.join(map(re.escape,dict)))
    def one_xlat(match):
        return dict[match.group(0)]
    return rx.sub(one_xlat,str)
  
 
#init logger
def initlog(date,job_type):
    logger = logging.getLogger('%s.py' % job_type)
    hdlr = logging.FileHandler('../log/%s/%s_%s.txt' % (now,job_type,date),mode="wb")
    formatter = logging.Formatter('[%(levelname)s] [%(asctime)s]: %(message)s')
    hdlr.setFormatter(formatter)
    logger.addHandler(hdlr)
    logger.setLevel(logging.DEBUG)
    return logger
 
def send_mail(mail_alarm='on',title='mail title',content='mail content',receiver=None):
    """
        usage: send_email(title,content,receiver)
        shell: echo -e "hello" | mail -s "title" receivers
    """
    if mail_alarm != 'on':
        logger.info('alarm mail is off ...')
        return 0
    if receiver == None:
        logger.info('Mail receiver address (%s) invalid !' %(receiver))
        exit(-1)
    code = os.system("echo -e \"%s\" | mail -s \"%s\" \"%s\"" %(content,title,receiver))
    if 0 == code:
        logger.info('Success to send mail (%s:%s) to %s' %(title,content,receiver))
        return 0
    else:  
        logger.error('Fail to send mail (%s:%s) to %s, error code (%s) ,program exit ...' %(title,content,receiver,code))
        exit(-1)
 
def get_jobtracker_running_job_name_ids(url):
    while True:
        try:
            lines = urllib.urlopen(url).readlines()
        except:
            raise
            return False
        if lines:
            break
        else:
            logger.error("%s not reachable" % url)
     
    found = False
    jobs = {}
    for line in lines:
        line = line.strip()
        if found == True:
            if line.startswith('<h2'):
                break
            o = re.search('(job_[0-9]+_[0-9]+).+id="name_[0-9]+">([^<]+)<', line)
            if o:
                name, jid = o.groups()
                jobs[name] = jid
        if line == """<h2 id="running_jobs">Running Jobs</h2>""":
            found = True
    return jobs
 
#define class:
#global conf
class GlobalConf:
    def __init__(self):
        self.retry_interval = 0
        self.output_path = ''
        self.root_code_path = ''
        self.hadoop_client = ''
        self.re_run_number = 0
        self.jobtracker = ''
        self.map_dict = {'$output_path':'-','$date_pre':'-','$date':'-','$job_name':'-'}
        self.mail_receiver_in = None
        self.mail_alarm = 'on'
        self.mail_receiver_out = None
 
#define functions:
#load global config
def globalConfLoad(config_file):
    #load conf
    config = ConfigParser.ConfigParser()
    config.read(config_file)
    #init conf
    global_conf = GlobalConf()
    global_conf.retry_interval = config.getint("global","retry_interval")
    global_conf.output_path = config.get("global","output_path")
    global_conf.map_dict['$output_path'] = global_conf.output_path
    global_conf.root_code_path = config.get("global","root_code_path")
    global_conf.hadoop_client = config.get("global","hadoop_client")
    global_conf.re_run_number = config.getint("global","re_run_number")
    global_conf.jobtracker = config.get("global", "jobtracker")   
    global_conf.mail_receiver_in = config.get("global","mail_receiver_in")
    global_conf.mail_receiver_out = config.get("global","mail_receiver_out")
    global_conf.mail_alarm = config.get("global","mail_alarm")
    return global_conf
 
#test path exist
def checkHadoopFile(path,date,global_conf):
    path_after_parse = multiReplace(path,global_conf.map_dict)
    cmd = '%s fs -test -e %s' % (global_conf.hadoop_client,path_after_parse)
    code, msg = commands.getstatusoutput(cmd)
    if code == 0:
        return True
    elif msg:
        logger.warning('hadoop file (%s) not found , error_id : %s' %(path_after_parse,msg))
        return False
    return False
 
def getPreDate(n = 1 , date = 'today'):
    '''
        get string of date before n days
    '''
    #date_pre = (datetime.datetime.strptime(date,'%Y%m%d')+datetime.timedelta(days=1)).strftime('%Y%m%d')
    if date == 'today':
        date_object = datetime.date.today()
    else:
        date_object = datetime.datetime.strptime(date,'%Y%m%d')
    date_pre = (date_object + datetime.timedelta(days=-n)).strftime('%Y%m%d')
    return date_pre
 
def stopTask(job_spec_conf, date):
    task_prefix = "%s_%s_%s" % (options.pipeline, date, job_spec_conf.job_name)
    found = True
    while found:
        found = False
        running_jobs = get_jobtracker_running_job_name_ids(global_conf.jobtracker)
        for job in running_jobs:
            if job.startswith(task_prefix):
                logger.info("%s: killing job %s " % (job_spec_conf, running_jobs[job]))
                os.system('%s job -kill %s' % (global_conf.hadoop_client, running_jobs[job]))
                found = True
 
def cleanData(job_spec_conf, date, stop = False):
    if stop:
        stopTask(job_spec_conf, date)
    data_path = job_spec_conf.output
    ready_path = job_spec_conf.output_ready
    if 0 == os.system('%s fs -test -e %s' %(global_conf.hadoop_client,ready_path)):
        os.system('%s fs -rmr %s && %s fs -rmr %s' %(global_conf.hadoop_client,ready_path,global_conf.hadoop_client,data_path))
        logger.info("[%s] clean data (%s,%s)" %(job_spec_conf.job_name,data_path,ready_path))
 
def startMultiThead(date, tasks, options):
    logger.info('Begin to run the whole job of day: %s' % date)
    all_job_list = []
    all_job_list = loadJobConf(options.config_file,date)
    all_job_dict = {}
    for job in all_job_list:
        all_job_dict[job.job_name] = job
    logger.info('all jobs in conf : %s' % json.dumps([i.job_name for i in all_job_list]))
    # select some jobs
    if tasks:
        job_conf_list = []
        for jobname in tasks:
            if jobname in all_job_dict:
                job_conf_list.append(all_job_dict[jobname])
            else:
                logger.error('job name (%s) error ! please check with configure file(%s),all job names:%s'%(jobname,options.config_file,repr(all_job_dict.keys())))
                return False
    else:
        job_conf_list = all_job_list
    # check whether each job need to run or not
    job_conf_list = [c for c in job_conf_list if checkTask(c, date, options)]
    logger.info('jobs need to run : %s' % repr([i.job_name for i in job_conf_list]))
    if len(job_conf_list) <= 0:
        logger.info('no job in list , exit ...')
        return False
    # clean old data
    for c in job_conf_list:
        cleanData(c, date, True)
    if options.kill_task:
        return True
    # start multithread
    if options.parallel:
        # run all jobs at the same time
        threads = [threading.Thread(target = tryRunTask, args = (c, date, options)) for c in job_conf_list]
        for t in threads:
            t.start()
        for t in threads:
            t.join()
    else:
        # run one by one
        for c in job_conf_list:
            tryRunTask(c, date, options)
    logger.info('End of day: %s' % date)
    
def runOneTask(job_spec_conf, date, options):
    start_time = datetime.datetime.strptime(getPreDate(n=0)+job_spec_conf.detect_start_time,'%Y%m%d%H:%M')
    end_time = datetime.datetime.strptime(getPreDate(n=0)+job_spec_conf.detect_end_time,'%Y%m%d%H:%M')
    # wait some time until start
    if job_spec_conf.delay_before_start > 0:
        logger.info('[%s] sleep %s mins before start to detect ...'%(job_spec_conf.job_name,job_spec_conf.delay_before_start))
        time.sleep(job_spec_conf.delay_before_start*60)
    # Initialize input path
    input_ready_number = 0
    detect_input_dict = {}
    for i,v in enumerate(job_spec_conf.input_ready):
        detect_input_dict[v] = job_spec_conf.inputs[i]
    input_ready_list = []
    input_path_list = []
    # waiting
    while True:
        now_time = datetime.datetime.now()
        time_info = '[now %s,start %s,end %s]' %(str(now_time),str(start_time),str(end_time))
        #count ready input file
        tmp_key_list = detect_input_dict.keys()
        for tmp_input_ready in tmp_key_list:
            if checkHadoopFile(tmp_input_ready,date,global_conf):
                logger.info('[%s] detect input file (%s): ready ...' % (job_spec_conf.job_name,tmp_input_ready))
                input_ready_list.append(tmp_input_ready)
                input_path_list.append(detect_input_dict[tmp_input_ready])
                del detect_input_dict[tmp_input_ready]
            else:
                logger.warning('[%s] detect input file (%s): not ready... %s' % (job_spec_conf.job_name,tmp_input_ready,time_info))
        input_ready_number = len(input_ready_list)
        logger.info('[%s] detect result : ready num => %d, must input num => %d, all input num => %d...' % (job_spec_conf.job_name,input_ready_number,len(job_spec_conf.must_input),job_spec_conf.input_number))
        # check whether match min input
        match = 0
        if input_ready_number > 0:
            match = 1
        for i in job_spec_conf.must_input:
            if i not in input_ready_list:
                match = 0
                break
        if options.run_force and match :
            logger.warning("[%s] run forcely... input data ready info : %s/%s " %(job_spec_conf.job_name,input_ready_number,job_spec_conf.input_number))
            break
        #too early
        if now_time < start_time:
            logger.info('[%s] time enough , wait until start time ... %s' %(job_spec_conf.job_name,time_info))
            logger.info('[%s] sleep %d minutes (retry_interval)' % (job_spec_conf.job_name,global_conf.retry_interval))
            time.sleep(global_conf.retry_interval*60)
            continue
        #all ready
        if input_ready_number == job_spec_conf.input_number :
            logger.info('[%s] detect_start_time %s , detect_end_time %s , all inputs are ready ...' %(job_spec_conf.job_name,job_spec_conf.detect_start_time,job_spec_conf.detect_end_time))
            break
        # too late
        if options.enable_timeout and now_time >= end_time:
            if match:
                logger.warning('[%s] input ready before time over, ... input data ready info : %s/%s ' %(job_spec_conf.job_name,input_ready_number,job_spec_conf.input_number))
                break
            else:
                logger.warning('[%s] time over , but input files not enough , exit now ... input data ready info : %s/%s ' %(job_spec_conf.job_name,input_ready_number,job_spec_conf.input_number))
                exit(-1)
        logger.info('[%s] sleep %d minutes (retry_interval)' % (job_spec_conf.job_name,global_conf.retry_interval))
        time.sleep(global_conf.retry_interval*60)
             
    # run
    hadoop_job_cmd = 'sh %s%s %s %s %s %s %s %s %s %s %s &>../log/%s/%s_%s.txt' % (
                global_conf.root_code_path,
                job_spec_conf.code_path,
                global_conf.hadoop_client,
                '"%s"' % (';'.join(input_path_list)),
                job_spec_conf.output,
                job_spec_conf.job_name,
                global_conf.root_code_path,
                date,
                "%s_%s_%s" % (options.pipeline, date, job_spec_conf.job_name),
                job_spec_conf.map_con_num,
                job_spec_conf.reduce_num,
                now,
                job_spec_conf.job_name,
                date)
    logger.info('[%s] hadoop job command: %s' % (job_spec_conf.job_name, hadoop_job_cmd))
    code, msg = commands.getstatusoutput(hadoop_job_cmd)
    if code != 0:
        logger.error('[%s]: job Failled when running, error_code = %s(%s)' %(job_spec_conf.job_name,code,msg))
        title = '[merge-wise][ERROR] [%s] Job failed when running !'%(job_spec_conf.job_name)
        content = '[ERROR] [%s] Job failed when running , error_code = %s(%s)'%(job_spec_conf.job_name,code,msg)
        logger.error('mail_receiver_in=%s'%(global_conf.mail_receiver_in))
        send_mail(global_conf.mail_alarm,title,content,global_conf.mail_receiver_in)
        return False
    # result info : all input,ready input,miss input
    result_dict = {'input_list':[],'input_num':0,'ready_list':[],'ready_num':0,'miss_list':[],'miss_num':0}
    for i in job_spec_conf.input_ready:
        if i in input_ready_list:
            result_dict['ready_list'].append(i)
        else:
            result_dict['miss_list'].append(i)
        result_dict['input_list'].append(i)
    result_dict['ready_num'] = len(result_dict['ready_list'])
    result_dict['miss_num'] = len(result_dict['miss_list'])
    result_dict['input_num'] = len(result_dict['input_list'])
    logger.info('[%s] result_dict : %s' %(job_spec_conf.job_name,repr(result_dict)))
     
    # create ready file
    output_ready = job_spec_conf.output_ready
    hadoop_ready_cmd = 'echo %s | %s fs -put - %s' %(json.dumps(result_dict),global_conf.hadoop_client,output_ready)
    logger.info('create ready file : %s' %(hadoop_ready_cmd))
    code, msg = commands.getstatusoutput(hadoop_ready_cmd)
    if code != 0:
        logger.error('[%s] fail to create ready file (%s) , error_code = %s(%s)'%(job_spec_conf.job_name,output_ready,code,msg))
        title = '[merge-wise][ERROR][%s] Failed to create ready file !'%(job_spec_conf.job_name)
        content = '[ERROR] [%s] Failed to create ready file (%s),error_code = %s(%s)'%(job_spec_conf.job_name,output_ready,code,msg)
        send_mail(global_conf.mail_alarm,title,content,global_conf.mail_receiver_in)
        return False
    if result_dict['miss_num'] > 0:
        miss_source_info = ','.join([i.strip().split('/')[-3] for i in result_dict['miss_list']])
        logger.info('[%s] job finished at the end without sources(%s)' %(job_spec_conf.job_name,miss_source_info))
        title = '[merge-wise][WARNING][%s] Job finished at the end without sources(%s)' %(job_spec_conf.job_name,miss_source_info)
        content = '[WARNING][%s] Job finished at the end without sources(%s)\n\nmiss_info = %s\n\nready_info = %s' %(job_spec_conf.job_name,miss_source_info,repr(result_dict['miss_list']),repr(result_dict['ready_list']))
        send_mail(global_conf.mail_alarm,title,content,global_conf.mail_receiver_out)
    if options.signal_zk:
        cmd_out = os.system("%s cr -register  %s -cronID %s" % (global_conf.hadoop_client, job_spec_conf.output, job_spec_conf.job_name))
        if cmd_out != 0:
            title = '[merge-wise][ERROR][%s] Failed to reigster zk data (%s)!'%(job_spec_conf.job_name,job_spec_conf.output)
            content = '[ERROR][%s] Failed to reigster zk data (%s) ! error_code = %s '%(job_spec_conf.job_name, job_spec_conf.output ,str(cmd_out))
            send_mail(global_conf.mail_alarm,title,content,global_conf.mail_receiver_in)
            logger.error('[%s] job Failled, reigster zk data (%s) error, error_code = %s' %(job_spec_conf.job_name, job_spec_conf.output,str(cmd_out)))
            return False
    logger.info('[%s] job finish.' % (job_spec_conf.job_name))
    return True
 
def checkTask(job_spec_conf, date, options):
    # check whether job needs to run or not
    output_ready = job_spec_conf.output_ready
    if options.rerun or options.run_force:
        return True
    #if options.fix_data and checkMissTask(job_spec_conf, date):
    #    return True
    if checkHadoopFile(output_ready, date, global_conf):
        logger.info('check task %s : exist, %s' %(output_ready,job_spec_conf.job_name))
        return False
    else:
        logger.info('check task %s : not exist, %s' %(output_ready,job_spec_conf.job_name))
        return True
 
def tryRunTask(job_spec_conf, date, options):
    logger.info('try to run task %s of %s' % (job_spec_conf.job_name, date))
    for i in range(options.retry):
        if runOneTask(job_spec_conf, date, options):
            return
        if i == (options.retry - 1):
            break
        logger.error('Task %s on %s failed, try again ...' % (job_spec_conf.job_name, date))
        time.sleep(options.retry_sleep)
        cleanData(job_spec_conf, date, False)
    logger.error('Task %s on %s failed ...' % (job_spec_conf.job_name, date))
     
#main define here:
def main():
    global options
    global logger
    global global_conf
 
    usage = "usage: %start.py [options] [tasks]"
    parser = OptionParser(usage=usage)
    parser.add_option("-f", "--disable_timeout",
                      action = "store_false", dest = "enable_timeout", default = True,
                      help = "disable job timeout")
    parser.add_option("-d", "--date", dest = "date", default = getPreDate(), help = "last day for job")
    parser.add_option("-n", "--total_day", type = "int", dest = "total_day", default = 1, help = "total day for job")
    parser.add_option("-s", "--sequence_run", action = "store_false", dest = "parallel", default = True, help = "run task one by one")
    parser.add_option("-x", "--fix_data", action = "store_true", dest = "fix_data", default = False, help = "fix data")
    parser.add_option("-p", "--pipeline", dest = "pipeline", default = "start", help = "pipeline name")
    parser.add_option("-r", "--retry", type = "int", dest = "retry", default = 1, help = "job retry times")
    parser.add_option("-R", "--rerun", action = "store_true", dest = "rerun", default = False, help = "rerun tasks")
    parser.add_option("-w", "--retry_sleep", type = "int", dest = "retry_sleep", default = 300, help = "job retry sleep time, in seconds")
    parser.add_option("-a", "--run_force", action = "store_true", dest = "run_force", default = False, help = "run job no matter the job has run already or data are not all ready")
    parser.add_option("-c", "--config_file", dest = "config_file", default = "conf/job_conf.ini", help = "configure file path")
    parser.add_option("-K", "--kill", action = "store_true", dest = "kill_task", default = False, help = "Kill tasks")
    parser.add_option("-S", "--signal", action = "store_true", dest = "signal_zk", default = False, help = "Signal data reay in zk")
 
    (options, tasks) = parser.parse_args()
    logger = initlog(options.date, options.pipeline)
    global_conf = globalConfLoad(options.config_file)
    logger.info('=================================================')
    logger.info('options=%s tasks=%s' %(repr(options),repr(tasks)))
    #logger.info('options=%s tasks=%s' %(repr(options),repr(tasks)))
    # get previous n days list before date, then start it
    for i in xrange(options.total_day):
        date_i = getPreDate(n=i,date=options.date)
        startMultiThead(date_i, tasks, options)
    logger.info('Finish. Time to quit soon ...')
#main start:
if __name__ == '__main__':
    # call build.sh to update local directory
    os.system('sh build.sh')
    now = getPreDate( n = 0 )
    main()
 
# */* vim: set expandtab ts=4 sw=4 sts=4 tw=400: */
```

- 【2021-1-26】其它调度框架，[Python 实现并发Pipeline](https://zhuanlan.zhihu.com/p/260175181)
    - 对于Pipeline 根据侧重点的不同，有两种实现方式
    1. 用于加速多线程任务的pipeline
        - 用于加速多线程任务的 Pipeline 主要强调 任务的顺序执行， 转移之间不涉及过于复杂的逻辑。所以 每个pipe 通过自身调用 next pipe。整体上强调 后向连续性。
    2. 用于控制流程的pipeline
        - 用于流程控制的piepline， 强调任务的 逻辑性， 由外部 manager 来控制 pipeline 的执行方向。整体上强调前向的依赖性， 使用拓扑排序确定执行顺序。


# Python生态系统


## Numpy

- 【2020-12-28】[NumPy初学教程，可视化指南](https://www.toutiao.com/i6911204024628806148/)
  - ![](https://p3-tt.byteimg.com/origin/pgc-image/21a2426ec0304256950e4f80596b2b4a?from=pc)
  - ![](https://p3-tt.byteimg.com/origin/pgc-image/cc4fbb6975bf4799b413cb5ec9c694f3?from=pc)

- Numpy知识点汇总

![](https://raw.githubusercontent.com/woaielf/woaielf.github.io/master/_posts/Pic/1704/170415-1.png)

![](https://raw.githubusercontent.com/woaielf/woaielf.github.io/master/_posts/Pic/1704/170415-2.png)

![](https://raw.githubusercontent.com/woaielf/woaielf.github.io/master/_posts/Pic/1704/170415-3.png)

Reference
> 《Python for data analysis》 <br>
[NumPy Reference — NumPy v1.12 Manual](https://docs.scipy.org/doc/numpy/reference/?v=20170515184114)

## Pandas

- Pandas 提供了数据结构——DataFrame，可以高效的处理一些数据分析任务。我们日常分析的数据，大多是存储在类似 excel 的数据表中，Pandas 可以灵活的按列或行处理数据，几乎是最常用的工具了。

- [Pandas按行按列遍历Dataframe的几种方式](https://blog.csdn.net/sinat_29675423/article/details/87972498)
    - ![](https://img-blog.csdnimg.cn/20190227142817847.png)
    - 简单对上面三种方法进行说明：
        - iterrows(): 按行遍历，将DataFrame的每一行迭代为(index, Series)对，可以通过row[name]对元素进行访问。
        - itertuples(): 按行遍历，将DataFrame的每一行迭代为元祖，可以通过row[name]对元素进行访问，比iterrows()效率高。
        - iteritems():按列遍历，将DataFrame的每一列迭代为(列名, Series)对，可以通过row[index]对元素进行访问。

![](https://raw.githubusercontent.com/woaielf/woaielf.github.io/master/_posts/Pic/1704/170422-1.png)

![](https://raw.githubusercontent.com/woaielf/woaielf.github.io/master/_posts/Pic/1704/170422-2.png)

![](https://raw.githubusercontent.com/woaielf/woaielf.github.io/master/_posts/Pic/1704/170422-3.png)

![](https://raw.githubusercontent.com/woaielf/woaielf.github.io/master/_posts/Pic/1704/170422-4.png)

![](https://raw.githubusercontent.com/woaielf/woaielf.github.io/master/_posts/Pic/1704/170422-5.png)

![](https://raw.githubusercontent.com/woaielf/woaielf.github.io/master/_posts/Pic/1704/170422-6.png)

![](https://raw.githubusercontent.com/woaielf/woaielf.github.io/master/_posts/Pic/1704/170422-7.png)


### pandas操作命令

- [pandas-数据的合并与拼接](https://www.cnblogs.com/keye/p/10791705.html)
- Pandas包的merge、join、concat方法可以完成数据的合并和拼接
    - **merge**方法主要基于两个dataframe的共同列进行合并
    - **join**方法主要基于两个dataframe的索引进行合并
    - **concat**方法是对series或dataframe进行行拼接或列拼接。

- (1) pandas的merge方法是基于共同列，将两个dataframe连接起来。
- merge方法的主要参数：
    - left/right：左/右位置的dataframe。
    - how：数据合并的方式。
        - left：基于左dataframe列的数据合并；
        - right：基于右dataframe列的数据合并；
        - outer：基于列的数据外合并（取并集）；
        - inner：基于列的数据内合并（取交集）；默认为'inner'。
    - on：用来合并的列名，这个参数需要保证两个dataframe有相同的列名。
    - left_on/right_on：左/右dataframe合并的列名，也可为索引，数组和列表。
    - left_index/right_index：是否以index作为数据合并的列名，True表示是。
    - sort：根据dataframe合并的keys排序，默认是。
    - suffixes：若有相同列且该列没有作为合并的列，可通过suffixes设置该列的后缀名，一般为元组和列表类型。
- 示例：
    - 内连接：
        - ![](https://img2018.cnblogs.com/blog/1235684/201904/1235684-20190429170030512-309759819.png)
    - 外链接
        - ![](https://img2018.cnblogs.com/blog/1235684/201904/1235684-20190429170055862-1927816021.png)
    - index和column的内连接方法
        - ![](https://img2018.cnblogs.com/blog/1235684/201904/1235684-20190429170503579-1916082061.png)



```python
# 单列的内连接
# 定义df1
import pandas as pd
import numpy as np

df1 = pd.DataFrame({'alpha':['A','B','B','C','D','E'],'feature1':[1,1,2,3,3,1],
            'feature2':['low','medium','medium','high','low','high']})
# 定义df2
df2 = pd.DataFrame({'alpha':['A','A','B','F'],'pazham':['apple','orange','pine','pear'],
            'kilo':['high','low','high','medium'],'price':np.array([5,6,5,7])})
# print(df1)
# print(df2)
# 基于共同列alpha的内连接
df3 = pd.merge(df1,df2,how='inner',on='alpha')
# 基于共同列alpha的内连接,若两个dataframe间除了on设置的连接列外并无相同列，则该列的值置为NaN。
df4 = pd.merge(df1,df2,how='outer',on='alpha')
# 基于共同列alpha的左连接
df5 = pd.merge(df1,df2,how='left',on='alpha')
# 基于共同列alpha的右连接
df6 = pd.merge(df1,df2,how='right',on='alpha')
# 基于共同列alpha和beta的内连接
df7 = pd.merge(df1,df2,on=['alpha','beta'],how='inner')
# 基于共同列alpha和beta的右连接
df8 = pd.merge(df1,df2,on=['alpha','beta'],how='right')
# 基于df1的beta列和df2的index连接
df9 = pd.merge(df1,df2,how='inner',left_on='beta',right_index=True)
# 基于df1的alpha列和df2的index内连接(修改相同列的后缀名)
df9 = pd.merge(df1,df2,how='inner',left_on='beta',right_index=True,suffixes=('_df1','_df2'))
df3
```

- (2) join方法
    - join方法是基于index连接dataframe，merge方法是基于column连接，连接方法有内连接，外连接，左连接和右连接，与merge一致。 
    - join和merge的连接方法类似，这里就不展开join方法了，建议用merge方法。
- 示例
    - ![](https://img2018.cnblogs.com/blog/1235684/201904/1235684-20190429170956399-607825585.png)

```python
caller = pd.DataFrame({'key': ['K0', 'K1', 'K2', 'K3', 'K4', 'K5'], 'A': ['A0', 'A1', 'A2', 'A3', 'A4', 'A5']})
other = pd.DataFrame({'key': ['K0', 'K1', 'K2'],'B': ['B0', 'B1', 'B2']})
print(caller)
print(other)# lsuffix和rsuffix设置连接的后缀名
# 基于index连接
caller.join(other,lsuffix='_caller', rsuffix='_other',how='inner')
# 基于key列进行连接
caller.set_index('key').join(other.set_index('key'),how='inner')
```

- (3) concat方法
    - concat方法是拼接函数，有行拼接和列拼接，默认是行拼接，拼接方法默认是外拼接（并集），拼接的对象是pandas数据类型。 
- 示例

```python
df1 = pd.Series([1.1,2.2,3.3],index=['i1','i2','i3'])
df2 = pd.Series([4.4,5.5,6.6],index=['i2','i3','i4'])
print(df1)
print(df2)

# 行拼接
pd.concat([df1,df2])
# 对行拼接分组,行拼接若有相同的索引，为了区分索引，我们在最外层定义了索引的分组情况。
pd.concat([df1,df2],keys=['fea1','fea2'])
# 列拼接,默认是并集
pd.concat([df1,df2],axis=1)
# 列拼接的内连接（交）
pd.concat([df1,df2],axis=1,join='inner')
# 列拼接的内连接（交）
pd.concat([df1,df2],axis=1,join='inner',keys=['fea1','fea2'])
# 指定索引[i1,i2,i3]的列拼接
pd.concat([df1,df2],axis=1,join_axes=[['i1','i2','i3']])

df1 = pd.DataFrame({'key': ['K0', 'K1', 'K2', 'K3', 'K4', 'K5'], 'A': ['A0', 'A1', 'A2', 'A3', 'A4', 'A5']})
df2 = pd.DataFrame({'key': ['K0', 'K1', 'K2'],'B': ['B0', 'B1', 'B2']})
print(df1)
print(df2)

# 行拼接
pd.concat([df1,df2])
# 列拼接
pd.concat([df1,df2],axis=1)
# 判断是否有重复的列名，若有则报错
pd.concat([df1,df2],axis=1,verify_integrity = True)

```



Reference
> 《Python for data analysis》<br>
[pandas: powerful Python data analysis toolkit — pandas 0.20.1 documentation](http://pandas.pydata.org/pandas-docs/stable/?v=20170515184114)

### 遍历

- 【2020-12-25】遍历dataframe

```python
df = pd.DataFrame({'a': range(0, 10000), 'b': range(10000, 20000)})
import time
list1 = []
start = time.time()
for i,r in df.iterrows():
    list1.append((r['a'], r['b']))
print("iterrows耗时  :",time.time()-start)

list1 = []
start = time.time()
for ir in df.itertuples():
    list1.append((ir[1], ir[2]))    
print("itertuples耗时:",time.time()-start)

list1 = []
start = time.time()
for r in zip(df['a'], df['b']):
    list1.append((r[0], r[1]))
print("zip耗时       :",time.time()-start)
```

- 结果：
    - iterrows耗时  : 0.7355637550354004
    - itertuples耗时: 0.008462905883789062
    - zip耗时       : 0.003980875015258789

### 多表合并

- 【2020-12-31】一次合并多张表

```python
import pandas as pd
from functools import reduce
 
data_dir = '/home/work/data/北链讲盘培训价值分析'
#data_file = '/home/work/data/北链讲盘过关用户业绩样例.csv'
bu = ['京北', '京东南']
group = ['未参加', '低分', '高分']
df_dict = {}
for b in bu:
    df_dict[b] = {}
    for g in group:
        data_file = '{}/{}-{}.csv'.format(data_dir, b, g)
        print('开始读数据：{}'.format(data_file))
        df_dict[b][g] = pd.read_csv(data_file, encoding='gbk')
        df_dict[b][g] = df_dict[b][g].rename(columns={'stat_date':'日期', 'avg(assign_amt)':'培训{}的经纪人日均业绩'.format(g)})
        df_final.loc[:,'alpha'].apply(lambda x:'{}-hh'.format(x))
        df_dict[b][g]['日期'] = df_dict[b][g]['日期'].apply(lambda x: '{}-{}-{}'.format(str(x)[:4],str(x)[4:6],str(x)[6:8]))
        # .dt.dayofweek
        df_dict[b][g]['星期几'] = pd.to_datetime(df_dict[b][g]['日期']).dt.dayofweek
    #df_dict[b]['合并'] = df.merge(df_dict[b]['未参加'], df_dict[b]['低分'], df_dict[b]['高分'], how='inner', on='日期')
    merge_list = [df_dict[b]['未参加'], df_dict[b]['低分'], df_dict[b]['高分']]
    df_dict[b]['合并'] = reduce(lambda left,right: pd.merge(left,right,on='日期',how='outer'), merge_list).fillna(0)
    df_dict[b]['合并'].to_excel('{}/{}_合并后.xlsx'.format(data_dir, b))
 
#df = pd.read_csv(data_file, encoding='gbk')
#!head $data_file
#df3 = pd.merge(df1,df2,how='inner',on='alpha')
df_dict['京北']['合并']
```

### 日期转换

- [pandas字符串转日期处理方式](https://blog.csdn.net/weixin_41685388/article/details/103860881)
- 代码

```python

#提取年月日时分秒：方法1
df = pd.read_csv(r"spider.csv",header=None,names=['datetime','url','name','x','y'],encoding='utf-8')
df['datetime'] = pd.to_datetime(df['datetime'],errors='coerce')   #先转化为datetime类型,默认format='%Y-%m-%d %H:%M:%S'
df['date'] = df['datetime'].dt.date   #转化提取年-月-日
df['year'] =df['datetime'].dt.year.fillna(0).astype("int")   #转化提取年 ,
#如果有NaN元素则默认转化float64型，要转换数据类型则需要先填充空值,在做数据类型转换
df['month'] = df['datetime'].dt.month.fillna(0).astype("int")  #转化提取月
df['%Y_%m'] = df['year'].map(str) + '-' + df['month'].map(str) #转化获取年-月
df['day'] = df['datetime'].dt.day.fillna(0).astype("int")      #转化提取天
df['hour'] = df['datetime'].dt.hour.fillna(0).astype("int")    #转化提取小时
df['minute'] = df['datetime'].dt.minute.fillna(0).astype("int") #转化提取分钟
df['second'] = df['datetime'].dt.second.fillna(0).astype("int") #转化提取秒
df['dayofyear'] = df['datetime'].dt.dayofyear.fillna(0).astype("int") #一年中的第n天
df['weekofyear'] = df['datetime'].dt.weekofyear.fillna(0).astype("int") #一年中的第n周
df['weekday'] = df['datetime'].dt.weekday.fillna(0).astype("int") #周几，一周里的第几天，Monday=0, Sunday=6
df['quarter'] = df['datetime'].dt.quarter.fillna(0).astype("int")  #季度
display(df.head())
```


## Matplotlib

- Python 有非常丰富的第三方绘图库，matplotlib 使用起来也许并不是很便捷，因为图上每个元素都需要自己来定制。但仔细体会 matplotlib 背后的设计思想是很有趣的事情。seaborn 之类的绘图库是基于 matplotlib 封装的，因而后期需要自己灵活定制图形时就大大受用了。本文的两幅思维导图是基于两种不同的思路绘制的，偶有内容交叉，日常使用可以选择自己熟悉的方式（网上的教程大多是基于过程的函数式编程，即 pyplot 方法）。**建议配合最后附上出的参考资料学习**。

![](https://raw.githubusercontent.com/woaielf/woaielf.github.io/master/_posts/Pic/1704/170427-1.png)

![](https://raw.githubusercontent.com/woaielf/woaielf.github.io/master/_posts/Pic/1704/170427-2.png)

Reference
> 《Python for data analysis》<br>
[Matplotlib: Python plotting — Matplotlib 2.0.2 documentation](http://matplotlib.org/index.html) <br>
[绘图: matplotlib核心剖析](http://www.cnblogs.com/vamei/archive/2013/01/30/2879700.html#commentform) <br>
[【数据可视化】 之 Matplotlib](https://zhuanlan.zhihu.com/p/21443208?utm_medium=social&utm_source=qq?utm_medium=social&utm_source=qq) <br>
[Python--matplotlib绘图可视化知识点整理](http://python.jobbole.com/85106/) <br>
[一份非常好的Matplotlib 教程](http://blog.csdn.net/u011497262/article/details/52325705)


## pyecharts

- 【2020-11-27】多轮转化图

```python
import random
import pyecharts as pe
# 为了避免显示空白，加入以下代码执行后，重新刷新当前jupyter页面即可
pe.configure(
    jshost="https://pyecharts.github.io/assets/js",
    echarts_template_dir=None,
    force_js_embed=None,
    output_image=None,
    global_theme=None
)
# 代码参考：http://pyecharts.org/#/zh-cn/charts_base?id=heatmap%EF%BC%88%E7%83%AD%E5%8A%9B%E5%9B%BE%EF%BC%89
# x_axis = ["12a", "1a", "2a", "3a", "4a", "5a", "6a", "7a", "8a", "9a", "10a", "11a",
#     "12p", "1p", "2p", "3p", "4p", "5p", "6p", "7p", "8p", "9p", "10p", "11p"]
# y_axis = ["Saturday", "Friday", "Thursday", "Wednesday", "Tuesday", "Monday", "Sunday"]
# data = [[i, j, random.randint(0, 50)] for i in range(24) for j in range(7)]
page = pe.Page('多轮对话场景分析')
title = '投诉进度查询状态转化'
# (1) 节点状态转化图
x_axis = y_axis = attr_list
item_len = len(x_axis)
data = [[x_axis[i], y_axis[j], val_list[i][j]] for i in range(item_len) for j in range(item_len)]
heatmap = pe.HeatMap()
heatmap.add(
    '%s（x -> y）'%(title),
    x_axis,
    y_axis,
    data,
    is_visualmap=True,
    visual_range=[dh.get_values().min(), dh.get_values().max()],
    visual_text_color="#000",
    visual_orient="horizontal",
)
#heatmap.render()
page.add(heatmap)

# (2) Graph图谱
from pyecharts import Graph

nodes = [{"name": "结点1", "symbolSize": 10},
         {"name": "结点2", "symbolSize": 20},
         {"name": "结点3", "symbolSize": 30},
         {"name": "结点4", "symbolSize": 40},
         {"name": "结点5", "symbolSize": 50},
         {"name": "结点6", "symbolSize": 40},
         {"name": "结点7", "symbolSize": 30},
         {"name": "结点8", "symbolSize": 20}]
links = []
for i in nodes:
    for j in nodes:
        links.append({"source": i.get('name'), "target": j.get('name')})
nodes = json.loads(json.dumps(nodes))
graph = Graph("%s-状态图"%(title))
graph.add("", nodes, links, repulsion=80000, symbol='roundRect', layout='force',edgeSymbolSize= [0, 20],edgeSymbol =['circle', 'arrow'])
#graph.render()
page.add(graph)
# 集中渲染到html文件
page.render('muti_turn_vis.html')
page
```


# 结束
















