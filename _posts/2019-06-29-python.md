---
layout: post
title:  "Python技能"
date:   2019-06-29 19:17:00
categories: 技术工具
tags: Python Numpy Pandas pyecharts virtualenv yaml 面向对象 编码规范 装饰器 anaconda conda 设计模式 多进程 多线程 regex
author : 鹤啸九天
excerpt: Web开发相关技术知识点
mathjax: true
---

* content
{:toc}

# 总结

- 【2021-1-23】python代码调用过程可视化工具，[ryven](https://ryven.org/index)
  - ![](https://ryven.org/img/examples/ryven1.png)


- aadict包：在dict的基础上封装了一层，使其能够像操作属性一样使用字典。使得后续操作更加简便且易读，还可以预防不必要的异常出现。
- 代码示例

```python
from addict import Dict

configs = Dict()

configs.platform.status = "on"
configs.platform.web.task.name = "测试-1204"
configs.platform.web.periods.start = "2020-10-01"
configs.platform.web.periods.end = "2020-10-31"
# 获取不存在的key时会返回一个空字典，不用担心报KeyError，对返回值做判空处理即可
app_configs = configs.platform.app
assert app_configs == {}
```

# Python语法


## 基础数据结构

Python中的内置数据结构（Built-in Data Structure）:列表list、元组tuple、字典dict、集合set
- list
  - list的显著特征：
    - 列表中的每个元素都可变的，意味着可以对每个元素进行修改和删除；
    - 列表是有序的，每个元素的位置是确定的，可以用索引去访问每个元素；
    - 列表中的元素可以是Python中的任何对象；
    - 可以为任意对象就意味着元素可以是字符串、整数、元组、也可以是list等Python中的对象。
    - Python中包含6中內建的序列：列表，元组，字符串、Unicode字符串、buffer对象和xrange对象
- tuple
  - 元组Tuple，用法与List类似，但Tuple一经初始化，就不能修改，没有List中的append(), insert(), pop()等修改的方法，只能对元素进行查询
- dict
  -  字典dictionary全称这个概念就是基于现实生活中的字典原型，生活中的使用名称-内容对数据进行构建，Python中使用键(key)-值(value)存储，也就是java、C++中的map。
  - 字典中的数据必须以键值对的形式出现，即k,v： 
    - key:必须是可哈希的值，比如intmstring,float,tuple,但是，list,set,dict不行 ; value:任何值
  - 键不可重复，值可重复,键若重复字典中只会记该键对应的最后一个值
  - 字典中键(key)是不可变的，何为不可变对象，不能进行修改；而值(value)是可以修改的，可以是任何对象。在dict中是根据key来计算value的存储位置，如果每次计算相同的key得出的结果不同，那dict内部就完全混乱了。
- set
  - 集合更接近数学上集合的概念。集合中每个元素都是无序的、不重复的任意对象。可以通过集合去判断数据的从属关系，也可以通过集合把数据结构中重复的元素减掉。集合可做集合运算，可添加和删除元素。
  - 集合内数据无序，即无法使用索引和分片, 集合内部数据元素具有唯一性，可以用来排除重复数据, 集合内的数据:str,int,float,tuple,冰冻集合等，即内部只能放置可哈希数据
  - frozen set:冰冻集合是不可以进行任何修改的集合
frozenset是一种特殊集合
  - 常用方法
    - intersection：交集
    - difference：差集
    - union：并集
    - issubset：检查一个集合是否为另一个子集
    - issuperset：检查一个集合是否为另一个超集 


```python
mylist = ['Google', 'Yahoo', 'Baidu']
#变更索引位置1Yahoo的内容为Microsoft
mylist[1] = 'Microsoft'
#获取索引位置1到5的数据，注意这里只会取到索引位置4,这里叫做取头不取尾
mylist[1:5]   # 'Tencent', 'Microsoft', 'Baidu', 'Alibaba'
#获取从最头到索引位置5的数据
mylist[ :5]   #'Google', 'Tencent', 'Microsoft', 'Baidu', 'Alibaba'

#获取从索引位置2开到最后的数据
mylist[2:]    #'Microsoft', 'Baidu', 'Alibaba','Sina'
mylist.append('Alibaba')  #运行结果： ['Google', 'Microsoft', 'Baidu', 'Alibaba']
mylist.insert(1, 'Tencent')  # ['Google', 'Tencent', 'Microsoft', 'Baidu', 'Alibaba']
# 删除尾部元素
mylist.pop()      # 会返回被删除元素
# 删除指定位置的元素
mylist.pop(1)  # 删除索引为1的元素，并返回删除的元素
mylist.remove('Microsoft') #删除列表中的Microsoft
del mylist[1:3]       #删除列表中索引位置1到位置 3 的数据
mylist.sort()          # 排序 [1, 2 ,4, 5]
print(len(mylist)) # 长度

a = [1,2,3,4,5,6]
#在a的数据基础上每个数据乘以10，再生成一个列表b，
b = [i*10 for i in a]
#生成一个从1到20的列表
a = [x for x in range(1,20)]
#把a中所有偶数生成一个新的列表b
b = [m for m in a if m % 2 == 0]
print(b)

# tuple
a = (1,2,3,4)

# dict
d = {}
d = dict() # 创建空字典2
d = {"one":1,"two":2,"three":3,"four":4} #直接赋值方式
d = dict.fromkeys(d.keys(), "222")
d = {k:v for k,v in d.items()} #常规字典生成式
d = {k:v for k,v in d.items() if v % 2 ==0} #加限制条件的字典生成方式
print(d.get("one333"))
del d["one"] #删除一个数据,使用del
d.clear() # 清空字典
print(d)

if "two" in d:
    print("key")

#for k in d:
#for v in d.values():
for k in d.keys():
    print(k,d[k])
for k,v in d.items():
    print(k,'--->',v)
#for key, value in enumerate(words):

# 通用函数：len,max,min,dict
d = {"one":1,"two":2,"three":3,"four":4}
print(max(d), min(d), len(d))

#集合的定义
s = set()
s = frozenset() # 冰冻集合
s = set([1,2,3])
s.add(6)
s.remove(2)

s1 = {1,2,3,4,5,6,7}
s2 = {5,6,7,8,9}

#交集
s_1 = s1.intersection(s2)
#差集
s_2 = s1.difference(s2)
#并集
s_3 = s1.union(s2)
#检查一个集合是否为另一个子集
s_4 = s1.issubset(s2)
print("检查子集结果：",s_4)
#检查一个集合是否为另一个超集
s_5 = s1.issuperset(s2)
print("检查超集结果：",s_5)

```

## 高级数据结构

- 【2021-3-11】[Python高级数据结构详解](https://www.jb51.net/article/62930.htm)
- Collection、Array、Heapq、Bisect、Weakref、Copy以及Pprint这些数据结构的用法
- Collections
  - collections模块包含了内建类型之外的一些有用的工具，例如Counter、defaultdict、OrderedDict、deque以及nametuple。其中Counter、deque以及defaultdict是最常用的类。
  - Counter：统计频次
  - Deque：双端队列
    - Deque是一种由队列结构扩展而来的双端队列(double-ended queue)，队列元素能够在队列两端添加或删除。因此它还被称为头尾连接列表(head-tail linked list)
    - Deque支持线程安全的，经过优化的append和pop操作，在队列两端的相关操作都能够达到近乎O(1)的时间复杂度。虽然list也支持类似的操作，但是它是对定长列表的操作表现很不错，而当遇到pop(0)和insert(0, v)这样既改变了列表的长度又改变其元素位置的操作时，其复杂度就变为O(n)了
  - Defaultdict: 默认字典
- Array
  - array模块定义了一个很像list的新对象类型，不同之处在于它限定了这个类型只能装一种类型的元素。
  - 省空间，但时间慢：如存储一千万个整数，用list至少需要160MB的存储空间，而使用array只需要40MB。但虽然说能够节省空间，array上基本操作比list慢。
  - 列表推导式(list comprehension)时，会将array整个转换为list，使得存储空间膨胀。一个可行的替代方案是使用生成器表达式创建新的array
- heapq
  - heapq模块使用一个用堆实现的优先级队列。堆是一种简单的有序列表，并且置入了堆的相关规则。堆是一种树形的数据结构，树上的子节点与父节点之间存在顺序关系。
  - 函数：
    - heappush(heap, x) : 将x压入堆中
    - heappop(heap): 从堆中弹出最小的元素
    - heapify(heap) : 让列表具备堆特征
    - heapreplace(heap, x) : 弹出最小的元素，并将x压入堆中
    - nlargest(n, iter) : 返回iter中n个最大的元素
    - nsmallest(n, iter): 返回iter中n个最小的元素
- Bisect
  - bisect模块能够提供保持list元素序列的支持，使用了二分法完成大部分的工作。它在向一个list插入元素的同时维持list是有序的。
- Weakref
  - weakref模块能够帮助我们创建Python引用，却不会阻止对象的销毁操作。
  - strong reference是一个对对象的引用次数、生命周期以及销毁时机产生影响的指针。
  - Weak reference则是对对象的引用计数器不会产生影响。当一个对象存在weak reference时，并不会影响对象的撤销。这就说，如果一个对象仅剩下weak reference，那么它将会被销毁。
- Copy()
  - 通过shallow或deep copy语法提供复制对象的函数操作。shallow和deep copying的不同之处在于对于混合型对象的操作(混合对象是包含了其他类型对象的对象，例如list或其他类实例)。
  1. 对于shallow copy而言，它创建一个新的混合对象，并且将原对象中其他对象的引用插入新对象。
  2. 对于deep copy而言，它创建一个新的对象，并且递归地复制源对象中的其他对象并插入新的对象中。
- Pprint()
  - Pprint模块能够提供比较优雅的数据结构打印方式，如果你需要打印一个结构较为复杂，层次较深的字典或是JSON对象时，使用Pprint能够提供较好的打印结果。

```python
from collections import Counter
 
li = ["Dog", "Cat", "Mouse", 42, "Dog", 42, "Cat", "Dog"]
a = Counter(li)
print(a) # Counter({'Dog': 3, 42: 2, 'Cat': 2, 'Mouse': 1})
print(len(set(li))) # 4
print(a.most_common(3)) # [('Dog', 3), ('Cat', 2), ('Mouse', 1)]

from collections import deque

q = deque(range(5))
q.append(5)
q.appendleft(6)
print q
print q.pop()
print q.popleft()
print q.rotate(3) # 队列的旋转操作,Right rotate(正参数)是将右端的元素移动到左端，而Left rotate(负参数)则相反

from collections import defaultdict

location = defaultdict(['a', 'b']) # 有序
location = defaultdict(('a', 'b')) # 无序

# 对于较大的array，这种in-place修改能够比用生成器创建一个新的array至少提升15%的速度
import array

a = array.array("i", [1,2,3,4,5])
b = array.array(a.typecode, (2*x for x in a)) # 节省空间
for i, x in enumerate(a): # 提效
    a[i] = 2*x

import heapq
 
heap = []
for value in [20, 10, 30, 50, 40]:
    heapq.heappush(heap, value) # 入堆

while heap:
    print heapq.heappop(heap) # 出堆

nums = [1, 8, 2, 23, 7, -4, 18, 23, 42, 37, 2]
print(heapq.nlargest(3, nums)) # Prints [42, 37, 23]
print(heapq.nsmallest(3, nums)) # Prints [-4, 1, 2]
expensive = heapq.nlargest(3, portfolio, key=lambda s: s['price']) # 针对字典的复杂操作

import bisect
 
a = [(0, 100), (150, 220), (500, 1000)]
bisect.insort_right(a, (250,400))
print bisect.bisect(a, (550, 1200)) # 5 寻找插入点
print a # [(0, 100), (150, 220), (250, 400), (500, 1000)]



import weakref

a = Foo() #created
b = a() # 强引用
b = weakref.ref(a) # 弱引用
b = weakref.proxy(a) # proxy更像是一个strong reference，但当对象不存在时会抛出异常
# 删除strong reference的时候，对象将立即被销毁
del a

import copy

a = [1,2,3]
b = [4,5]
c = [a,b]

d = c # 正常复制
print id(c) == id(d)          # True - d is the same object as c
print id(c[0]) == id(d[0])    # True - d[0] is the same object as c[0]

d = copy.copy(c) # Shallow Copy 浅拷贝，创建一个新的容器，其包含的引用指向原对象中的对象
print id(c) == id(d)          # False - d is now a new object
print id(c[0]) == id(d[0])    # True - d[0] is the same object as c[0]

d = copy.deepcopy(c) # Deep Copy 深拷贝，创建的对象包含的引用指向复制出来的新对象
print id(c) == id(d)          # False - d is now a new object
print id(c[0]) == id(d[0])    # False - d[0] is now a new object

import pprint

matrix = [ [1,2,3], [4,5,6], [7,8,9] ]
a = pprint.PrettyPrinter(width=20)
a.pprint(matrix)

# [[1, 2, 3],
#  [4, 5, 6],
#  [7, 8, 9]]

```


# 安装

## 自动编译安装python

- 【2020-7-3】脚本如下：

```shell

#-----自定义区------
# 下载python3
src_file='https://www.python.org/ftp/python/3.8.3/Python-3.8.3.tgz'
file_name="${src_file##*/}"
install_dir=~/bin
#-------------------
 
[ -e ${file_name} ]||{
        wget ${src_file}
        echo "下载完毕..."
}&& echo "文件已存在, $file_name"
# 解压
tar zxvf ${file_name}
echo "安装目录: $install_dir"
# 安装
new_dir=${file_name%.*}
cd $new_dir
./configure --prefix=${install_dir}/python38
# 如果不设置安装目录prefix, 就会提示sudo权限
make && make install
echo "安装完毕，请设置环境变量"
 
# 设置环境变量
#vim ~/.bash_profile
echo "
alias python3='${install_dir}/python38/bin/python3.8'
alias pip3='${install_dir}/python38/bin/pip3'
" >> ~/.bash_profile
 
echo '生效'
source ~/.bash_profile


echo '修改pip源'
mkdir ~/.pip
echo "
[global]
index-url = https://pypi.tuna.tsinghua.edu.cn/simple
[install]
trusted-host=mirrors.aliyun.com
" > ~/.pip/pip.conf

# 或者一行命令设置
#pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple

```


## anaconda

- 下载

```shell
# mac环境下安装
#brew cask install anaconda
brew install anaconda
export PATH="/usr/local/anaconda3/bin:$PATH"

# 官方地址，https://www.anaconda.com/products/individual，上面有最新版本地址
wget https://repo.anaconda.com/archive/Anaconda3-2021.05-Linux-x86_64.sh

# 获取mincoda linux最新版
wget -c https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh
# mincoda mac用户
curl -O https://repo.anaconda.com/miniconda/Miniconda3-latest-MacOSX-x86_64.sh

# 清华镜像地址: https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/, 找最新下载地址
wget https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/Anaconda3-5.3.1-Linux-x86_64.sh

bash Anaconda3-2019.03-Linux-x86_64.sh
# [2021-6-15]注意：直接使用以上命令会出错：from conda.cli import main ModuleNotFoundError: No module named 'conda'
# 解法：加-u
bash Anaconda3-2019.03-Linux-x86_64.sh -u 

```

- 安装完成之后会多几个应用
   - Anaconda Navigtor ：用于管理工具包和环境的图形用户界面，后续涉及的众多管理命令也可以在 Navigator 中手工实现。
   - Jupyter notebook ：基于web的交互式计算环境，可以编辑易于人们阅读的文档，用于展示数据分析的过程。
   - qtconsole ：一个可执行 IPython 的仿终端图形界面程序，相比 Python Shell 界面，qtconsole 可以直接显示代码生成的图形，实现多行代码输入执行，以及内置许多有用的功能和函数。
   - spyder ：一个使用Python语言、跨平台的、科学运算集成开发环境。
- 加入环境变量
   - 安装器若提示“`Do you wish the installer to prepend the Anaconda install location to PATH in your /home/<user>/.bash_profile ?`，建议输入“yes”。
   - 如果输入“no”，则需要手动添加路径。添加 export PATH="/<anaconda_path>/bin:$PATH" 在 .bashrc 或者 .bash_profile 中。

- 注意：
   - 不要擅自在bash_profile中添加alias别名！会导致虚拟环境切换后python、pip转换失效

```shell
# anaconda 环境
export PATH="~/anaconda3/bin:$PATH"
# 以下语句不要添加！
alias python='/home/wangqiwen004/anaconda3/bin/python'
alias pip='/home/wangqiwen004/anaconda3/bin/pip'
# 如果仍然失效，强制使用变量切换
sra(){
    CONDA_ROOT="~/anaconda3"
    env=$1
    conda activate $env
    export LD_LIBRARY_PATH="$CONDA_ROOT/envs/$env/lib:$LD_LIBRARY_PATH" 
    export PATH=$CONDA_ROOT/envs/$env/bin:$PATH
}
# 【2020-7-10】以上方法不支持默认环境base的切换，优化如下：
sra(){
    CONDA_ROOT="~/anaconda3"
    # 获取当前虚拟环境名称列表(不含base)
    env_list=(`conda info -e | awk '{if($1!~/#|base/)printf $1" "}'`)
    env=$1
    conda activate $env
    echo "env=$env, str=${env_list[@]}"
    # 判断是否匹配已有环境名称
    # echo "${env_list[@]}" | grep $env && echo "yes" || echo "no"
    #[[ "$1" =~ "${env_str}" ]] && echo "yes" || echo "no"
    #[[ ${env_list[@]/${env}/} != ${env_list[@]} ]] && {
    res="no"
    for i in ${env_list[@]}
    do
         [ "$i" == "$env" ] && res="yes"
    done
    [ $res == "yes" ] && {
        echo "找到目标环境$env"
        export LD_LIBRARY_PATH="$CONDA_ROOT/envs/$env/lib:$LD_LIBRARY_PATH"
        export PATH=$CONDA_ROOT/envs/$env/bin:$PATH
    }||{
        echo "启用默认环境base"
        env="base"
        export LD_LIBRARY_PATH="$CONDA_ROOT/lib:$LD_LIBRARY_PATH"
        export PATH=$CONDA_ROOT/bin:$PATH
    }
    echo "环境切换完毕: --> $env"
}
alias srd='conda deactivate'
# 激活的使用方法
sra learn
```

- 注意：不要这样加！

### 常用命令

- 汇总如下：
   -  [anaconda完全手册](https://www.jianshu.com/p/eaee1fadc1e9)
   - [Anaconda介绍、安装及使用教程](https://zhuanlan.zhihu.com/p/32925500)

```shell
conda --version # 查看版本
activate # 切换到base环境
activate learn # 切换到learn环境
conda create -n learn python=3 # 创建一个名为learn的环境并指定python版本为3(的最新版本，也可以是2.7、3.6等))
conda create -n learn numpy matplotlib python=2.7 # 创建环境同时安装必要的包
conda create -n py36_tf1 python=3.6 tensorflow==1.11 # [2020-7-21]
conda create --prefix="D:\\my_python\\envs\\my_py_env"  python=3.6.3 # 自定义虚拟环境
conda create --name env_name --clone learn # 克隆环境 learn -> env_name
conda env list # 列出conda管理的所有环境, conda info -e 
source activate learn # linux下激活虚拟环境conda activate，windows下为：activate learn
source deactivate # linux下关闭虚拟环境conda deactivate，windows下为：deactivate
conda list # 列出当前环境的所有包
conda list -n learn # 列出某环境下的所有包
conda install requests #安装requests包, 同pip install
conda install -n your_env_name [package] # 即可安装package到your_env_name中
conda remove requests #卸载requets包
conda remove -n learn --all # 删除learn环境及下属所有包
conda remove --name learn  package_name  # 删除learn环境下某个包
conda update requests # 更新requests包
conda search pyqtgraph # 搜索包(模糊查找)
conda search --full-name pyqtgraph # 精确查找
conda env export > environment.yaml # 导出当前环境的包信息
conda env create -f environment.yaml # 用配置文件创建新的虚拟环境

# 添加Anaconda的TUNA镜像
conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/
# TUNA的help中镜像地址加有引号，需要去掉
 
# 设置搜索时显示通道地址
conda config --set show_channel_urls yes

# 【2021-6-15】或直接修改文件
vim ~/.condarc

channels:
  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/
  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/
  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge/
  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch/
ssl_verify: true

```

### 问题解决

- 使用conda install 安装各种包的时候速度很慢，参考：[conda install速度慢](https://blog.csdn.net/mojiewangday/article/details/105583026)
- 解决
   - 修改conda镜像路径
   - 执行如下命令，更换仓库径路为清华镜像路径

```
conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/
conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/
```
   - 在自己用户目录C:\Users<你的用户名>下生成一个文件，名字为：~/.condarc

```
conda config --set show_channel_urls yes
```
   - 修改.condarc文件为如下:

```
channels:
  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/
show_channel_urls: true
ssl_verify: true
```

   - 执行完上述三步，conda的镜像路径就更换完毕，如不放心可以 conda info 查看 channel URLs 信息已经更改。

- 【2020-8-22】执行conda install flask-restplus时，pip没问题
   - Solving environment: failed with initial frozen solve. Retrying with flexible solve
   - 解决：执行
      - conda config --add channels conda-forge
      - conda config --set channel_priority flexible

【2021-7-15】问题：
- PackagesNotFoundError: The following packages are not available from current channels
- 解决：conda config --add channels https://anaconda.org (缺失的源)

# Python功能

## 路径文件

除了os，还有Path，代码：

```python
import os

os.path.abspath(path)	#返回绝对路径
os.path.basename(path)	#返回文件名
os.path.commonprefix(list)	#返回list(多个路径)中，所有path共有的最长的路径
os.path.dirname(path)	#返回文件路径
os.path.exists(path)	#路径存在则返回True,路径损坏返回False
os.path.lexists	#路径存在则返回True,路径损坏也返回True
os.path.expanduser(path)	#把path中包含的"~"和"~user"转换成用户目录
os.path.expandvars(path)	#根据环境变量的值替换path中包含的"$name"和"${name}"
os.path.getatime(path)	#返回最近访问时间（浮点型秒数）
os.path.getmtime(path)	#返回最近文件修改时间
os.path.getctime(path)	#返回文件 path 创建时间
os.path.getsize(path)	#返回文件大小，如果文件不存在就返回错误
os.path.isabs(path)	#判断是否为绝对路径
os.path.isfile(path)	#判断路径是否为文件
os.path.isdir(path)	#判断路径是否为目录
os.path.islink(path)	#判断路径是否为链接
os.path.ismount(path)	#判断路径是否为挂载点
os.path.join(path1[, path2[, ...]])	#把目录和文件名合成一个路径
os.path.normcase(path)	#转换path的大小写和斜杠
os.path.normpath(path)	#规范path字符串形式
os.path.realpath(path)	#返回path的真实路径
os.path.relpath(path[, start])	#从start开始计算相对路径
os.path.samefile(path1, path2)	#判断目录或文件是否相同
os.path.sameopenfile(fp1, fp2)	#判断fp1和fp2是否指向同一文件
os.path.samestat(stat1, stat2)	#判断stat tuple stat1和stat2是否指向同一个文件
os.path.split(path)	#把路径分割成 dirname 和 basename，返回一个元组
os.path.splitdrive(path)	#一般用在 windows 下，返回驱动器名和路径组成的元组
os.path.splitext(path)	#分割路径中的文件名与拓展名
os.path.splitunc(path)	#把路径分割为加载点与文件
os.path.walk(path, visit, arg)	#遍历path，进入每个目录都调用visit函数，visit函数必须有3个参数(arg, dirname, names)，dirname表示当前目录的目录名，names代表当前目录下的所有文件名，args则为walk的第三个参数
os.path.supports_unicode_filenames	#设置是否支持unicode路径名

from pathlib import Path
p.cwd() # 获取当前路径
p.stat()  # 获取当前文件的信息
p.exists()  # 判断当前路径是否是文件或者文件夹
p.glob(filename)  # 获取路径下的所有符合filename的文件，返回一个generator
p.rglob(filename)  # 与上面类似，只不过是返回路径中所有子文件夹的符合filename的文件
p.is_dir()  # 判断该路径是否是文件夹
p.is_file()  # 判断该路径是否是文件
p.iterdir()  #当path为文件夹时，通过yield产生path文件夹下的所有文件、文件夹路径的迭代器
P.mkdir(parents=Fasle)  # 根据路径创建文件夹,parents=True时，会依次创建路径中间缺少的文件夹
p_news = p/'new_dirs/new_dir'
p_news.mkdir(parents=True)
P.open(mode=’r’, buffering=-1, encoding=None, errors=None, newline=None)  #类似于open()函数
p.rename(target)  # 当target是string时，重命名文件或文件夹;当target是Path时，重命名并移动文件或文件夹
p.replace(target)  # 重命名当前文件或文件夹，如果target所指示的文件或文件夹已存在，则覆盖原文件
p.parent(),p.parents()  # parent获取path的上级路径，parents获取path的所有上级路径
p.is_absolute()  # 判断path是否是绝对路径
p.match(pattern)  # 判断path是否满足pattern
p.rmdir()  # 当path为空文件夹的时候，删除该文件夹
p.name  # 获取path文件名
p.suffix  # 获取path文件后缀
```

## 正则表达式

正则解析，[图](https://pic3.zhimg.com/80/v2-6be63a41e531f003f2931160fdadaffa_720w.jpg) 源自[Python正则表达式](https://www.jianshu.com/p/5a34f2bf394a)

![](https://pic3.zhimg.com/80/v2-6be63a41e531f003f2931160fdadaffa_720w.jpg)

Python的**re模块**主要定义了9个常量、12个函数、1个异常
re.compile模式：
- re.I(re.IGNORECASE): 忽略大小写（括号内是完整写法，下同）
- M(MULTILINE): 多行模式，改变'^'和'$'的行为（参见上图）
- S(DOTALL): 点任意匹配模式，改变'.'的行为
- L(LOCALE): 使预定字符类 \w \W \b \B \s \S 取决于当前区域设定
- U(UNICODE): 使预定字符类 \w \W \b \B \s \S \d \D 取决于unicode定义的字符属性
- X(VERBOSE): 详细模式。这个模式下正则表达式可以是多行，忽略空白字符，并可以加入注释


前后查找的操作符：
- (?=)	正向前查找
- (?!)	负向前查找
- (?<=)	正向后查找
- (?<!)	负向后查找

正则表达re模块共有12个函数，概览：
- search、match、fullmatch：查找一个匹配项
  - search：查找任意位置的匹配项
  - match：必须从字符串开头匹配
  - fullmatch：整个字符串与正则完全匹配
  - 以上三种方法返回match对象，几个常用的方法如下：
    - m.start() 返回匹配到的字符串的起使字符在原始字符串的索引
    - m.end() 返回匹配到的字符串的结尾字符在原始字符串的索引
    - m.group() 返回指定组的匹配结果
    - m.groups() 返回所有组的匹配结果
    - m.span() 以元组形式给出对应组的开始和结束位置
- findall、finditer：查找多个匹配项
  - 1）findall： 从字符串任意位置查找，返回一个列表
  - 2）finditer：从字符串任意位置查找，返回一个迭代器，适用大量匹配项
- split： 分割
  - re.split(r"[;:]","var A;B;C:integer;")
- sub，subn：替换
  - re.sub('\d+','*','aaa34bvsa56s',count=1)
  - 与 re.sub函数 功能一致，只不过返回一个元组 (字符串, 替换次数)
  - re.subn('\d','*','aaa34bvsa56s')#每个数字都替换一次
- compile函数、template函数: 将正则表达式的样式编译为一个 正则表达式对象

正则语法总结：[图](https://upload-images.jianshu.io/upload_images/3326314-f7e9014ceb942f11.png)
![](https://upload-images.jianshu.io/upload_images/3326314-f7e9014ceb942f11.png)

代码示例

```python
import re

p = re.compile(r'(\w+) (\w+)(?P<sign>.*)', re.DOTALL)

print ("正则表达式:", p.pattern) # (\w+) (\w+)(?P<sign>.*)
print ("匹配模式:", p.flags) # 48
print ("分组数目:", p.groups) #  3
print ("别名分组:", p.groupindex) # {'sign': 3}

m = re.match(r'(\w+) (\w+)(?P<sign>.*)', 'hello world!')
 
print ("原始字符串:", m.string) # 要匹配的原始字符串
print ("原始正则表达式:", m.re) # 要匹配的原始正则
print ("索引范围: [{}, {}]".format(m.pos, m.endpos)) # [0, 12]
print ("捕获的最后一个分组索引:", m.lastindex) # 3
print ("捕获的最后一个分组别名:", m.lastgroup) # sign
print ("获取多个组:", m.group(1, 2)) # ('hello', 'world')
print ("获取全部组（元组）:", m.groups()) # ('hello', 'world', '!')
print ("获取全部别名组（字典）:", m.groupdict()) # {'sign': '!'}
print ("第2个字符串范围: [{},{}]".format(m.start(2), m.end(2))) # (6, 11)
print ("第2个字符串范围:", m.span(2)) # (6, 11)
print ("局部重排:", m.expand(r'\2 \1\3')) # world hello!

# 槽位字典
pattern_city = re.compile('(?P<city>西安|武汉|青岛|广州|重庆)市?(?P<district>城六|咸阳|高陵|长安)区?',re.X)
pattern_loan = re.compile('(?P<loan>全款|公积金|商业|组合)贷款?')

pattern_info = re.compile('(?P<city>西安|武汉|青岛|广州|重庆)市?(?P<district>城六|咸阳|高陵|长安)区?(?P<loan>全款|公积金|商业|组合)贷款?',re.X)

slot_dict = {'city': '-', 'loan':'-'}

query = '西安长安区商业贷款'
res1 = pattern_info.match(query)
print(res1) # match对象
print(res1.groupdict()) # 返回字典（含别名）
res2 = pattern_info.findall(query) # 返回列表
print(res2)
```

**regex模块**

【2021-8-9】[Python的regex模块——更强大的正则表达式引擎](https://www.cnblogs.com/animalize/p/4949219.html)

内置的re模块不支持一些高级特性，比如下面这几个：
- 固化分组    Atomic grouping
- 占有优先量词    Possessive quantifiers
- 可变长度的逆序环视    Variable-length lookbehind
- 递归匹配    Recursive patterns
- （起始/继续）位置锚\G    Search anchor
2009年，Matthew Barnett写了一个更强大正则表达式引擎——**regex模块**，这是一个Python的第三方模块。
- 安装：pip install regex
除了上面这几个高级特性，还有很多有趣、有用的东西，本文大致介绍一下，很多内容取自regex的文档。

regex基本兼容re模块，现有的程序可以很容易切换到regex模块：import regex as re；

regex有Version 0和Version 1两个工作模式，其中的Version 0基本兼容现有的re模块，以下是区别：

||Version 0  （基本兼容re模块）	|Version 1|
|---|---|---|
|启用方法|默认模式| 设置regex.DEFAULT_VERSION = regex.V1，或者在表达式里写上(?V1)|
|内联flag|内联flag只能作用于整个表达式，不可关闭。|内联flag可以作用于局部表达式，可以关闭。|
|字符组	|只支持简单的字符组。|字符组里可以有嵌套的集合，也可以做集合运算（并集、交集、差集、对称差集）。|
|大小写匹配	| 默认支持普通的Unicode字符大小写，如Й可匹配й。这与Python3里re模块的默认行为相同。|默认支持完整的Unicode字符大小写，如ss可匹配ß。可惜不支持Unicode组合字符与单一字符的大小写匹配，所以感觉这个特性不太实用。可以在表达式里写上(?-f)关闭此特性。|

V1模式默认开启.FULLCASE（完整的忽略大小写匹配）。通常用不上这个，所以在忽略大小写匹配时用(?-f)关闭.FULLCASE即可，这样速度更快一点，例如：(?i-f)tag

Version 0模式和re模块不兼容之处
- \s的范围。
  - 在re中，\s在这一带的范围是0x09 ~ 0x0D，0x1C ~ 0x1E。
  - 在regex中，\s采用的是Unicode 6.3+标准的\p{Whitespace}，在这一带的范围有所缩小，只有：0x09 ~ 0x0D。

- regex有模糊匹配（fuzzy matching）功能，能针对字符进行模糊匹配，提供了3种模糊匹配：
  - i，模糊插入
  - d，模糊删除
  - s，模糊替换
  - 以及e，包括以上三种模糊

```python
import regex as re

# 两次匹配都是把捕获到的内容放到编号为1捕获组中，在某些情况很方便
regex.match(r"(?|(first)|(second))", "first").groups() # ('first',)
regex.match(r"(?|(first)|(second))", "second").groups() # ('second',)

# 局部范围的flag控制。在re模块，flag只能作用于整个表达式，现在可以作用于局部范围；全局范围的flag控制，如 (?si-f)<B>good</B>
# (?i:)是打开忽略大小写，(?-i:)则是关闭忽略大小写。
# 如果有多个flag挨着写既可，如(?is-f:)，减号左边的是打开，减号右边的是关闭。
regex.search(r"<B>(?i:good)</B>", "<B>GOOD</B>") # <regex.Match object; span=(0, 11), match='<B>GOOD</B>'>

# 可重复使用的子句
regex.search(r'(?(DEFINE)(?P<quant>\d+)(?P<item>\w+))(?&quant) (?&item)', '5 elephants') # <regex.Match object; span=(0, 11), match='5 elephants'> ， 此例中，定义之后，(?&quant)表示\d+，(?&item)表示\w+。如果子句很复杂，能省不少事。

# 模糊匹配：匹配hello，其中最多容许有两个字符的错误。
regex.findall('(?:hello){s<=2}', 'hallo')
# ['hallo']

# 部分匹配。可用于验证用户输入，当输入不合法字符时，立刻给出提示。

# 编译好的正则式用pickle存储到文件，直接pickle.load()就不必再次编译，节省时间

```


## yaml

YAML是JSON（源）的超集。每个有效的JSON文件也是一个有效的YAML文件。这意味着您拥有所有期望的类型：整数，浮点数，字符串，布尔值，空值。以及序列和图。

### yaml介绍

- `YAML`是一种直观的能够被电脑识别的的**数据序列化**格式，容易被人类阅读，并且容易和脚本语言交互。`YAML`类似于`XML`，但是语法比XML简单得多，对于转化成数组或可以hash的数据时是很简单有效的。
  1. **大小写**敏感
  2. 使用**缩进**表示层级关系
  3. 缩进时不允许使用Tab，只允许使用**空格**
  4. 缩进的空格**数目不重要**，只要相同层级的元素左对齐即可
  5. \# 表示注释，从它开始到行尾都被忽略
- 支持的数据类型
  - 字符串，整型，浮点型，布尔型，null，时间，日期
- 主要特性，更多：[Python YAML用法详解](https://blog.csdn.net/lmj19851117/article/details/78843486)
  - & 锚点 和 * 引用
  - 强制转换，用!!实现
  - 同一个yaml文件中，可以用 --- 来分段
  - 构造器(constructors)、表示器(representers)、解析器(resolvers )
  - 包含子文件：!include
- 安装
  - pip install pyyaml

### yaml示例

- 【2021-4-6】[大多数程序员都不知道的6个YAML功能](https://www.toutiao.com/i6934590069487518212/)
- config.yaml内容：

```yaml
name: Tom Smith
name: 'Tom Smith' # str等效表达
# 长字符串(换行符非必须,仅为了好看) , > 
disclaimer: >
    Lorem ipsum dolor sit amet, consectetur adipiscing elit.
    In nec urna pellentesque, imperdiet urna vitae, hendrerit
    odio. Donec porta aliquet laoreet. Sed viverra tempus fringilla.
# 多行字符串, | 相当于行间换行符\n
mail_signature: |
      Martin Thoma
      Tel. +49 123 4567

age: 37
list_by_square_bracets_0: 
  - foo
  - bar
list_by_square_bracets_1: [foo, bar] # list的等效表达

spouse:
    name: Jane Smith
    age: 25
map_by_curly_braces: {foo: bar, bar: baz} # dict的等效表达
children:
 - name: Jimmy Smith
   age: 15
 - name1: Jenny Smith
   age1: 12
# YAML支持11种写布尔值的方法
bool_a : yes # true/True
bool_b : no # false/False
# 三个-表示多文档，返回list，如 [{'foo': 'bar'}, {'fizz': 'buzz'}]
---
# 这个例子输出一个字典，其中value包括所有基本类型
str: "Hello World!"
int: 110
float: 3.141
boolean: true  # or false
None: null  # 也可以用 ~ 号来表示 null
time: 2016-09-22t11:43:30.20+08:00  # ISO8601，写法百度
date: 2016-09-22  # 同样ISO8601
name: &name 灰蓝 # 设置被引用字段别名
tester: *name # * 取引用内容(仅含值)
# 成对引用(锚)
localhost: &localhost1
    host: 127.0.0.1
user:
    <<: *localhost1 # <<表示按照K/V形式成对展开，合并键
    db: 8

a: !!str 3.14 # 转字符串
b: !!int "123" # 转int
# 复杂类型
tuple_example: !!python/tuple
  - 1337
  - 42
set_example: !!set {1337, 42}
date_example: !!timestamp 2020-12-31
```

- 操作方法

```python
import yaml

# 文件
f = open(r'config.yml')
# 字符串
f = '''
---
name: James
age: 20
---
name: Lily
age: 19
'''
y = yaml.load(f) 
y = yaml.load_all(f) # 多个yaml区域
for data in y:
    print(data)
# 转成yaml文档
obj1 = {'name': 'Silenthand Olleander',
            'race': 'Human',
            'traits': ['ONE_HAND', 'ONE_EYE']
}
obj2 = {"name": "James", "age": 20}
print(yaml.dump(obj1，))
# 中文输出
import json
print(json.dumps(obj1, ensure_ascii=False, indent=2))
print(yaml.dump(d,default_flow_style=False, indent=2, allow_unicode=True))
f = open(r'out_config.yml','w')
print(yaml.dump(obj2,f))
yaml.dump_all([obj1, obj2], f) # 一次输出多个片区
```

### 高级功能

- 【2021-3-16】文件包含，YAML不包括任何种类的“import”或“include”语句。不过可以通过重写构造器来实现
  - 研究源码（[yaml文件嵌套](https://blog.csdn.net/tanruixing/article/details/88128818)）发现：yaml嵌套还是基于在load中重载一个钩子类，对yaml的value进行解析处理
  - 通过value前缀判定的！注意下面的a:后面要有一个空格。
- 参考：
  - [yaml 锚点*和包含include](http://www.bdata-cap.com/newsinfo/36658.html)
  - [yaml文件嵌套](https://blog.csdn.net/tanruixing/article/details/88128818)
  - [如何在Yaml文件引用其他Yaml文件(使用python Pyyaml)](https://www.cnblogs.com/robynn/p/8253783.html)


```yaml
# === 文件 bar.yaml ====
- 3.6
- [1, 2, 3]

# === 文件 foo.yaml ====
a: 1
b:
    - 1.43
    - 543.55
# 包含别的文件
c: !include bar.yaml
```

- python调用代码

```python
import os
import yaml

class Loader(yaml.Loader):

    def __init__(self, stream):
        self._root = os.path.split(stream.name)[0]
        super(Loader, self).__init__(stream)

    def include(self, node):
        filename = os.path.join(self._root, self.construct_scalar(node))
        with open(filename, 'r') as f:
            return yaml.load(f, Loader)
# 添加新的语法标记
Loader.add_constructor('!include', Loader.include)

def load_yaml(yaml_file):
    """
    :param yaml_file:
    :return:
    """
    with open(yaml_file, 'r') as f:
        config = yaml.load(f, Loader)
    return config

y = load_yaml('foo.yaml')
print(yaml.dump(y))
```
- 简洁版

```python
import yaml
import os

def yaml_include(loader, node):
# Get the path out of the yaml file
	file_name = os.path.join(os.path.dirname(loader.name), node.value)
	with open(file_name) as inputfile:
		return yaml.load(inputfile)

yaml.add_constructor("!include", yaml_include)
stream = open('foo.yaml', 'r')
print(yaml.dump(yaml.load(stream)))
```

- 或者

```python

#!/usr/bin/python
import os.path
import yaml
 
class IncludeLoader(yaml.Loader):
    def __init__(self, *args, **kwargs):
        super(IncludeLoader, self).__init__(*args, **kwargs)
        self.add_constructor('!include', self._include)
        if 'root' in kwargs:
            self.root = kwargs['root']
        elif isinstance(self.stream, file):
            self.root = os.path.dirname(self.stream.name)
        else:
            self.root = os.path.curdir
 
    def _include(self, loader, node):
        oldRoot = self.root
        filename = os.path.join(self.root, loader.construct_scalar(node))
        self.root = os.path.dirname(filename)
        data = yaml.load(open(filename, 'r'))
        self.root = oldRoot
        return data
 
class Config(object):
    def __init__(self, path):
        config_file = open(path, 'r')
        self.config_content = yaml.load(config_file, IncludeLoader)
        config_file.closed
 
    def getConfigContent(self):
        return self.config_content
```




## 打日志

日志级别等级
- CRITICAL > ERROR > WARNING > INFO > DEBUG > NOTSET

### [logging](https://docs.python.org/3/library/logging.html)模块

- [python日志基于时间切分和基于文件大小切分](https://www.jianshu.com/p/5a4e226444bd)
- 代码：

```python
#！coding:utf-8
import logging
import logging.handlers
import datetime, time

#logging    初始化工作
logger = logging.getLogger("zjlogger")
logger.setLevel(logging.DEBUG)

# 添加TimedRotatingFileHandler
# (1) 定义一个1秒换一次log文件的handler, 保留3个旧log文件
rf_handler = logging.handlers.TimedRotatingFileHandler(filename="all.log",when='S',interval=1, backupCount=3)
# (2) 写入文件，如果文件超过100个Bytes，仅保留5个文件。
handler = logging.handlers.RotatingFileHandler('logs/myapp.log', maxBytes=100, backupCount=5)

rf_handler.setFormatter(logging.Formatter("%(asctime)s - %(levelname)s - %(filename)s[:%(lineno)d] - %(message)s"))

#在控制台打印日志
handler = logging.StreamHandler()
handler.setLevel(logging.DEBUG)
handler.setFormatter(logging.Formatter("%(asctime)s - %(levelname)s - %(message)s"))

logger.addHandler(rf_handler)
logger.addHandler(handler)

while True:
    # 不同日志等级
    logger.debug('debug message')
    logger.info('info message')
    logger.warning('warning message')
    logger.error('error message')
    logger.critical('critical message')
    time.sleep(1)

```

### loguru模块

- 【2021-3-30】[不想手动再配置logging？那可以试试loguru](https://blog.csdn.net/BF02jgtRS00XKtCx/article/details/109126972)
- logging 库采用模块化设计，虽然可以设置不同的 handler 来进行组合，但是在配置上通常较为繁琐；而且如果不是特别处理，在一些多线程或多进程的场景下使用 logging 还会导致日志记录会出现错乱或是丢失的情况。
- loguru库不仅能够减少繁琐的配置过程还能实现和 logging 类似的功能，同时还能保证日志记录的线程进程安全，又能够和 logging 相兼容，并进一步追踪异常也能进行代码回溯。一个专为像我这样懒人而生日志记录库
- logger 本身就是一个已经实例化好的对象，如果没有特殊的配置需求，那么自身就已经带有通用的配置参数；同时它的用法和 logging 库输出日志时的用法一致, 可配置部分相比于 logging 每次要导入特定的handler再设定一些formatter来说是更为「傻瓜化」了
- 日志文件留存、压缩，甚至自动清理。可以通过对 rotation 、compression 和 retention 三个参数进行设定来满足：
rotation 参数能够帮助我们将日志记录以大小、时间等方式进行分割或划分

```python
#!pip install loguru
from loguru import logger

logger.debug("debug message"    ) 
logger.info("info level message") 
logger.warning("warning level message") 
logger.critical("critical level message")

# 可配置
import os
 
logger.add(os.path.expanduser("~/Desktop/testlog.log"))
# 序列化配置：通过 serialize 参数将其转化成序列化的 json 格式，最后将导入类似于 MongoDB、ElasticSearch 这类数 NoSQL 数据库中用作后续的日志分析。
logger.add(os.path.expanduser("~/Desktop/testlog.log"), serialize=True)
# loguru 集成了一个名为 better_exceptions 的库，不仅能够将异常和错误记录，并且还能对异常进行追溯
logger.add(os.path.expanduser("~/Desktop/exception_log.log"), backtrace=True, diagnose=True)
# 与logging兼容
import logging.handlers
file_handler = logging.handlers.RotatingFileHandler(LOG_FILE, encoding="utf-8")
logger.add(file_handler)
# -------------
logger.info("hello, world!")

# 日志自动清理
LOG_DIR = os.path.expanduser("~/Desktop/logs")
LOG_FILE = os.path.join(LOG_DIR, "file_{time}.log")
if os.path.exits(LOG_DIR):
    os.mkdir(LOG_DIR)
# ①自动切分日志，大小、时间等方式进行分割或划分
logger.add(LOG_FILE, rotation = "200KB")
# ②分割文件的数量越来越多，可以进行压缩对日志进行压缩、留存
logger.add(LOG_FILE, rotation = "200KB", compression="zip")
# ③只想保留一段时间内的日志并对超期的日志进行删除；
#   当然对 retention 传入整数时，该参数表示的是所有文件的索引，而非要保留的文件数；只有两个时间最近的日志文件会被保留下来，其他都被直接清理掉了
logger.add(LOG_FILE, rotation="200KB",retention=1)
for n in range(10000):
    logger.info(f"test - {n}")
                                                                                                                          
```
- loguru 还为输出的日志信息带上了不同的颜色样式（schema），使得结果更加美观
![](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy81bXQwZXd2OU9TM0xNdE5uOUZxeWhScnB1a2lhc1B6WjhxQ0FVUFg3N3JpYlM1ZVFpY3dPTWVObG5vbkFoNDlGMVlKcmE0cmpuQjNvTTFoYUJpYTNwNXlNY0EvNjQw?x-oss-process=image/format,png)

## Python版本转换

- 【2020-8-24】Python代码版本转换

```shell
2to3 -help # 帮助
2to3 -w .　　#将当前整个文件夹代码从python2转到python3
# python2文件会在.py后面再加上一个后缀.bak，而新生成的python3文件使用之前python2文件的命名

```

### virtualenv

- 【2018-1-3】[Python--Virtualenv简明教程](http://python.jobbole.com/85398/)，[virtualenv官方文档](http://virtualenv.readthedocs.org/en/latest/virtualenv.html)
- python沙盒环境，virtualenv创建一个拥有自己安装目录的环境, 这个环境不与其他虚拟环境共享库, 能够方便的管理python版本和管理python库
- （1）安装：

```shell
pip install virtualenv
#或者由于权限问题使用sudo临时提升权限
sudo pip install virtualenv
virtualenv -h #获得帮助
```
- （2）创建：
    - 用virtualenv管理python环境
    - virtualenv ENV  # 创建一个名为ENV的目录, 并且安装了ENV/bin/python
    - 生成ENV目录，包含bin（解释器）、include和lib（python库）文件夹
    - 运行 virtualenv --system-site-packages ENV, 会继承/usr/lib/python2.7/site-packages下的所有库, 最新版本virtualenv把把访问全局site-packages作为默认行为
- （3）激活虚拟环境：

```shell
cd ENV
source ./bin/activate  #激活当前virtualenv
(ENV)➜  ENV git:(master) ✗ #注意终端发生了变化
```

【2020-5-14】注意：windows下没有bin目录，参考，执行方法：.\scripts\activate.bat

- （4）关闭
    - deactivate
- （5）指定python版本
    - 可以使用-p PYTHON_EXE选项在创建虚拟环境的时候指定python版本

```shell
#创建python2.7虚拟环境
➜  Test git:(master) ✗ virtualenv -p /usr/bin/python2.7 ENV2.7
virtualenv -p "C:\Program Files (x86)\Python\Python27" py2
```


## Python依赖包

- 【2020-8-23】python项目依赖包管理
- 安装：
   - pip install -r requirements.txt
- 自动生成requirements.txt
   - pip install pipreqs 
   - pipreqs /path/to/project
- 生成requirements.txt文件

```shell
pip freeze  #显示所有依赖
pip freeze > requirement.txt  #生成requirement.txt文件
pip install -r requirement.txt  #根据requirement.txt生成相同的环境
```

- 对比分析

|工具包|优点|缺点|
|---|---|---|
|pip freeze|包含列表完全|不相关的依赖包也会包含进来|
|pipreqs|只会包含项目 imports 的包|包含列表不是很完全|
|pip-compile|精准控制项目依赖包|需要手动操作，不方便|

## 单元测试

- 一个完整的测试脚本（用例）一般包含以下几个步骤：
  - 环境准备或检查
  - 执行业务操作
  - 断言结果
  - 清理环境
- 而测试框架一般还要完成用例加载，批量执行，异常控制，结果输出等功能。基础的测试框架一般只提供执行控制方面的功能。
- 单测框架
  - **unittest**: Python自带，最基础的单元测试框架
  - **nose**: 基于unittest开发，易用性好，有许多插件
  - **pytest**: 同样基于unittest开发，易用性好，信息更详细，插件众多
  - **robot framework**：一款基于Python语言的关键字驱动测试框架，有界面，功能完善，自带报告及log清晰美观
- 总结（[Python测试框架对比----unittest, pytest, nose, robot framework对比](https://my.oschina.net/u/4339883/blog/3479038)）
  - unittest比较基础，二次开发方便，适合高手使用；pytest/nose更加方便快捷，效率更高，适合小白及追求效率的公司；robot framework由于有界面及美观的报告，易用性更好，灵活性及可定制性略差。
- [2020-11-6] 经验总结, 提交代码前，执行单测，确保代码功能无误
- 主目录下执行命令: [pytest使用笔记（一）](https://www.pianshen.com/article/4986199865/)
   - pytest # 整体报告
   - pytest -q # 运行简单模式测试报告
   - pytest -x # 在第一个测试用例发生错误时就停止运行
   - pytest --maxfail=2 # 在第2个测试用例发生错误时就停止运行
   - pytest test_mod.py # 执行具体文件
   - pytest test # 运行单个文件的用例
   - pytest -k "TestClass" # 运行某些包含关键字的用例，如包含TestClass的用例
   - pytest test_class.py::TestClass # 运行某一文件内特定模块的用例
   - pytest -m slow # 运行用@ pytest.mark.slow装饰器修饰的用例

- 安装
  - pylint工具安装：sudo pip install pylint --ingore-installed
- 使用：pylint test.py
- 结果：MESSAGE_TYPE 有如下几种：
  - (C) 惯例。违反了编码风格标准
  - (R) 重构。写得非常糟糕的代码。
  - (W) 警告。某些 Python 特定的问题。
  - (E) 错误。很可能是代码中的错误。
  - (F) 致命错误。阻止 Pylint 进一步运行的错误。
- 问题
  - No config file found, using default configuration——解决：生成pylint配置文件，touch ~/.pylintrc
  - C: 19, 0: Trailing newlines (trailing-newlines)——删除多余的空行
  - C: 17, 4: Invalid constant name "new" (invalid-name)——全局变量名字全部大写
  - C: 9, 0: Exactly one space required after comma——多参数时，逗号后加空格
  - C: 13, 4: Invalid variable name "s" (invalid-name)——局部变量命名过短，3个字符以上(避免单字母，除了计数器+迭代器；避免双下划线开头并结尾；避免包/模块名中的连字符-)，_开头表示protected，__开头表示private
  - C: 10, 0: Old-style class defined. (old-style-class)——类定义中没有加object。，类名大写字母开头（pascal风格，如CapWord，模块、函数名小写字母与_，如lower_with_under.py）
  - R: 10, 0: Too few public methods (1/2) (too-few-public-methods)——缺少修改类属性值的方法
  - C: 25, 0: Trailing whitespace (trailing-whitespace)——行尾有空格


## 编码规范

【2017-11-23】python编码规范，目前有google和pep8两种，pylint默认pep8，[Google python编码规范](https://zh-google-styleguide.readthedocs.io/en/latest/google-python-styleguide/python_style_rules/)，[如何用pylint规范代码风格](https://www.ibm.com/developerworks/cn/linux/l-cn-pylint/)
   - 安装方法：sudo pip install -U pep8/pylint
- 【2018-7-13】[python性能优化的20条建议](https://segmentfault.com/a/1190000000666603),【2019-1-11】[Python如何正确使用import？](https://blog.csdn.net/Greenovia/article/details/79399475)，总结import的各种用法及区别，精华

代码示例

```python
#!/usr/bin/env python
# coding:utf8
"""
    test sample. google编码规范：(URL不受80字符限制)
    https://zh-google-styleguide.readthedocs.io/en/latest/google-python-styleguide/python_style_rules/
    2020-7-16
    wangqiwen004@ke.com
"""
 
#(1)、以单下划线开头，表示这是一个保护成员，只有类对象和子类对象自己能访问到这些变量。以单下划线开头的变量和函数被默认当作是内部函数，使用from module improt *时不会被获取，但是使用import module可以获取
#(2)、以单下划线结尾仅仅是为了区别该名称与关键词
#(3)、双下划线开头，表示为私有成员，只允许类本身访问，子类也不行。在文本上被替换为_class__method
#(4)、双下划线开头，双下划线结尾。一种约定，Python内部的名字，用来区别其他用户自定义的命名,以防冲突。是一些 Python 的“魔术”对象，表示这是一个特殊成员，例如：定义类的时候，若是添加__init__方法，那么在创建类的实例的时候，实例会自动调用这个方法，一般用来对实例的属性进行初使化，Python不建议将自己命名的方法写为这种形式。
 
#import的包一定要使用;import包分成3部分，依次排序：①系统包②第三方包③自定义包。每部分按照字母顺序排序，一次不能导入多个包
import sys
 
class MyClass(object):
    """class测试: 类名满足Pascal风格"""
    public_name = '-public-' # public
    _myname = '-protected' # protected，不能通过import *导入，其它正常
    __private_name = '-private-' # private，实例、子类不能使用
  
    def __init__(self, name="wang"): # 特殊函数方法
        self._myname = name
        print '我的名字是%s'%(self._myname)
  
    def say(self):
        """打招呼"""
        print '你好,我是%s,%s,%s'%(self._myname, self.public_name, self.__private_name)
        return 'yes'
  
    def modify(self, name="-"):
        """更改属性值"""
        self._myname = name
  
def my_fun(value=0, delta=9):
    """
        外部函数：名字_连接。多参数时,逗号后面加一个空格
    """
    res = value + delta
    return res
  
def main():
    """main function"""
    #main里的都是全局变量,需要大写
    value = 3
    new = my_fun(value)
    v_result = MyClass("wqw")
    #不能访问protected、private变量.W._myname, W.__private_name
    #超过80字符时，可以用\换行，注：(),[]时可省略\
    print >> sys.stdout, 'hello,related values are listed as : %s , %s,I am \
        %s,%s ...'%(value, new, v_result.say(), v_result.public_name)
    print >> sys.stdout, 'hello,related values are listed as : %s , %s,I am %s,%s ...'%(value, new, v_result.say(), v_result.public_name) # pylint: disable=line-too-long
    #参考:怎么关闭某类检测：How do I disable a Pylint warning?
    #https://stackoverflow.com/questions/4341746/how-do-i-disable-a-pylint-warning
  
if __name__ == '__main__':
    A = 3 # 此处为全局变量,一律大写
    main()
  
# */* vim: set expandtab ts=4 sw=4 sts=4 tw=400: */
```

# 高级语法

Python三器：`装饰器`、`迭代器`和`生成器`

## 装饰器

- 【2021-4-26】[不懂Python装饰器，你敢说会Python?](http://www.toutiao.com/i6954182546359910924)

- 场景：多个函数中添加计时代码
- 解法：装饰器

### 定义

把原来的函数给包了起来，在不改变原函数代码的情况下，在外面起到了装饰作用，这就是传说中的装饰器。它其实就是个普通的函数。

### 示例

```python
import time

# （1）不带参数装饰器
def timer(func):
 '''统计函数运行时间的装饰器'''
 def wrapper():
  start = time.time()
  func()
  end = time.time()
  used = end - start
  print(f'{func.__name__} used {used}')
 return wrapper

# 先调用timer函数，生成一个包装了step1的新的函数timed_step1.
# 剩下的就是调用这个新的函数time_step1()，它会帮我们记录时间。
timed_step1 = timer(step1)
timed_step1()
# 简洁版
timer(step1)()

# （2）带参数的装饰器
#   wrapper使用了通配符，*args代表所有的位置参数，**kwargs代表所有的关键词参数。这样就可以应对任何参数情况。
#   wrapper调用被装饰的函数的时候，只要原封不动的把参数再传递进去就可以
def timer(func):
 '''统计函数运行时间的装饰器'''
 def wrapper(*args, **kwargs):
  start = time.time()
  func(*args, **kwargs)
  end = time.time()
  used = end - start
  print(f'{func.__name__} used {used}')
 return wrapper

# @符号，语法糖衣
@timer
def step1(num):
 print(f'我走了#{num}步')

step1(5)

# （3）带返回值的装饰器
def timer(func):
 '''统计函数运行时间的装饰器'''
 def wrapper(*args, **kwargs):
  start = time.time()
  ret_value = func(*args, **kwargs)
  end = time.time()
  used = end - start
  print(f'{func.__name__} used {used}')
  return ret_value
 return wrapper

@timer
def add(num1, num2):
 return num1 + num2

sum = add(5, 8)
print(sum)
```



# python技能


## uuid生成

- [python生成并处理uuid的方法](https://blog.csdn.net/yl416306434/article/details/80569688)

- uuid是128位的全局唯一标识符（univeral unique identifier），通常用32位的一个字符串的形式来表现。有时也称guid(global unique identifier)。python中自带了uuid模块来进行uuid的生成和管理工作。（具体从哪个版本开始有的不清楚。。）
- python中的uuid模块基于信息如MAC地址、时间戳、命名空间、随机数、伪随机数来uuid。具体方法有如下几个：　　
   - uuid.uuid1()　　基于MAC地址，时间戳，随机数来生成唯一的uuid，可以保证全球范围内的唯一性。
   - uuid.uuid2()　　算法与uuid1相同，不同的是把时间戳的前4位置换为POSIX的UID。不过需要注意的是python中没有基于DCE的算法，所以python的uuid模块中没有uuid2这个方法。
   - uuid.uuid3(namespace,name)　　通过计算一个命名空间和名字的md5散列值来给出一个uuid，所以可以保证命名空间中的不同名字具有不同的uuid，但是相同的名字就是相同的uuid了。【感谢评论区大佬指出】namespace并不是一个自己手动指定的字符串或其他量，而是在uuid模块中本身给出的一些值。比如uuid.NAMESPACE_DNS，uuid.NAMESPACE_OID，uuid.NAMESPACE_OID这些值。这些值本身也是UUID对象，根据一定的规则计算得出。
   - uuid.uuid4()　　通过伪随机数得到uuid，是有一定概率重复的
   - uuid.uuid5(namespace,name)　　和uuid3基本相同，只不过采用的散列算法是sha1

一般而言，在对uuid的需求不是很复杂的时候，uuid1方法就已经够用了

```python
import uuid

name = 'test_name'
# namespace = 'test_namespace'
namespace = uuid.NAMESPACE_URL

print uuid.uuid1()
print uuid.uuid3(namespace,name)
print uuid.uuid4()
print uuid.uuid5(namespace,name)
```

## 性能优化

- 【2020-9-10】[Python语言优化](https://www.cnblogs.com/pypypy/p/11995237.html)
- 原生的python通常都是由cpython实现，而cpython的运行效率，确实让人不敢恭维，比较好的解决方案有cython、numba、pypy等等

### cython

![](https://yqfile.alicdn.com/d0a10d313a638bc6a18eb6ef4b1632920a2b133a.png)


### numba

![](https://yqfile.alicdn.com/94af3cf79a6a076262e647cf491247928939ff79.jpeg)

### pypy

![](https://yqfile.alicdn.com/ae64deb6d0ce804914e023290429be7812e62a16.jpeg)

## 异步

- Python语言没有真正的进程，而是通过协程来实现，多线程也受制于GIL
  - Python因为有GIL（全局解释锁）这玩意，不可能有真正的多线程的存在，因此很多情况下都会用multiprocessing实现并发，而且在Python中应用多线程还要注意关键地方的同步，不太方便，用**协程**代替**多线程**和**多进程**是一个很好的选择，因为它吸引人的特性：主动调用/退出，状态保存，避免cpu上下文切换等…
- 参考  
  - [从0到1，Python异步编程的演进之路](https://zhuanlan.zhihu.com/p/25228075)

多线程

```python
mutex = threading.Lock() #创建锁
mutex.acquire([timeout]) #锁定
# 锁定方法acquire可以有一个超时时间的可选参数timeout。如果设定了timeout，则在超时后通过返回值可以判断是否得到了锁，从而可以进行一些其他的处理。
mutex.release() #释放
```

完整示例

```python
#!/usr/bin/env python
#coding=utf-8
import threading
import time
 
class MyThread(threading.Thread):
    def run(self):
        global num
        time.sleep(1)
 
        if mutex.acquire(1): 
            num = num+1
            msg = self.name+' set num to '+str(num)
            print msg
            mutex.release()
num = 0
mutex = threading.Lock()
def test():
    for i in range(5):
        t = MyThread()
        t.start()
if __name__ == '__main__':
    test()
```

输出：
- Thread-1 set num to 1
- Thread-3 set num to 2
- Thread-4 set num to 3
- Thread-5 set num to 4
- Thread-2 set num to 5


```python
import threading
import time

lock = threading.Lock() #创建锁

def fun(data):
    try:
        lock.acquire(True) #锁定
        print("------> fun 1:",time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(time.time())),data)
        time.sleep(5)
        print("------> fun 2:", time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(time.time())),data)
    finally:
        lock.release()#释放

threading.Thread(target = fun, name='socket_tcp_server', kwargs={'data':100}).start()
threading.Thread(target = fun, name='socket_tcp_server', kwargs={'data':200}).start()
threading.Thread(target = fun, name='socket_tcp_server', kwargs={'data':300}).start()
threading.Thread(target = fun, name='socket_tcp_server', kwargs={'data':400}).start()
```


### 协程

- 协程，又称作Coroutine。从字面上来理解，即协同运行的例程，它是比是线程（thread）更细量级的用户态线程
  - 特点是允许用户的主动调用和主动退出，挂起当前的例程然后返回值或去执行其他任务，接着返回原来停下的点继续执行。
- 问题
  - 函数都是线性执行的，怎么会说执行到一半返回，等会儿又跑到原来的地方继续执行
  - 答案是用yield语句
    - yield能把一个函数变成一个generator，与return不同，yield在函数中返回值时会保存函数的状态，使下一次调用函数时会从上一次的状态继续执行，即从yield的下一条语句开始执行
    - 好处：数列较大时，可以一边循环一边计算的机制，节省了存储空间，提高了运行效率
  - 操作系统具有getcontext和swapcontext这些特性，通过系统调用，我们可以把上下文和状态保存起来，切换到其他的上下文，这些特性为coroutine的实现提供了底层的基础。操作系统的Interrupts和Traps机制则为这种实现提供了可能性
  - ![](https://pic2.zhimg.com/80/v2-a4989f7971c96d897c94b9d956ee743d_720w.png)

代码

```python
def fib(max):
    n, a, b = 0, 0, 1
    while n  max:
        # 每次调用函数时，都要耗费大量时间循环做重复的事情
        # print b
        # 用yield，则会生成一个generator，需要时，调用它的next方法获得下一个值
        yield b
        a, b = b, a + b
        n = n + 1
```
用协程实现生产者-消费者模式

```python
#-*- coding:utf-8
def consumer():
    status = True
    while True:
        n = yield status
        print("我拿到了{}!".format(n))
        if n == 3:
            status = False

def producer(consumer):
    n = 5
    while n > 0:
    # yield给主程序返回消费者的状态
        yield consumer.send(n)
        n -= 1

if __name__ == '__main__':
    # 由于yield，不同于一般函数执行，Python会当做一个generator对象
    c = consumer() 
    # 将consumer（即变量c，它是一个generator）中的语句推进到第一个yield语句出现的位置
    #    函数里的status = True和while True:都已经被执行了，程序停留在n = yield status的位置（未执行）
    c.send(None) # 如果漏写这一句，那程序直接报错
    # 定义了producer的生成器，注意的是这里我们传入了消费者的生成器，来让producer跟consumer通信。
    p = producer(c)
    # 循环地运行producer和获取它yield回来的状态
    for status in p:
        if status == False:
            print("我只要3,4,5就行啦")
            break
    print("程序结束")
```
- 让生产者发送1,2,3,4,5给消费者，消费者接受数字，返回状态给生产者，而我们的消费者只需要3,4,5就行了，当数字等于3时，会返回一个错误的状态。最终我们需要由主程序来监控生产者－消费者的过程状态，调度结束程序。
- 生产者p调用了消费者c的send()方法，把n发送给consumer（即c），在consumer中的n = yield status，n拿到的是消费者发送的数字，同时，consumer用yield的方式把状态（status）返回给消费者，注意：这时producer（即消费者）的consumer.send()调用返回的就是consumer中yield的status！消费者马上将status返回给调度它的主程序，主程序获取状态，判断是否错误，若错误，则终止循环，结束程序。上面看起来有点绕，其实这里面generator.send(n)的作用是：把n发送generator(生成器)中yield的赋值语句中，同时返回generator中yield的变量（结果）。
- 结果

```
我拿到了5!
我拿到了4!
我拿到了3!
我只要3,4,5就行啦
程序结束
```

- Coroutine与Generator
- 相似：
  - 都是不用return来实现重复调用的函数/对象，都用到了yield(中断/恢复)的方式来实现。
- 区别
  - generator总是生成值，一般是迭代的序列
  - coroutine关注的是消耗值，是数据(data)的消费者
  - coroutine不会与迭代操作关联，而generator会
  - coroutine强调协同控制程序流，generator强调保存状态和产生数据
- **协程**相比于**多进程**和**多线程**的优点
  - ①多进程和多线程创建开销大
  - ②一个难以根治的缺陷，处理进程之间或线程之间的协作问题，因为是依赖多进程和多线程的程序在不加锁的情况下通常是不可控的，而协程则可以完美地解决协作问题，由用户来决定协程之间的调度。

### asyncio

- asyncio是python 3.4中新增的模块，它提供了一种机制，使得你可以用协程（coroutines）、IO复用（multiplexing I/O）在单线程环境中编写并发模型。
- 根据官方说明，asyncio模块主要包括了：
  - 具有特定系统实现的事件循环（event loop）;
  - 数据通讯和协议抽象（类似Twisted中的部分);
  - TCP，UDP,SSL，子进程管道，延迟调用和其他;
  - Future类;
  - yield from的支持;
  - 同步的支持;
  - 提供向线程池转移作业的接口;

下面来看下asyncio的一个例子：

```python
import asyncio

async def compute(x, y):
    print("Compute %s + %s ..." % (x, y))
    await asyncio.sleep(1.0)
    return x + y

async def print_sum(x, y):
    result = await compute(x, y)
    print("%s + %s = %s" % (x, y, result))

loop = asyncio.get_event_loop()
loop.run_until_complete(print_sum(1, 2))
loop.close()
```

### 效率进阶之路

- 爬虫效率进阶之路：
  - （1）urllib和urllib2包抓5个页面总共耗时7.6秒
  - （2）requests包6.5秒！虽然比用urllib快了1秒多，但是总体来说，他们基本还是处于同一水平线的，程序并没有快很多，这一点的差距或许是requests对请求做了优化导致的。
  - （3）lxml库→正则，继续提升1s
  - （4）多线程：threading库有效的解决了阻塞等待的问题，足足比之前的程序快了80%！只需要1.4秒就可完成电影列表的抓取。
    - 多线程受限于GIL，能否用多进程提速？
  - （5）多进程：ProcessPoolExecutor库实现多进程
    - ThreadPoolExecutor和ProcessPoolExecutor是Python3.2之后引入的分别对线程池和进程池的一个封装，如果使用Python2.x，需要安装futures这个库才能使用
    - 多进程带来的优点（cpu处理）并没有得到体现，反而创建和调度进程带来的开销要远超出它的正面效应，拖了一把后腿。即便如此，多进程带来的效益相比于之前单进程单线程的模型要好得多。
  - （6）基于协程的网络库，叫做gevent，异步功能，只有1.2秒，果然很快
    - gevent给予了我们一种以同步逻辑来书写异步程序的能力，看monkey.patch_all()这段代码，它是整个程序实现异步的黑科技
    - 但gevent的魔术给他带来了一定的困惑，并非Pythonic的清晰优雅
  - （7）async/await：同步的requests库改成了支持asyncio的aiohttp库，使用3.5的async/await语法（3.5之前用@asyncio.coroutine和yield from代替）写出了协程版本，1.7s
    - 清晰优雅的协程可以说实现异步的最优方案之一
    - 协程的机制使得我们可以用同步的方式写出异步运行的代码。
  - 思考：
    - 更换零件带来的优化小，而且扩展性有限
    - 网络应用通常瓶颈都在IO层面，解决等待读写的问题比提高文本解析速度来的更有性价比！
- 但Python在3.5版本中引入了关于协程的语法糖async和await
  - 用async修饰将普通函数和生成器函数包装成异步函数和异步生成器
  - 通过await调用async修饰的异步函数
  - async 用来声明一个函数为异步函数，异步函数的特点是能在函数执行过程中挂起，去执行其他异步函数，等到挂起条件（假设挂起条件是sleep(5)）消失后，也就是5秒到了再回来执行。
  - await 用来用来声明程序挂起，比如异步程序执行到某一步时需要等待的时间很长，就将此挂起，去执行其他的异步程序。
- 参考
  - [Python Async/Await入门指南](https://zhuanlan.zhihu.com/p/27258289)

```python
# 异步函数（协程）
async def async_function():
    return 1
# 异步生成器
async def async_generator():
    yield 1
# 通过类型判断可以验证函数的类型
import types
print(type(function) is types.FunctionType)
print(type(generator()) is types.GeneratorType)
print(type(async_function()) is types.CoroutineType)
print(type(async_generator()) is types.AsyncGeneratorType)

# 直接调用异步函数不会返回结果，而是返回一个coroutine对象：
print(async_function())
## <coroutine object async_function at 0x102ff67d8>

# 协程需要通过其他方式来驱动，因此可以使用这个协程对象的send方法给协程发送一个值：
print(async_function().send(None))
# 改进：捕获协程的真正返回值，新建run函数驱动协程
def run(coroutine):
    try:
        coroutine.send(None)
    except StopIteration as e:
        return e.value
# 在协程函数中，可以通过await语法来挂起自身的协程，并等待另一个协程完成直到返回结果：
async def async_function():
    return 1

async def await_coroutine():
    result = await async_function()
    print(result)
    
run(await_coroutine())
# 1


```


## 多进程

### 多任务调度框架

- 【2021-1-21】多年前写的多任务调度框架，每天处理20T上网数据

```python
# ***************************************************************************
# *
# * Copyright (c) 2012 Baidu.com, Inc. All Rights Reserved
# *
# **************************************************************************/
  
#/**
# * @file start.py
# * @author wangqiwen@baidu.com(zhanglun@baidu.com)
# * @date 2013/04/26 16:32
# * detect if data is ready
# * process data use codes defined in conf
# * output data to path in conf
# * multi thread , multi job commit
# * log is added
# **/
 
#import packages:
import ConfigParser
import string
import os
import sys
import threading
import time
import datetime
import logging
import urllib, re
import commands
import json
 
sys.path.append('.')
 
from optparse import OptionParser
 
#define class:
#job spec conf
class JobSpecConf:
    def __init__(self):
        self.job_name = None
        self.input_ready = []
        self.inputs = []
        self.must_input = []
        self.code_path = None
        self.detect_start_time = 0
        self.detect_end_time = 0
        self.delay_before_start = 0
        self.output_ready = None
        self.map_con_num = 0
        self.reduce_num = 0
 
#load jon conf
def loadJobConf(conf_file,date):
    config = ConfigParser.ConfigParser()
    config.read(conf_file)
    global_conf.map_dict['$date'] = date
    global_conf.map_dict['$date_pre'] = getPreDate(date=date)
    # tranform job conf info
    job_conf_list = []
    try:
        for section in config.sections():
            # all job session name starts with 'job' in configre file
            if not section.startswith('job'):
                continue
            # new job conf object
            job_spec_conf = JobSpecConf()
            job_spec_conf.job_name = config.get(section,"job_name")
            # update job_name in map_dict
            global_conf.map_dict['$job_name'] = job_spec_conf.job_name
            # get input path & ready
            item_list = config.options(section)
            path_list = [multiReplace(config.get(section,i),global_conf.map_dict) for i in item_list if i.startswith('input_path_')]
            ready_list = [multiReplace(config.get(section,i),global_conf.map_dict) for i in item_list if i.startswith('input_ready_')]
            job_spec_conf.input_number = len(path_list)
            if len(path_list) != len(ready_list):
                logger.error("configure file(%s) error: input path & ready in section %s doesn't match !" %(conf_file,section))
                return job_conf_list
            job_spec_conf.inputs = path_list
            job_spec_conf.input_ready = ready_list
            if 'must_input' in item_list:
                job_spec_conf.must_input = [multiReplace(config.get(section,'input_ready_%s'%(i)),global_conf.map_dict) for i in config.get(section,"must_input").split('&')]
                logger.info('[%s] must_input = %s'%(job_spec_conf.job_name,repr(job_spec_conf.must_input)))
            # get other info
            job_spec_conf.output = multiReplace(config.get(section,"output"),global_conf.map_dict)
            job_spec_conf.output_ready = multiReplace(config.get(section,"output_ready"),global_conf.map_dict)
            job_spec_conf.code_path = multiReplace(config.get(section,"code_path"),global_conf.map_dict)
            job_spec_conf.detect_start_time = config.get(section,"detect_start_time")
            job_spec_conf.detect_end_time = config.get(section,"detect_end_time")
            job_spec_conf.delay_before_start = config.getint(section,"delay_before_start")
            job_spec_conf.map_con_num = config.get(section,"map_con_num")
            job_spec_conf.reduce_num = config.get(section,"reduce_num")
            job_conf_list.append(job_spec_conf)
        logger.info("job conf info: %s" %(repr(job_conf_list)))
    except Exception,err:
        logger.error("error:(%s) when loading configure file(file:%s,section:%s)" %(err,conf_file,section))
    return job_conf_list
 
def multiReplace(str,dict):
    rx = re.compile('|'.join(map(re.escape,dict)))
    def one_xlat(match):
        return dict[match.group(0)]
    return rx.sub(one_xlat,str)
  
 
#init logger
def initlog(date,job_type):
    logger = logging.getLogger('%s.py' % job_type)
    hdlr = logging.FileHandler('../log/%s/%s_%s.txt' % (now,job_type,date),mode="wb")
    formatter = logging.Formatter('[%(levelname)s] [%(asctime)s]: %(message)s')
    hdlr.setFormatter(formatter)
    logger.addHandler(hdlr)
    logger.setLevel(logging.DEBUG)
    return logger
 
def send_mail(mail_alarm='on',title='mail title',content='mail content',receiver=None):
    """
        usage: send_email(title,content,receiver)
        shell: echo -e "hello" | mail -s "title" receivers
    """
    if mail_alarm != 'on':
        logger.info('alarm mail is off ...')
        return 0
    if receiver == None:
        logger.info('Mail receiver address (%s) invalid !' %(receiver))
        exit(-1)
    code = os.system("echo -e \"%s\" | mail -s \"%s\" \"%s\"" %(content,title,receiver))
    if 0 == code:
        logger.info('Success to send mail (%s:%s) to %s' %(title,content,receiver))
        return 0
    else:  
        logger.error('Fail to send mail (%s:%s) to %s, error code (%s) ,program exit ...' %(title,content,receiver,code))
        exit(-1)
 
def get_jobtracker_running_job_name_ids(url):
    while True:
        try:
            lines = urllib.urlopen(url).readlines()
        except:
            raise
            return False
        if lines:
            break
        else:
            logger.error("%s not reachable" % url)
     
    found = False
    jobs = {}
    for line in lines:
        line = line.strip()
        if found == True:
            if line.startswith('<h2'):
                break
            o = re.search('(job_[0-9]+_[0-9]+).+id="name_[0-9]+">([^<]+)<', line)
            if o:
                name, jid = o.groups()
                jobs[name] = jid
        if line == """<h2 id="running_jobs">Running Jobs</h2>""":
            found = True
    return jobs
 
#define class:
#global conf
class GlobalConf:
    def __init__(self):
        self.retry_interval = 0
        self.output_path = ''
        self.root_code_path = ''
        self.hadoop_client = ''
        self.re_run_number = 0
        self.jobtracker = ''
        self.map_dict = {'$output_path':'-','$date_pre':'-','$date':'-','$job_name':'-'}
        self.mail_receiver_in = None
        self.mail_alarm = 'on'
        self.mail_receiver_out = None
 
#define functions:
#load global config
def globalConfLoad(config_file):
    #load conf
    config = ConfigParser.ConfigParser()
    config.read(config_file)
    #init conf
    global_conf = GlobalConf()
    global_conf.retry_interval = config.getint("global","retry_interval")
    global_conf.output_path = config.get("global","output_path")
    global_conf.map_dict['$output_path'] = global_conf.output_path
    global_conf.root_code_path = config.get("global","root_code_path")
    global_conf.hadoop_client = config.get("global","hadoop_client")
    global_conf.re_run_number = config.getint("global","re_run_number")
    global_conf.jobtracker = config.get("global", "jobtracker")   
    global_conf.mail_receiver_in = config.get("global","mail_receiver_in")
    global_conf.mail_receiver_out = config.get("global","mail_receiver_out")
    global_conf.mail_alarm = config.get("global","mail_alarm")
    return global_conf
 
#test path exist
def checkHadoopFile(path,date,global_conf):
    path_after_parse = multiReplace(path,global_conf.map_dict)
    cmd = '%s fs -test -e %s' % (global_conf.hadoop_client,path_after_parse)
    code, msg = commands.getstatusoutput(cmd)
    if code == 0:
        return True
    elif msg:
        logger.warning('hadoop file (%s) not found , error_id : %s' %(path_after_parse,msg))
        return False
    return False
 
def getPreDate(n = 1 , date = 'today'):
    '''
        get string of date before n days
    '''
    #date_pre = (datetime.datetime.strptime(date,'%Y%m%d')+datetime.timedelta(days=1)).strftime('%Y%m%d')
    if date == 'today':
        date_object = datetime.date.today()
    else:
        date_object = datetime.datetime.strptime(date,'%Y%m%d')
    date_pre = (date_object + datetime.timedelta(days=-n)).strftime('%Y%m%d')
    return date_pre
 
def stopTask(job_spec_conf, date):
    task_prefix = "%s_%s_%s" % (options.pipeline, date, job_spec_conf.job_name)
    found = True
    while found:
        found = False
        running_jobs = get_jobtracker_running_job_name_ids(global_conf.jobtracker)
        for job in running_jobs:
            if job.startswith(task_prefix):
                logger.info("%s: killing job %s " % (job_spec_conf, running_jobs[job]))
                os.system('%s job -kill %s' % (global_conf.hadoop_client, running_jobs[job]))
                found = True
 
def cleanData(job_spec_conf, date, stop = False):
    if stop:
        stopTask(job_spec_conf, date)
    data_path = job_spec_conf.output
    ready_path = job_spec_conf.output_ready
    if 0 == os.system('%s fs -test -e %s' %(global_conf.hadoop_client,ready_path)):
        os.system('%s fs -rmr %s && %s fs -rmr %s' %(global_conf.hadoop_client,ready_path,global_conf.hadoop_client,data_path))
        logger.info("[%s] clean data (%s,%s)" %(job_spec_conf.job_name,data_path,ready_path))
 
def startMultiThead(date, tasks, options):
    logger.info('Begin to run the whole job of day: %s' % date)
    all_job_list = []
    all_job_list = loadJobConf(options.config_file,date)
    all_job_dict = {}
    for job in all_job_list:
        all_job_dict[job.job_name] = job
    logger.info('all jobs in conf : %s' % json.dumps([i.job_name for i in all_job_list]))
    # select some jobs
    if tasks:
        job_conf_list = []
        for jobname in tasks:
            if jobname in all_job_dict:
                job_conf_list.append(all_job_dict[jobname])
            else:
                logger.error('job name (%s) error ! please check with configure file(%s),all job names:%s'%(jobname,options.config_file,repr(all_job_dict.keys())))
                return False
    else:
        job_conf_list = all_job_list
    # check whether each job need to run or not
    job_conf_list = [c for c in job_conf_list if checkTask(c, date, options)]
    logger.info('jobs need to run : %s' % repr([i.job_name for i in job_conf_list]))
    if len(job_conf_list) <= 0:
        logger.info('no job in list , exit ...')
        return False
    # clean old data
    for c in job_conf_list:
        cleanData(c, date, True)
    if options.kill_task:
        return True
    # start multithread
    if options.parallel:
        # run all jobs at the same time
        threads = [threading.Thread(target = tryRunTask, args = (c, date, options)) for c in job_conf_list]
        for t in threads:
            t.start()
        for t in threads:
            t.join()
    else:
        # run one by one
        for c in job_conf_list:
            tryRunTask(c, date, options)
    logger.info('End of day: %s' % date)
    
def runOneTask(job_spec_conf, date, options):
    start_time = datetime.datetime.strptime(getPreDate(n=0)+job_spec_conf.detect_start_time,'%Y%m%d%H:%M')
    end_time = datetime.datetime.strptime(getPreDate(n=0)+job_spec_conf.detect_end_time,'%Y%m%d%H:%M')
    # wait some time until start
    if job_spec_conf.delay_before_start > 0:
        logger.info('[%s] sleep %s mins before start to detect ...'%(job_spec_conf.job_name,job_spec_conf.delay_before_start))
        time.sleep(job_spec_conf.delay_before_start*60)
    # Initialize input path
    input_ready_number = 0
    detect_input_dict = {}
    for i,v in enumerate(job_spec_conf.input_ready):
        detect_input_dict[v] = job_spec_conf.inputs[i]
    input_ready_list = []
    input_path_list = []
    # waiting
    while True:
        now_time = datetime.datetime.now()
        time_info = '[now %s,start %s,end %s]' %(str(now_time),str(start_time),str(end_time))
        #count ready input file
        tmp_key_list = detect_input_dict.keys()
        for tmp_input_ready in tmp_key_list:
            if checkHadoopFile(tmp_input_ready,date,global_conf):
                logger.info('[%s] detect input file (%s): ready ...' % (job_spec_conf.job_name,tmp_input_ready))
                input_ready_list.append(tmp_input_ready)
                input_path_list.append(detect_input_dict[tmp_input_ready])
                del detect_input_dict[tmp_input_ready]
            else:
                logger.warning('[%s] detect input file (%s): not ready... %s' % (job_spec_conf.job_name,tmp_input_ready,time_info))
        input_ready_number = len(input_ready_list)
        logger.info('[%s] detect result : ready num => %d, must input num => %d, all input num => %d...' % (job_spec_conf.job_name,input_ready_number,len(job_spec_conf.must_input),job_spec_conf.input_number))
        # check whether match min input
        match = 0
        if input_ready_number > 0:
            match = 1
        for i in job_spec_conf.must_input:
            if i not in input_ready_list:
                match = 0
                break
        if options.run_force and match :
            logger.warning("[%s] run forcely... input data ready info : %s/%s " %(job_spec_conf.job_name,input_ready_number,job_spec_conf.input_number))
            break
        #too early
        if now_time < start_time:
            logger.info('[%s] time enough , wait until start time ... %s' %(job_spec_conf.job_name,time_info))
            logger.info('[%s] sleep %d minutes (retry_interval)' % (job_spec_conf.job_name,global_conf.retry_interval))
            time.sleep(global_conf.retry_interval*60)
            continue
        #all ready
        if input_ready_number == job_spec_conf.input_number :
            logger.info('[%s] detect_start_time %s , detect_end_time %s , all inputs are ready ...' %(job_spec_conf.job_name,job_spec_conf.detect_start_time,job_spec_conf.detect_end_time))
            break
        # too late
        if options.enable_timeout and now_time >= end_time:
            if match:
                logger.warning('[%s] input ready before time over, ... input data ready info : %s/%s ' %(job_spec_conf.job_name,input_ready_number,job_spec_conf.input_number))
                break
            else:
                logger.warning('[%s] time over , but input files not enough , exit now ... input data ready info : %s/%s ' %(job_spec_conf.job_name,input_ready_number,job_spec_conf.input_number))
                exit(-1)
        logger.info('[%s] sleep %d minutes (retry_interval)' % (job_spec_conf.job_name,global_conf.retry_interval))
        time.sleep(global_conf.retry_interval*60)
             
    # run
    hadoop_job_cmd = 'sh %s%s %s %s %s %s %s %s %s %s %s &>../log/%s/%s_%s.txt' % (
                global_conf.root_code_path,
                job_spec_conf.code_path,
                global_conf.hadoop_client,
                '"%s"' % (';'.join(input_path_list)),
                job_spec_conf.output,
                job_spec_conf.job_name,
                global_conf.root_code_path,
                date,
                "%s_%s_%s" % (options.pipeline, date, job_spec_conf.job_name),
                job_spec_conf.map_con_num,
                job_spec_conf.reduce_num,
                now,
                job_spec_conf.job_name,
                date)
    logger.info('[%s] hadoop job command: %s' % (job_spec_conf.job_name, hadoop_job_cmd))
    code, msg = commands.getstatusoutput(hadoop_job_cmd)
    if code != 0:
        logger.error('[%s]: job Failled when running, error_code = %s(%s)' %(job_spec_conf.job_name,code,msg))
        title = '[merge-wise][ERROR] [%s] Job failed when running !'%(job_spec_conf.job_name)
        content = '[ERROR] [%s] Job failed when running , error_code = %s(%s)'%(job_spec_conf.job_name,code,msg)
        logger.error('mail_receiver_in=%s'%(global_conf.mail_receiver_in))
        send_mail(global_conf.mail_alarm,title,content,global_conf.mail_receiver_in)
        return False
    # result info : all input,ready input,miss input
    result_dict = {'input_list':[],'input_num':0,'ready_list':[],'ready_num':0,'miss_list':[],'miss_num':0}
    for i in job_spec_conf.input_ready:
        if i in input_ready_list:
            result_dict['ready_list'].append(i)
        else:
            result_dict['miss_list'].append(i)
        result_dict['input_list'].append(i)
    result_dict['ready_num'] = len(result_dict['ready_list'])
    result_dict['miss_num'] = len(result_dict['miss_list'])
    result_dict['input_num'] = len(result_dict['input_list'])
    logger.info('[%s] result_dict : %s' %(job_spec_conf.job_name,repr(result_dict)))
     
    # create ready file
    output_ready = job_spec_conf.output_ready
    hadoop_ready_cmd = 'echo %s | %s fs -put - %s' %(json.dumps(result_dict),global_conf.hadoop_client,output_ready)
    logger.info('create ready file : %s' %(hadoop_ready_cmd))
    code, msg = commands.getstatusoutput(hadoop_ready_cmd)
    if code != 0:
        logger.error('[%s] fail to create ready file (%s) , error_code = %s(%s)'%(job_spec_conf.job_name,output_ready,code,msg))
        title = '[merge-wise][ERROR][%s] Failed to create ready file !'%(job_spec_conf.job_name)
        content = '[ERROR] [%s] Failed to create ready file (%s),error_code = %s(%s)'%(job_spec_conf.job_name,output_ready,code,msg)
        send_mail(global_conf.mail_alarm,title,content,global_conf.mail_receiver_in)
        return False
    if result_dict['miss_num'] > 0:
        miss_source_info = ','.join([i.strip().split('/')[-3] for i in result_dict['miss_list']])
        logger.info('[%s] job finished at the end without sources(%s)' %(job_spec_conf.job_name,miss_source_info))
        title = '[merge-wise][WARNING][%s] Job finished at the end without sources(%s)' %(job_spec_conf.job_name,miss_source_info)
        content = '[WARNING][%s] Job finished at the end without sources(%s)\n\nmiss_info = %s\n\nready_info = %s' %(job_spec_conf.job_name,miss_source_info,repr(result_dict['miss_list']),repr(result_dict['ready_list']))
        send_mail(global_conf.mail_alarm,title,content,global_conf.mail_receiver_out)
    if options.signal_zk:
        cmd_out = os.system("%s cr -register  %s -cronID %s" % (global_conf.hadoop_client, job_spec_conf.output, job_spec_conf.job_name))
        if cmd_out != 0:
            title = '[merge-wise][ERROR][%s] Failed to reigster zk data (%s)!'%(job_spec_conf.job_name,job_spec_conf.output)
            content = '[ERROR][%s] Failed to reigster zk data (%s) ! error_code = %s '%(job_spec_conf.job_name, job_spec_conf.output ,str(cmd_out))
            send_mail(global_conf.mail_alarm,title,content,global_conf.mail_receiver_in)
            logger.error('[%s] job Failled, reigster zk data (%s) error, error_code = %s' %(job_spec_conf.job_name, job_spec_conf.output,str(cmd_out)))
            return False
    logger.info('[%s] job finish.' % (job_spec_conf.job_name))
    return True
 
def checkTask(job_spec_conf, date, options):
    # check whether job needs to run or not
    output_ready = job_spec_conf.output_ready
    if options.rerun or options.run_force:
        return True
    #if options.fix_data and checkMissTask(job_spec_conf, date):
    #    return True
    if checkHadoopFile(output_ready, date, global_conf):
        logger.info('check task %s : exist, %s' %(output_ready,job_spec_conf.job_name))
        return False
    else:
        logger.info('check task %s : not exist, %s' %(output_ready,job_spec_conf.job_name))
        return True
 
def tryRunTask(job_spec_conf, date, options):
    logger.info('try to run task %s of %s' % (job_spec_conf.job_name, date))
    for i in range(options.retry):
        if runOneTask(job_spec_conf, date, options):
            return
        if i == (options.retry - 1):
            break
        logger.error('Task %s on %s failed, try again ...' % (job_spec_conf.job_name, date))
        time.sleep(options.retry_sleep)
        cleanData(job_spec_conf, date, False)
    logger.error('Task %s on %s failed ...' % (job_spec_conf.job_name, date))
     
#main define here:
def main():
    global options
    global logger
    global global_conf
 
    usage = "usage: %start.py [options] [tasks]"
    parser = OptionParser(usage=usage)
    parser.add_option("-f", "--disable_timeout",
                      action = "store_false", dest = "enable_timeout", default = True,
                      help = "disable job timeout")
    parser.add_option("-d", "--date", dest = "date", default = getPreDate(), help = "last day for job")
    parser.add_option("-n", "--total_day", type = "int", dest = "total_day", default = 1, help = "total day for job")
    parser.add_option("-s", "--sequence_run", action = "store_false", dest = "parallel", default = True, help = "run task one by one")
    parser.add_option("-x", "--fix_data", action = "store_true", dest = "fix_data", default = False, help = "fix data")
    parser.add_option("-p", "--pipeline", dest = "pipeline", default = "start", help = "pipeline name")
    parser.add_option("-r", "--retry", type = "int", dest = "retry", default = 1, help = "job retry times")
    parser.add_option("-R", "--rerun", action = "store_true", dest = "rerun", default = False, help = "rerun tasks")
    parser.add_option("-w", "--retry_sleep", type = "int", dest = "retry_sleep", default = 300, help = "job retry sleep time, in seconds")
    parser.add_option("-a", "--run_force", action = "store_true", dest = "run_force", default = False, help = "run job no matter the job has run already or data are not all ready")
    parser.add_option("-c", "--config_file", dest = "config_file", default = "conf/job_conf.ini", help = "configure file path")
    parser.add_option("-K", "--kill", action = "store_true", dest = "kill_task", default = False, help = "Kill tasks")
    parser.add_option("-S", "--signal", action = "store_true", dest = "signal_zk", default = False, help = "Signal data reay in zk")
 
    (options, tasks) = parser.parse_args()
    logger = initlog(options.date, options.pipeline)
    global_conf = globalConfLoad(options.config_file)
    logger.info('=================================================')
    logger.info('options=%s tasks=%s' %(repr(options),repr(tasks)))
    #logger.info('options=%s tasks=%s' %(repr(options),repr(tasks)))
    # get previous n days list before date, then start it
    for i in xrange(options.total_day):
        date_i = getPreDate(n=i,date=options.date)
        startMultiThead(date_i, tasks, options)
    logger.info('Finish. Time to quit soon ...')
#main start:
if __name__ == '__main__':
    # call build.sh to update local directory
    os.system('sh build.sh')
    now = getPreDate( n = 0 )
    main()
 
# */* vim: set expandtab ts=4 sw=4 sts=4 tw=400: */
```

- 【2021-1-26】其它调度框架，[Python 实现并发Pipeline](https://zhuanlan.zhihu.com/p/260175181)
    - 对于Pipeline 根据侧重点的不同，有两种实现方式
    1. 用于加速多线程任务的pipeline
        - 用于加速多线程任务的 Pipeline 主要强调 任务的顺序执行， 转移之间不涉及过于复杂的逻辑。所以 每个pipe 通过自身调用 next pipe。整体上强调 后向连续性。
    2. 用于控制流程的pipeline
        - 用于流程控制的piepline， 强调任务的 逻辑性， 由外部 manager 来控制 pipeline 的执行方向。整体上强调前向的依赖性， 使用拓扑排序确定执行顺序。

# 面向对象

面向对象三大特点：封装、继承和多态

## 接口实现

接口的特点如下：
- 1、类通过继承接口的方式，来继承接口的抽象方法；
- 2、接口并不是类（虽然编写类和方法的方式很相似）；
- 3、类描述对象的属性和方法（实现接口的类，必须实现接口内所描述的所有方法，否则必须声明为抽象类）；
- 4、接口包含类要实现的方法（接口无法被实例化，但可以被实现）；

总结：接口只定义规范，不负责具体实现（具体实现由具体的实现者完成）

python中也有interface的概念，但是python其本身不提供interface的实现，需要通过第三方扩展库来使用类似interface的功能，一般都是Zope.interface
- Python面向对象编程之Zope.interface安装使用（ @implementer）implements

```python
# coding=utf-8
from zope.interface import Interface
from zope.interface.declarations import implementer
 
# 定义接口
class MyMiss(Interface):
    def imissyouatlost(self,miss):
        """Say i miss you at lost to miss"""
 
@implementer(MyMiss) # 继承接口
class Miss:
    def imissyouatlost(self,somebody):
        """Say i miss you at lost to somebody"""
        return "i miss you at lost, %s!" % somebody
 
if __name__ == '__main__':
    z = Miss()
    hi = z.imissyouatlost('Zy')
    print(hi)

```


# Python生态系统


## Numpy

- 【2020-12-28】[NumPy初学教程，可视化指南](https://www.toutiao.com/i6911204024628806148/)
  - ![](https://p3-tt.byteimg.com/origin/pgc-image/21a2426ec0304256950e4f80596b2b4a?from=pc)
  - ![](https://p3-tt.byteimg.com/origin/pgc-image/cc4fbb6975bf4799b413cb5ec9c694f3?from=pc)

- Numpy知识点汇总

![](https://raw.githubusercontent.com/woaielf/woaielf.github.io/master/_posts/Pic/1704/170415-1.png)

![](https://raw.githubusercontent.com/woaielf/woaielf.github.io/master/_posts/Pic/1704/170415-2.png)

![](https://raw.githubusercontent.com/woaielf/woaielf.github.io/master/_posts/Pic/1704/170415-3.png)

Reference
> 《Python for data analysis》 <br>
[NumPy Reference — NumPy v1.12 Manual](https://docs.scipy.org/doc/numpy/reference/?v=20170515184114)

### 基本操作

```python
import numpy as np
a = np.arange(9).reshape(3,3)
b = a.view() # 视图方法创造一个新的数组对象指向同一数据
print 'a=\n', a
print '生成同型矩阵：\n', np.zeros_like(a)
np.empty_like(a), np.ones_like(a)
print 'a形状参数：', len(a), a.size, a.shape

b = [[2,4,1],[1,3,2]]
print np.reshape(b, (3,2))

c = (a==3) | (a==5)
print '数值筛选:下标\n{}\n取出的数值{}'.format(c, a[c])
a[2,1:] = 0
print '分块赋值后的a=\n{}'.format(a)
print 'np.all(a)={}, np.any(a)={}'.format(np.all(a), np.any(a))

a = np.arange(9) # 一维数组
a = a.reshape(3,3) # 一维变二维 (9,) -> (3,3)
print 'a=', a
print 'double:\n', a*2 # 逐个元素处理
a1 = np.repeat(a, 2) # 元素重复几遍（列向）
print 'repeat用法:(2)\n', a1
a1 = np.repeat(a, 2, axis=0) # 行向重复
print 'repeat用法:(2, axis=0)\n', a1
#print(np.repeat(a, (2, 1), axis=0))
a2 = np.tile(a, 2) # 以对象为单位重复
print 'tile用法:(2)\n', a2
a2 = np.tile(a, (2,2)) # 对a重复2*2
print 'tile用法:(2,2)\n', a2
a = np.squeeze(a)  # 从数组的形状中删除单维条目，即把shape中为1的维度去掉
```

### 拼接

Python中numpy数组的合并有很多方法，如
- list.append(): list方法，占用内存大
- np.concatenate()：不会占用太多内存
- np.stack()
- np.hstack()：水平拼接
- np.vstack()：垂直拼接
- np.dstack()
其中最泛用的是第一个和第二个。第一个可读性好，比较灵活，但是占内存大。第二个则没有内存占用大的问题

复制操作:
- np.repeat：复制的是多维数组的每一个元素；axis来控制复制的行和列
- np.tile：复制的是多维数组本身；

```python
import numpy as np

a = np.arange(9) # 一维数组
a = a.reshape(3,3) # 一维变二维 (9,) -> (3,3)
print 'a=', a
print 'double:', a*2 # 逐个元素处理
a1 = np.repeat(a, 2) # 元素重复几遍
print 'repeat用法:', a1
a2 = np.tile(a, 2) # 矩阵重复
print 'tile用法:', a2
a = np.squeeze(a)  # 从数组的形状中删除单维条目，即把shape中为1的维度去掉
a = np.arange(9).reshape(3,3)
b = a * 2

print '水平拼接:', np.hstack((a,b)) # 水平拼接
print '水平拼接:', np.concatenate((a,b),axis=1) # 水平拼接
print '垂直拼接:', np.concatenate((a,b),axis=0) # 水平拼接
print '垂直拼接:', np.vstack((a,b))
print '纵轴拼接:', np.dstack((a,b))
#print '列组合:', column_stack(a) # 一维场景，二维同hstack
#print '行组合:', row_stack(a) # 一维场景，二维同vstack
```

### 代数&矩阵

```python
import numpy as np

a=[[400,-201],[-800,401]]
rank = np.linalg.matrix_rank(a) # 秩
norm = np.linalg.norm(a) # 范数
det = np.linalg.det(a) # 特征值 -399.99999
inv = np.linalg.inv(a) # 求逆
pinv = np.linalg.pinv(a) # 求伪逆
value, vector = np.linalg.eig(a) # 特征值和特征向量
value = np.linalg.eigvals(a) # 特征值
cond = np.linalg.cond(a) # 条件数 2503,病态
qr = np.linalg.qr(a) # 	QR分解: 正交矩阵*上三角
svd = np.linalg.svd(a) #  SVD分解
```


### 文件处理

Numpy提供了几种数据保存的方法
- 二进制：只能保存为二进制文件，且不能保存当前数据的行列信息
  - tofile与fromfile
  - save与load：Numpy专用的二进制格式保存数据，它们会自动处理元素类型和形状等信息。savez()提供了将多个数组存储至一个文件的能力，调用load()方法返回的对象，可以使用数组名对各个数组进行读取。这种方法，保存文件的后缀名字一定会被置为.npy
- 文本格式：
  - savetxt与loadtxt

```python
#------二进制-------
a.tofile("filename.bin")
 b = numpy.fromfile("filename.bin",dtype = **)
np.save("a.npy", a.reshape(3,4))
c = np.load("a.npy")
# 存储多个数组到一个文件
a = np.array([[1,2,3],[4,5,6]])
b = np.arange(0,1.0,0.1)
c = np.sin(b)
np.savez("result.npz", a, b, sin_arr=c)  #使用sin_arr命名数组c
r = np.load("result.npz") #加载一次即可
#------文本-------
numpy.savetxt("filename.txt",a)
b =  numpy.loadtxt("filename.txt")
```



## Pandas

- Pandas 提供了数据结构——DataFrame，可以高效的处理一些数据分析任务。我们日常分析的数据，大多是存储在类似 excel 的数据表中，Pandas 可以灵活的按列或行处理数据，几乎是最常用的工具了。

Pandas(Python data analysis)是一个Python数据分析的开源库。名字源于panel data（计量经济学术语，面板数据）和python data analysis

pandas三种数据结构：
- Series（一维）
- DataFrame（二维）
- Panel（三维）

- [Pandas按行按列遍历Dataframe的几种方式](https://blog.csdn.net/sinat_29675423/article/details/87972498)
    - ![](https://img-blog.csdnimg.cn/20190227142817847.png)
    - 简单对上面三种方法进行说明：
        - iterrows(): 按行遍历，将DataFrame的每一行迭代为(index, Series)对，可以通过row[name]对元素进行访问。
        - itertuples(): 按行遍历，将DataFrame的每一行迭代为元祖，可以通过row[name]对元素进行访问，比iterrows()效率高。
        - iteritems():按列遍历，将DataFrame的每一列迭代为(列名, Series)对，可以通过row[index]对元素进行访问。

![](https://raw.githubusercontent.com/woaielf/woaielf.github.io/master/_posts/Pic/1704/170422-1.png)

![](https://raw.githubusercontent.com/woaielf/woaielf.github.io/master/_posts/Pic/1704/170422-2.png)

![](https://raw.githubusercontent.com/woaielf/woaielf.github.io/master/_posts/Pic/1704/170422-3.png)

![](https://raw.githubusercontent.com/woaielf/woaielf.github.io/master/_posts/Pic/1704/170422-4.png)

![](https://raw.githubusercontent.com/woaielf/woaielf.github.io/master/_posts/Pic/1704/170422-5.png)

![](https://raw.githubusercontent.com/woaielf/woaielf.github.io/master/_posts/Pic/1704/170422-6.png)

![](https://raw.githubusercontent.com/woaielf/woaielf.github.io/master/_posts/Pic/1704/170422-7.png)


### 查看数据

代码：

```python
import pandas as pd

#查看、检查数据
df.head(n)# 查看DataFrame对象的前n行
df.tail(n)# 查看DataFrame对象的最后n行
df.shape()# 查看行数和列数
df.info()# 查看索引、数据类型和内存信息
df.describe()# 查看数值型列的汇总统计
s.value_counts(dropna=False)# 查看Series对象的唯一值和计数
df.apply(pd.Series.value_counts)# 查看DataFrame对象中每一列的唯一值和计数

# 数据统计
df.describe()# 查看数据值列的汇总统计
df.mean()# 返回所有列的均值
df.corr()# 返回列与列之间的相关系数
df.count()# 返回每一列中的非空值的个数
df.max()# 返回每一列的最大值
df.min()# 返回每一列的最小值
df.median()# 返回每一列的中位数
df.std()# 返回每一列的标准差

# 查找数据
df = pd.DataFrame({'BoolCol': [1, 2, 3, 3, 4],'attr': [22, 33, 22, 44, 66]}, index=[10,20,30,40,50])
df = pd.DataFrame({'BoolCol': [1, 2, 3, 3, 4],'attr': [22, 33, 22, 44, 66]})
print(df)
a1 = df[(df.BoolCol==3)&(df.attr==22)].index.tolist() # 返回下标
a2 = df[(df.BoolCol==3)&(df.attr==22)].values.tolist() # 返回数值
print(a1,a2)

#df.index # 行序号
df.columns # 列名
df.values # 数据内容
#df['lon'],df['lat'],df[:30] # 按照列名读取数据
#df.ix[:30,:3] # 使用ix、loc或者iloc(按照下标组合)进行行列双向读取，即切片操作
#df.ix[:20,['lon','lat']] # 跨属性组合选取
key_list = ['total_bedrooms','population']
df.loc[:100, key_list] # 同上?
#new = df.iloc[:20,[1,2]]
#new.describe # 基本统计信息
#type(new)
#df[df.lon>117] # 按照数值过滤筛选
#df[df.time<'2016-07-20']
#new.values.tolist() # DataFrame转成list结构
#[2018-11-2]另外一种方法：np.array(new).tolist()
#df.sort(columns='time') # 排序
#[2018-10-12] 将pandas元素提取(numpy结构)并转化为list结构
for idx in xrange(df.size):
    item = df.ix[idx,key_list].get_values().tolist()
#提取某个取值的数据，注意：==，不是=！
df[df.businesstype==7]
```

### 数据选择

代码：

```python
#数据选取
df[col]# 根据列名，并以Series的形式返回列
df[[col1, col2]]# 以DataFrame形式返回多列
s.iloc[0]# 按位置选取数据
s.loc['index_one']# 按索引选取数据
df.iloc[0,:]# 返回第一行
df.iloc[0,0]# 返回第一列的第一个元素
# 多个条件通过&、|连接
student[(student['Sex']=='F') & (student['Age']>12)]
# Pandas实现where filter
df[df['sex'] == 'Female']
df[df['total_bill'] > 20]
#在where子句中常常会搭配and, or, in, not关键词，Pandas中也有对应的实现
df[(df['sex'] == 'Female') & (df['total_bill'] > 20)] # and
df[(df['sex'] == 'Female') | (df['total_bill'] > 20)] # or
df[df['total_bill'].isin([21.01, 23.68, 24.59])] # in
df[-(df['sex'] == 'Male')] # not
df[-df['total_bill'].isin([21.01, 23.68, 24.59])]
# string function 使用str函数
df = df[(-df['sex'].isin(['male', 'female'])) & (-df.sex.str.contains('^m\d+$'))]
# 对where条件筛选后只有一行的dataframe取其中某一列的值，其两种实现方式如下：
total = df.loc[df['tip'] == 1.66, 'total_bill'].values[0]
total = df.get_value(df.loc[df['tip'] == 1.66].index.values[0], 'total_bill')
# 像sql一样操作
df.query('total_bill > 20')
```

### 数据清理

代码

```python
# 数据清理
df.columns = ['a','b','c']# 重命名列名
pd.isnull()# 检查DataFrame对象中的空值，并返回一个Boolean数组
pd.notnull()# 检查DataFrame对象中的非空值，并返回一个Boolean数组
df.drop([16,17]) # 删除16，17行,得到新的dataframe, df仍然保留
df.drop(inex=['a', 'b']) # 或者这样
df.drop([16,17], inplace=True) # 删除16，17行, 原地操作
df.dropna()# 删除所有包含空值的行
df.dropna(axis=1)# 删除所有包含空值的列
df.drop(['a', 'b'], axis=1) # 删除列a,b
df.drop(columns=['a', 'b']) # 或者这样
df.dropna(axis=1,thresh=n)# 删除所有小于n个非空值的行
df.fillna(x)# 用x替换DataFrame对象中所有的空值
s.astype(float)# 将Series中的数据类型更改为float类型
s.replace(1,'one')# 用‘one’代替所有等于1的值
s.replace([1,3],['one','three'])# 用'one'代替1，用'three'代替3
#Series对象值替换
s = df.iloc[2]#获取行索引为2数据
#单值替换
s.replace('?',np.nan)#用np.nan替换？
s.replace({'?':'NA'})#用NA替换？
#多值替换
s.replace(['?',r'$'],[np.nan,'NA'])#列表值替换
s.replace({'?':np.nan,'$':'NA'})#字典映射
#同缺失值填充方法类似
s.replace(['?','$'],method='pad')#向前填充
s.replace(['?','$'],method='ffill')#向前填充
s.replace(['?','$'],method='bfill')#向后填充
#limit参数控制填充次数
s.replace(['?','$'],method='bfill',limit=1)
#DataFrame对象值替换
#单值替换
df.replace('?',np.nan)#用np.nan替换？
df.replace({'?':'NA'})#用NA替换？
#按列指定单值替换
df.replace({'EMPNO':'?'},np.nan)#用np.nan替换EMPNO列中?
df.replace({'EMPNO':'?','ENAME':'.'},np.nan)#用np.nan替换EMPNO列中?和ENAME中.
#多值替换
df.replace(['?','.','$'],[np.nan,'NA','None'])##用np.nan替换？用NA替换. 用None替换$
df.replace({'?':'NA','$':None})#用NA替换？ 用None替换$
df.replace({'?','$'},{'NA',None})#用NA替换？ 用None替换$
#正则替换
df.replace(r'\?|\.|\$',np.nan,regex=True)#用np.nan替换？或.或$原字符
df.replace([r'\?',r'\$'],np.nan,regex=True)#用np.nan替换？和$
df.replace([r'\?',r'\$'],[np.nan,'NA'],regex=True)#用np.nan替换？用NA替换$符号
df.replace(regex={r'\?':None})
#value参数显示传递
df.replace(regex=[r'\?|\.|\$'],value=np.nan)#用np.nan替换？或.或$原字符
# [2019-08-29]
d = pd.DataFrame({'BoolCol': [1, 2, 3, 3, 4],'attr': ['a.,b', 'hello', 'world', ',', '4,5']})
d.replace({'attr':','}, '->') # 完全匹配
d.replace(r',', '->',regex=True) # 正则替换
d.replace(r',', '->',regex=True, inplace=True) # 修改原值
d[(df_2.ocr.isnull) & (d.ocr.str.contains(','))] # 检测是否包含,

#原文链接：https://blog.csdn.net/kancy110/article/details/72719340
df.rename(columns=lambda x: x + 1)# 批量更改列名
df.rename(columns={'old_name': 'new_ name'})# 选择性更改列名
df.set_index('column_one')# 更改索引列
df.rename(index=lambda x: x + 1)# 批量重命名索引


# 数据处理：Filter、Sort和GroupBy
df[df[col] > 0.5]# 选择col列的值大于0.5的行
df.sort_values(col1)# 按照列col1排序数据，默认升序排列
df.sort_values(col2, ascending=False)# 按照列col1降序排列数据
df.sort_values([col1,col2], ascending=[True,False])# 先按列col1升序排列，后按col2降序排列数据
df.groupby(col)# 返回一个按列col进行分组的Groupby对象
df.groupby([col1,col2])# 返回一个按多列进行分组的Groupby对象
df.groupby(col1)[col2]# 返回按列col1进行分组后，列col2的均值
df.pivot_table(index=col1, values=[col2,col3], aggfunc=max)# 创建一个按列col1进行分组，并计算col2和col3的最大值的数据透视表
df.groupby(col1).agg(np.mean)# 返回按列col1分组的所有列的均值
data.apply(np.mean)# 对DataFrame中的每一列应用函数np.mean
data.apply(np.max,axis=1)# 对DataFrame中的每一行应用函数np.max

# 数据过滤
import pandas as pd 
data_file = 'name.csv'
df = pd.read_csv(data_file)
# 编码转换 gbk -> utf8
df['path'] = df['path_name'].apply(lambda x:x.decode('gbk').encode('utf8'))
#print df['path_name'][3].decode('gbk').encode('utf8')
df

# 数据合并
df1.append(df2)# 将df2中的行添加到df1的尾部
df.concat([df1, df2],axis=1)# 将df2中的列添加到df1的尾部
df1.join(df2,on=col1,how='inner')# 对df1的列和df2的列执行SQL形式的join
```

### 合并

- [pandas-数据的合并与拼接](https://www.cnblogs.com/keye/p/10791705.html)
- Pandas包的merge、join、concat方法可以完成数据的合并和拼接
    - **merge**方法主要基于两个dataframe的共同列进行合并
    - **join**方法主要基于两个dataframe的索引进行合并
    - **concat**方法是对series或dataframe进行行拼接或列拼接。

- (1) pandas的merge方法是基于共同列，将两个dataframe连接起来。
- merge方法的主要参数：
    - left/right：左/右位置的dataframe。
    - how：数据合并的方式。
        - left：基于左dataframe列的数据合并；
        - right：基于右dataframe列的数据合并；
        - outer：基于列的数据外合并（取并集）；
        - inner：基于列的数据内合并（取交集）；默认为'inner'。
    - on：用来合并的列名，这个参数需要保证两个dataframe有相同的列名。
    - left_on/right_on：左/右dataframe合并的列名，也可为索引，数组和列表。
    - left_index/right_index：是否以index作为数据合并的列名，True表示是。
    - sort：根据dataframe合并的keys排序，默认是。
    - suffixes：若有相同列且该列没有作为合并的列，可通过suffixes设置该列的后缀名，一般为元组和列表类型。
- 示例：
    - 内连接：
        - ![](https://img2018.cnblogs.com/blog/1235684/201904/1235684-20190429170030512-309759819.png)
    - 外链接
        - ![](https://img2018.cnblogs.com/blog/1235684/201904/1235684-20190429170055862-1927816021.png)
    - index和column的内连接方法
        - ![](https://img2018.cnblogs.com/blog/1235684/201904/1235684-20190429170503579-1916082061.png)



```python
# 单列的内连接
# 定义df1
import pandas as pd
import numpy as np

df1 = pd.DataFrame({'alpha':['A','B','B','C','D','E'],'feature1':[1,1,2,3,3,1],
            'feature2':['low','medium','medium','high','low','high']})
# 定义df2
df2 = pd.DataFrame({'alpha':['A','A','B','F'],'pazham':['apple','orange','pine','pear'],
            'kilo':['high','low','high','medium'],'price':np.array([5,6,5,7])})
# print(df1)
# print(df2)
# 基于共同列alpha的内连接
df3 = pd.merge(df1,df2,how='inner',on='alpha')
# 基于共同列alpha的内连接,若两个dataframe间除了on设置的连接列外并无相同列，则该列的值置为NaN。
df4 = pd.merge(df1,df2,how='outer',on='alpha')
# 基于共同列alpha的左连接
df5 = pd.merge(df1,df2,how='left',on='alpha')
# 基于共同列alpha的右连接
df6 = pd.merge(df1,df2,how='right',on='alpha')
# 基于共同列alpha和beta的内连接
df7 = pd.merge(df1,df2,on=['alpha','beta'],how='inner')
# 基于共同列alpha和beta的右连接
df8 = pd.merge(df1,df2,on=['alpha','beta'],how='right')
# 基于df1的beta列和df2的index连接
df9 = pd.merge(df1,df2,how='inner',left_on='beta',right_index=True)
# 基于df1的alpha列和df2的index内连接(修改相同列的后缀名)
df9 = pd.merge(df1,df2,how='inner',left_on='beta',right_index=True,suffixes=('_df1','_df2'))
df3
```

- (2) join方法
    - join方法是基于index连接dataframe，merge方法是基于column连接，连接方法有内连接，外连接，左连接和右连接，与merge一致。 
    - join和merge的连接方法类似，这里就不展开join方法了，建议用merge方法。
- 示例
    - ![](https://img2018.cnblogs.com/blog/1235684/201904/1235684-20190429170956399-607825585.png)

```python
caller = pd.DataFrame({'key': ['K0', 'K1', 'K2', 'K3', 'K4', 'K5'], 'A': ['A0', 'A1', 'A2', 'A3', 'A4', 'A5']})
other = pd.DataFrame({'key': ['K0', 'K1', 'K2'],'B': ['B0', 'B1', 'B2']})
print(caller)
print(other)# lsuffix和rsuffix设置连接的后缀名
# 基于index连接
caller.join(other,lsuffix='_caller', rsuffix='_other',how='inner')
# 基于key列进行连接
caller.set_index('key').join(other.set_index('key'),how='inner')
```

- (3) concat方法
    - concat方法是拼接函数，有行拼接和列拼接，默认是行拼接，拼接方法默认是外拼接（并集），拼接的对象是pandas数据类型。 
- 示例

```python
df1 = pd.Series([1.1,2.2,3.3],index=['i1','i2','i3'])
df2 = pd.Series([4.4,5.5,6.6],index=['i2','i3','i4'])
print(df1)
print(df2)

# 行拼接
pd.concat([df1,df2])
# 对行拼接分组,行拼接若有相同的索引，为了区分索引，我们在最外层定义了索引的分组情况。
pd.concat([df1,df2],keys=['fea1','fea2'])
# 列拼接,默认是并集
pd.concat([df1,df2],axis=1)
# 列拼接的内连接（交）
pd.concat([df1,df2],axis=1,join='inner')
# 列拼接的内连接（交）
pd.concat([df1,df2],axis=1,join='inner',keys=['fea1','fea2'])
# 指定索引[i1,i2,i3]的列拼接
pd.concat([df1,df2],axis=1,join_axes=[['i1','i2','i3']])

df1 = pd.DataFrame({'key': ['K0', 'K1', 'K2', 'K3', 'K4', 'K5'], 'A': ['A0', 'A1', 'A2', 'A3', 'A4', 'A5']})
df2 = pd.DataFrame({'key': ['K0', 'K1', 'K2'],'B': ['B0', 'B1', 'B2']})
print(df1)
print(df2)

# 行拼接
pd.concat([df1,df2])
# 列拼接
pd.concat([df1,df2],axis=1)
# 判断是否有重复的列名，若有则报错
pd.concat([df1,df2],axis=1,verify_integrity = True)
```


Reference
> 《Python for data analysis》<br>
[pandas: powerful Python data analysis toolkit — pandas 0.20.1 documentation](http://pandas.pydata.org/pandas-docs/stable/?v=20170515184114)

### 遍历

  【2020-12-25】遍历dataframe
- iterrows：最慢
- itertuples
- zip：最快

```python
df = pd.DataFrame({'a': range(0, 10000), 'b': range(10000, 20000)})
import time
list1 = []
start = time.time()
for i,r in df.iterrows():
    list1.append((r['a'], r['b']))
print("iterrows耗时  :",time.time()-start)

list1 = []
start = time.time()
for ir in df.itertuples():
    list1.append((ir[1], ir[2]))    
print("itertuples耗时:",time.time()-start)

list1 = []
start = time.time()
for r in zip(df['a'], df['b']):
    list1.append((r[0], r[1]))
print("zip耗时       :",time.time()-start)
```

- 结果：
    - iterrows耗时  : 0.7355637550354004
    - itertuples耗时: 0.008462905883789062
    - zip耗时       : 0.003980875015258789

### 多表合并

- 【2020-12-31】一次合并多张表

```python
import pandas as pd
from functools import reduce
 
data_dir = '/home/work/data/北链讲盘培训价值分析'
#data_file = '/home/work/data/北链讲盘过关用户业绩样例.csv'
bu = ['京北', '京东南']
group = ['未参加', '低分', '高分']
df_dict = {}
for b in bu:
    df_dict[b] = {}
    for g in group:
        data_file = '{}/{}-{}.csv'.format(data_dir, b, g)
        print('开始读数据：{}'.format(data_file))
        df_dict[b][g] = pd.read_csv(data_file, encoding='gbk')
        df_dict[b][g] = df_dict[b][g].rename(columns={'stat_date':'日期', 'avg(assign_amt)':'培训{}的经纪人日均业绩'.format(g)})
        df_final.loc[:,'alpha'].apply(lambda x:'{}-hh'.format(x))
        df_dict[b][g]['日期'] = df_dict[b][g]['日期'].apply(lambda x: '{}-{}-{}'.format(str(x)[:4],str(x)[4:6],str(x)[6:8]))
        # .dt.dayofweek
        df_dict[b][g]['星期几'] = pd.to_datetime(df_dict[b][g]['日期']).dt.dayofweek
    #df_dict[b]['合并'] = df.merge(df_dict[b]['未参加'], df_dict[b]['低分'], df_dict[b]['高分'], how='inner', on='日期')
    merge_list = [df_dict[b]['未参加'], df_dict[b]['低分'], df_dict[b]['高分']]
    df_dict[b]['合并'] = reduce(lambda left,right: pd.merge(left,right,on='日期',how='outer'), merge_list).fillna(0)
    df_dict[b]['合并'].to_excel('{}/{}_合并后.xlsx'.format(data_dir, b))
 
#df = pd.read_csv(data_file, encoding='gbk')
#!head $data_file
#df3 = pd.merge(df1,df2,how='inner',on='alpha')
df_dict['京北']['合并']
```

### 日期转换

- [pandas字符串转日期处理方式](https://blog.csdn.net/weixin_41685388/article/details/103860881)
- 代码

```python

#提取年月日时分秒：方法1
df = pd.read_csv(r"spider.csv",header=None,names=['datetime','url','name','x','y'],encoding='utf-8')
df['datetime'] = pd.to_datetime(df['datetime'],errors='coerce')   #先转化为datetime类型,默认format='%Y-%m-%d %H:%M:%S'
df['date'] = df['datetime'].dt.date   #转化提取年-月-日
df['year'] =df['datetime'].dt.year.fillna(0).astype("int")   #转化提取年 ,
#如果有NaN元素则默认转化float64型，要转换数据类型则需要先填充空值,在做数据类型转换
df['month'] = df['datetime'].dt.month.fillna(0).astype("int")  #转化提取月
df['%Y_%m'] = df['year'].map(str) + '-' + df['month'].map(str) #转化获取年-月
df['day'] = df['datetime'].dt.day.fillna(0).astype("int")      #转化提取天
df['hour'] = df['datetime'].dt.hour.fillna(0).astype("int")    #转化提取小时
df['minute'] = df['datetime'].dt.minute.fillna(0).astype("int") #转化提取分钟
df['second'] = df['datetime'].dt.second.fillna(0).astype("int") #转化提取秒
df['dayofyear'] = df['datetime'].dt.dayofyear.fillna(0).astype("int") #一年中的第n天
df['weekofyear'] = df['datetime'].dt.weekofyear.fillna(0).astype("int") #一年中的第n周
df['weekday'] = df['datetime'].dt.weekday.fillna(0).astype("int") #周几，一周里的第几天，Monday=0, Sunday=6
df['quarter'] = df['datetime'].dt.quarter.fillna(0).astype("int")  #季度
display(df.head())
```

### 排序

数据排序sort_index()和sort_values()

**sort_values**()
- 作用：既可以根据列数据，也可根据行数据排序。
- 注意：必须指定by参数，即必须指定哪几行或哪几列；无法根据index名和columns名排序（由.sort_index()执行）

DataFrame.sort_values(by, axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last')
- axis：{0 or ‘index’, 1 or ‘columns’}, default 0，默认按照列排序，即纵向排序；如果为1，则是横向排序。
- by：str or list of str；如果axis=0，那么by="列名"；如果axis=1，那么by="行名"。
- ascending：布尔型，True则升序，如果by=['列名1','列名2']，则该参数可以是[True, False]，即第一字段升序，第二个降序。
- inplace：布尔型，是否用排序后的数据框替换现有的数据框。
- kind：排序方法，{‘quicksort’, ‘mergesort’, ‘heapsort’}, default ‘quicksort’。似乎不用太关心。
- na_position：{‘first’, ‘last’}, default ‘last’，默认缺失值排在最后面。

**sort_index**()
- 作用：默认根据行标签对所有行排序，或根据列标签对所有列排序，或根据指定某列或某几列对行排序。
- 注意：df. sort_index()可以完成和df. sort_values()完全相同的功能，但python更推荐用只用df. sort_index()对“根据行标签”和“根据列标签”排序，其他排序方式用df.sort_values()。

sort_index(axis=0, level=None, ascending=True, inplace=False, kind='quicksort', na_position='last', sort_remaining=True, by=None)
- axis：0按照行名排序；1按照列名排序
- level：默认None，否则按照给定的level顺序排列---貌似并不是，文档
- ascending：默认True升序排列；False降序排列
- inplace：默认False，否则排序之后的数据直接替换原来的数据框
- kind：排序方法，{‘quicksort’, ‘mergesort’, ‘heapsort’}, default ‘quicksort’。似乎不用太关心。
- na_position：缺失值默认排在最后{"first","last"}
- by：按照某一列或几列数据进行排序，但是by参数貌似不建议使用



```python
import pandas as pd  

df = pd.DataFrame({'b':[1,2,2,3],'a':[4,3,2,1],'c':[1,3,8,2]},index=[2,0,1,3]) 

df.sort_values(by='b') #等同于df.sort_values(by='b',axis=0)
df.sort_values(by=[3,0],axis=1,ascending=[True,False])
df.sort_values(by=3,axis=1) #按第三行升序，必须指定axis=1

df.sort_index() #默认按“行标签”升序排序，或df.sort_index(axis=0, ascending=True)
df.sort_index(axis=1) #按“列标签”升序排序
#先按b列“降序”排列，因为b列中有相同值，相同值再按a列的“升序”排列
df.sort_index(by = ['b','a'],ascending = [False,True]) 

```

### 聚合统计

词频统计，类似collections里的Counter

```python
# 对train表里的intent列计算频次，并给出百分比（不用单独计算）
train.intent.value_counts(), train.intent.value_counts(normalize=True)
# 聚合统计
df.groupby('total_bedrooms')['population'].agg(['count', 'sum', 'mean'])[:10]
# pandas groupby常用功能
df.groupby(['dt','msg']).size().reset_index().rename(columns={0:"count"})
df.rename(columns={0:"count"})
df.groupby(['real_cost']).size() # 分组,频次计算,size()计算数目
df.groupby(['real_cost']).size() # 分组,频次计算,count()不含Nan值
df.groupby(['real_cost']).size().loc[10] # 查询某个分组下的聚合值
df.groupby(['real_cost']).get_group((10)) # 查询某个分组下所有记录
df.groupby(['real_cost'])['queuecount'].idxmin() # 各个分组下某个取值(queuecount)最小/大的记录数下标,idxmin,idxmax
df.groupby(['real_cost']).size().apply(lambda x:2*x).head() # apply,对统计值的二次处理
df.groupby(['real_cost']).size().reset_index()# reset_index重置index
# 用户当天第几次搜索某个brand_id
gp = data.groupby(["user_id","day","brand_id"])["hour"].rank().reset_index() 
gp.rename(columns={"hour":"rank"},inplace=True)
#agg 调用的时候要指定字段，apply 默认传入的是整个dataframe,transform 是针对输入的元素级别转换
df[['customer_type','real_cost']].agg(['sum','mean','min','max','median']) # 多个属性的统计信息
tmp = lambda x:x.sort_values(ascending = False).iloc[0]
tmp.__name__ = 'func'
df[['customer_type','real_cost']].agg({'customer_type':['sum','mean'],'real_cost':['sum',tmp]}) # 多个属性的统计信息（统计值不同,自定义聚合函数）
df.groupby(['real_cost']).transform(('sum')) # transform元素级别的变换
df['real_cost'].applymap(tmp) # 元素级别变换
```

### 采样

pandas随机采样，用于数据集划分;

```python
# 随机采样
df = pd.DataFrame([[1,'a'],[2,'b'],[3,'c'],[4,'d']], columns=['id','name'])
# DataFrame.sample(n=None, frac=None, replace=False, weights=None, random_state=None, axis=None)
#df.sample(frac=1).reset_index(drop=True)
df.sample(frac=1) # 随机打乱,100%乱序，frac是比例，axis行还是列向，replace是否放回，weights字符索引或概率数组
df.sample(n=3,random_state=1) # 抽3行，可重复数据
df.sample(frac=0.8, replace=True, random_state=1) # replace可重复
```

### 文件操作

```python
# pandas读取excel数据示例
# 【2016-7-30】 参考：十分钟搞定pandas
import pandas as pd
import numpy as np
# 读取数据 D:\work\用户建模画像\家公司挖掘\code\warren.xls
# 数据格式：time
print 'start'
df = pd.read_excel('C:\Users\warren\Desktop\warren.xlsx',index='time')
#[2018-4-11] 先安装!pip install xlrd才可以
df = pd.read_excel(open('your_xls_xlsx_filename','rb'), sheetname='Sheet 1')

df = pd.read_table('data/raw_tmp_new.txt',header=None,names=['a','b','c'])
data_file = 'D:/project/python_instruct/test_data1.csv'
df=pd.read_csv(data_file)#用read_csv读取的csv文件
df=pd.read_table(data_file, sep=',')#用read_table读取csv文件
df=pd.read_csv(data_file, header=None)#用read_csv读取无标题行的csv文件
df=pd.read_csv(data_file, names=['a', 'b', 'c', 'd', 'message'])#用read_csv读取自定义标题行的csv文件
names=['a', 'b', 'c', 'd', 'message']
df=pd.read_csv(data_file, names=names, index_col='message')#'read_csv读取时指定索引
parsed=pd.read_csv(data_file, index_col=['key1', 'key2'])#read_csv将多个列做成一个层次化索引
print(list(open(data_file)))
result=pd.read_table(data_file, sep='\s+')#read_table利用正则表达式处理文件读取

```

### 可视化

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
a=pd.Series(np.random.randn(1000),index=pd.date_range('20100101',periods=1000))
b=a.cumsum()
b.plot()
df.plot.area() # 面积图
df.plot.bar() # 柱形图
df.plot.barh() # 水平柱形图
df.plot.density() # 密度图
df.plot.kde()
df.plot.hist() # 直方图
df.plot.scatter()
df.plot.pie()

```

## Python技巧

- [30段极简Python代码：这些小技巧你都Get了吗](https://zhuanlan.zhihu.com/p/83998758)

Python 是机器学习最广泛采用的编程语言，它最重要的优势在于编程的易用性。如果读者对基本的 Python 语法已经有一些了解，那么这篇文章可能会给你一些启发。作者简单概览了 30 段代码，它们都是平常非常实用的技巧，我们只要花几分钟就能从头到尾浏览一遍。

### 1. 重复元素判定

以下方法可以检查给定列表是不是存在重复元素，它会使用 set() 函数来移除所有重复元素。

```python
def all_unique(lst):
    return len(lst) == len(set(lst))


x = [1,1,2,2,3,2,3,4,5,6]
y = [1,2,3,4,5]
all_unique(x) # False
all_unique(y) # True
```

### 2. 字符元素组成判定

检查两个字符串的组成元素是不是一样的。

```python
from collections import Counter

def anagram(first, second):
    return Counter(first) == Counter(second)

anagram("abcd3", "3acdb") # True
```

### 3. 内存占用

下面的代码块可以检查变量 variable 所占用的内存。

```python
import sys 

variable = 30 
print(sys.getsizeof(variable)) # 24
```

### 4. 字节占用

下面的代码块可以检查字符串占用的字节数。

```python
def byte_size(string):
    return(len(string.encode('utf-8')))

byte_size(' ') # 4
byte_size('Hello World') # 11  
```

### 5. 打印 N 次字符串

该代码块不需要循环语句就能打印 N 次字符串。

```python
n = 2; 
s ="Programming"; 

print(s * n);
# ProgrammingProgramming  
```

### 6. 大写第一个字母

以下代码块会使用 title() 方法，从而大写字符串中每一个单词的首字母。

```python
s = "programming is awesome"

print(s.title())
# Programming Is Awesome
```

### 7. 分块

给定具体的大小，定义一个函数以按照这个大小切割列表。

```python
from math import ceil

def chunk(lst, size):
    return list(
        map(lambda x: lst[x * size:x * size + size],
            list(range(0, ceil(len(lst) / size)))))

chunk([1,2,3,4,5],2)
# [[1,2],[3,4],5]
```

### 8. 压缩

这个方法可以将布尔型的值去掉，例如（False，None，0，“”），它使用 filter() 函数。

```python
def compact(lst):
    return list(filter(bool, lst))

compact([0, 1, False, 2, '', 3, 'a', 's', 34])
# [ 1, 2, 3, 'a', 's', 34 ]
```

### 9. 解包

如下代码段可以将打包好的成对列表解开成两组不同的元组。

```python
array = [['a', 'b'], ['c', 'd'], ['e', 'f']]
transposed = zip(*array)
print(transposed)
# [('a', 'c', 'e'), ('b', 'd', 'f')]
```

### 10. 链式对比

我们可以在一行代码中使用不同的运算符对比多个不同的元素。

```python
a = 3
print( 2 < a < 8) # True
print(1 == a < 2) # False
```

### 11. 逗号连接

下面的代码可以将列表连接成单个字符串，且每一个元素间的分隔方式设置为了逗号。

```python
hobbies = ["basketball", "football", "swimming"]

print("My hobbies are: " + ", ".join(hobbies))
# My hobbies are: basketball, football, swimming
```

### 12. 元音统计

以下方法将统计字符串中的元音 (‘a’,‘e’,‘i’,‘o’,‘u’) 的个数，它是通过正则表达式做的。

```python
import re

def count_vowels(str):
    return len(len(re.findall(r'[aeiou]', str, re.IGNORECASE)))

count_vowels('foobar') # 3
count_vowels('gym') # 0
```

### 13. 首字母小写

如下方法将令给定字符串的第一个字符统一为小写。

```python
def decapitalize(string):
    return str[:1].lower() + str[1:]

decapitalize('FooBar') # 'fooBar'
decapitalize('FooBar') # 'fooBar'
```

### 14. 展开列表

该方法将通过递归的方式将列表的嵌套展开为单个列表。

```python
def spread(arg):
    ret = []
    for i in arg:
        if isinstance(i, list):
            ret.extend(i)
        else:
            ret.append(i)
    return ret

def deep_flatten(lst):
    result = []
    result.extend(
        spread(list(map(lambda x: deep_flatten(x) if type(x) == list else x, lst))))
    return result

deep_flatten([1, [2], [[3], 4], 5]) # [1,2,3,4,5]
```

### 15. 列表的差

该方法将返回第一个列表的元素，其不在第二个列表内。如果同时要反馈第二个列表独有的元素，还需要加一句 set_b.difference(set_a)。

```python
def difference(a, b):
    set_a = set(a)
    set_b = set(b)
    comparison = set_a.difference(set_b)
    return list(comparison)


difference([1,2,3], [1,2,4]) # [3]
```

### 16. 通过函数取差

如下方法首先会应用一个给定的函数，然后再返回应用函数后结果有差别的列表元素。

```python
def difference_by(a, b, fn):
    b = set(map(fn, b))
    return [item for item in a if fn(item) not in b]


from math import floor
difference_by([2.1, 1.2], [2.3, 3.4],floor) # [1.2]
difference_by([{ 'x': 2 }, { 'x': 1 }], [{ 'x': 1 }], lambda v : v['x'])
# [ { x: 2 } ]
```

### 17. 链式函数调用

你可以在一行代码内调用多个函数。

```python
def add(a, b):
    return a + b

def subtract(a, b):
    return a - b

a, b = 4, 5
print((subtract if a > b else add)(a, b)) # 9 
```

### 18. 检查重复项

如下代码将检查两个列表是不是有重复项。

```python
def has_duplicates(lst):
    return len(lst) != len(set(lst))

x = [1,2,3,4,5,5]
y = [1,2,3,4,5]
has_duplicates(x) # True
has_duplicates(y) # False
```

### 19. 合并两个字典

下面的方法将用于合并两个字典。

```python
def merge_two_dicts(a, b):
    c = a.copy()   # make a copy of a 
    c.update(b)    # modify keys and values of a with the ones from b
    return c

a = { 'x': 1, 'y': 2}
b = { 'y': 3, 'z': 4}
print(merge_two_dicts(a, b))
# {'y': 3, 'x': 1, 'z': 4}
```

在 Python 3.5 或更高版本中，我们也可以用以下方式合并字典：

```python
def merge_dictionaries(a, b)
   return {**a, **b}

a = { 'x': 1, 'y': 2}
b = { 'y': 3, 'z': 4}
print(merge_dictionaries(a, b))
# {'y': 3, 'x': 1, 'z': 4}

```
### 20. 将两个列表转化为字典

如下方法将会把两个列表转化为单个字典。

```python
def to_dictionary(keys, values):
    return dict(zip(keys, values))


keys = ["a", "b", "c"]    
values = [2, 3, 4]
print(to_dictionary(keys, values))
# {'a': 2, 'c': 4, 'b': 3}
```

### 21. 使用枚举

我们常用 For 循环来遍历某个列表，同样我们也能枚举列表的索引与值。

```python
list = ["a", "b", "c", "d"]
for index, element in enumerate(list): 
    print("Value", element, "Index ", index, )

# ('Value', 'a', 'Index ', 0)
# ('Value', 'b', 'Index ', 1)
#('Value', 'c', 'Index ', 2)
# ('Value', 'd', 'Index ', 3)    
```

### 22. 执行时间

如下代码块可以用来计算执行特定代码所花费的时间。

```python
import time

start_time = time.time()

a = 1
b = 2
c = a + b
print(c) #3

end_time = time.time()
total_time = end_time - start_time
print("Time: ", total_time)

# ('Time: ', 1.1205673217773438e-05)  
```

### 23. Try else

我们在使用 try/except 语句的时候也可以加一个 else 子句，如果没有触发错误的话，这个子句就会被运行。

```python
try:
    2*3
except TypeError:
    print("An exception was raised")
else:
    print("Thank God, no exceptions were raised.")

#Thank God, no exceptions were raised.
```

### 24. 元素频率

下面的方法会根据元素频率取列表中最常见的元素。

```python
def most_frequent(list):
    return max(set(list), key = list.count)

list = [1,2,1,2,3,2,1,4,2]
most_frequent(list)  
```

### 25. 回文序列

以下方法会检查给定的字符串是不是回文序列，它首先会把所有字母转化为小写，并移除非英文字母符号。最后，它会对比字符串与反向字符串是否相等，相等则表示为回文序列。

```python
def palindrome(string):
    from re import sub
    s = sub('[\W_]', '', string.lower())
    return s == s[::-1]

palindrome('taco cat') # True
```

### 26. 不使用 if-else 的计算子

这一段代码可以不使用条件语句就实现加减乘除、求幂操作，它通过字典这一数据结构实现：

```python
import operator
action = {
    "+": operator.add,
    "-": operator.sub,
    "/": operator.truediv,
    "*": operator.mul,
    "**": pow
}
print(action['-'](50, 25)) # 25
```

### 27. Shuffle

该算法会打乱列表元素的顺序，它主要会通过 Fisher-Yates 算法对新列表进行排序：

```python
from copy import deepcopy
from random import randint

def shuffle(lst):
    temp_lst = deepcopy(lst)
    m = len(temp_lst)
    while (m):
        m -= 1
        i = randint(0, m)
        temp_lst[m], temp_lst[i] = temp_lst[i], temp_lst[m]
    return temp_lst

foo = [1,2,3]
shuffle(foo) # [2,3,1] , foo = [1,2,3]
```

### 28. 展开列表

将列表内的所有元素，包括子列表，都展开成一个列表。

```python
def spread(arg):
    ret = []
    for i in arg:
        if isinstance(i, list):
            ret.extend(i)
        else:
            ret.append(i)
    return ret

spread([1,2,3,[4,5,6],[7],8,9]) # [1,2,3,4,5,6,7,8,9]
```

### 29. 交换值

不需要额外的操作就能交换两个变量的值。

```python
def swap(a, b):
  return b, a

a, b = -1, 14
swap(a, b) # (14, -1)
spread([1,2,3,[4,5,6],[7],8,9]) # [1,2,3,4,5,6,7,8,9]
```

### 30. 字典默认值

通过 Key 取对应的 Value 值，可以通过以下方式设置默认值。如果 get() 方法没有设置默认值，那么如果遇到不存在的 Key，则会返回 None。

```python
d = {'a': 1, 'b': 2}
print(d.get('c', 3)) # 3
```

# 设计模式


## 单例模式

【2021-7-14】[单例设计模式的python实现](https://www.jianshu.com/p/6a1690f0dd00)

单例模式就是确保**一个类只有一个实例**.当你希望整个系统中,某个类只有一个实例时,单例模式就派上了用场.
- 比如,某个服务器的配置信息存在在一个文件中,客户端通过AppConfig类来读取配置文件的信息.如果程序的运行的过程中,很多地方都会用到配置文件信息,则就需要创建很多的AppConfig实例,这样就导致内存中有很多AppConfig对象的实例,造成资源的浪费.其实这个时候AppConfig我们希望它只有一份,就可以使用单例模式.

### 模块实现

python的模块就是天然的单例模式,因为模块在第一次导入的时候,会生成.pyc文件,当第二次导入的时候,就会直接加载.pyc文件,而不是再次执行模块代码.如果我们把相关的函数和数据定义在一个模块中,就可以获得一个单例对象了

新建一个python模块叫singleton, 文件名 mysingleton.py

```python
class Singleton(object):
    def foo(self):
        pass
singleton = Singleton()

```
调用时：

```python
from singleton.mysingleton import singleton
```


### 装饰器实现

装饰器里面的外层变量定义一个字典,里面存放这个类的实例.当第一次创建的收,就将这个实例保存到这个字典中.
然后以后每次创建对象的时候,都去这个字典中判断一下,如果已经被实例化,就直接取这个实例对象.如果不存在就保存到字典中.

```python
# encoding:utf-8
__author__ = 'Fioman'
__time__ = '2019/3/6 10:22'

def singleton(cls):
    # 单下划线的作用是这个变量只能在当前模块里访问,仅仅是一种提示作用
    # 创建一个字典用来保存类的实例对象
    _instance = {}

    def _singleton(*args, **kwargs):
        # 先判断这个类有没有对象
        if cls not in _instance:
            _instance[cls] = cls(*args, **kwargs)  # 创建一个对象,并保存到字典当中
        # 将实例对象返回
        return _instance[cls]

    return _singleton

@singleton
class A(object):
    a = 1

    def __init__(self, x=0):
        self.x = x
        print('这是A的类的初始化方法')

a1 = A(2)
a2 = A(3)
print(id(a1), id(a2))
```


### 类实现

调用类的instance方法,这样有一个弊端就是在使用类创建的时候,并不是单例了.也就是说在创建类的时候一定要用类里面规定的方法创建

注意：
- 这样的单例模式在单线程下是安全的,但是如果遇到多线程,就会出现问题.如果遇到多个线程同时创建这个类的实例的时候就会出现问题.

```python
# encoding:utf-8
__author__ = 'Fioman'
__time__ = '2019/3/6 11:06'

class Singleton(object):
    def __init__(self,*args,**kwargs):
        pass

    @classmethod
    def get_instance(cls, *args, **kwargs):
        # 利用反射,看看这个类有没有_instance属性
        if not hasattr(Singleton, '_instance'):
            Singleton._instance = Singleton(*args, **kwargs)

        return Singleton._instance

s1 = Singleton()  # 使用这种方式创建实例的时候,并不能保证单例
s2 = Singleton.get_instance()  # 只有使用这种方式创建的时候才可以实现单例
s3 = Singleton()
s4 = Singleton.get_instance()

print(id(s1), id(s2), id(s3), id(s4))
```

多线程安全版本

```python
# encoding:utf-8
__author__ = 'Fioman'
__time__ = '2019/3/6 11:26'
import threading


class Singleton(object):
    def __init__(self, *args, **kwargs):
        time.sleep(1) # init时加阻塞，以便暴露问题
        pass

    @classmethod
    def get_instance(cls, *args, **kwargs):
        if not hasattr(Singleton, '_instance'):
            Singleton._instance = Singleton(*args, **kwargs)

        return Singleton._instance

def task(arg):
    obj = Singleton.get_instance(arg)
    print(obj)

for i in range(10):
    t = threading.Thread(target=task, args=[i, ])
    t.start()

```

结果创建了10个不同的实例对象，因为在一个对象创建的过程中,另外一个对象也创建了.当它判断的时候,会先去获取_instance属性,因为这个时候还没有,它就会调用init()方法.结果就是调用了10次,然后就创建了10个对象.

```python
# encoding:utf-8
__author__ = 'Fioman'
__time__ = '2019/3/6 11:38'

import time
import threading

class Singleton(object):
    _instance_lock = threading.Lock()

    def __init__(self,*args,**kwargs):
        time.sleep(1)

    @classmethod
    def get_instance(cls,*args,**kwargs):
        if not hasattr(Singleton,'_instance'):
            with Singleton._instance_lock:
                if not hasattr(Singleton,'_instance'):
                    Singleton._instance = Singleton(*args,**kwargs)

        return Singleton._instance

def task(arg):
    obj = Singleton.get_instance(arg)
    print(obj)

for i in range(10):
    t = threading.Thread(target=task,args=[i,])
    t.start()

obj = Singleton.get_instance()
print(obj)
```

这种方式创建的单例,必须使用Singleton_get_instance()方法,如果使用Singleton()的话,得到的并不是单例.所以我们推荐使用__new__()方法来创建单例,这样创建的单例可以使用类名()的方法进行实例化对象

### 基于__new__方法实现的单例模式(推荐使用,方便)

知识点:
- 一个对象的实例化过程是先执行类的__new__方法,如果没有写,默认会调用object的__new__方法,返回一个实例化对象,然后再调用__init__方法,对这个对象进行初始化,我们可以根据这个实现单例.
- 在一个类的__new__方法中先判断是不是存在实例,如果存在实例,就直接返回,如果不存在实例就创建.

```python
# encoding:utf-8
__author__ = 'Fioman'
__time__ = '2019/3/6 13:36'
import threading


class Singleton(object):
    _instance_lock = threading.Lock()

    def __init__(self, *args, **kwargs):
        pass

    def __new__(cls, *args, **kwargs):
        if not hasattr(cls, '_instance'):
            with Singleton._instance_lock:
                if not hasattr(cls, '_instance'):
                    Singleton._instance = super().__new__(cls)

            return Singleton._instance

obj1 = Singleton()
obj2 = Singleton()
print(obj1, obj2)

def task(arg):
    obj = Singleton()
    print(obj)

for i in range(10):
    t = threading.Thread(target=task, args=[i, ])
    t.start()
```


# 结束
















