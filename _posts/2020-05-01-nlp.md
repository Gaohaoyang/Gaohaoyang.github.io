---
layout: post
title:  "自然语言处理-NLP"
date:   2020-05-01 21:50:00
categories: 自然语言处理
tags: 深度学习 NLP 自然语言处理 文本摘要 唐杰 指代消解 省略补全 kenlm fasttext 文本分类
excerpt: NLP各类知识点及工程落地方法汇总
author: 鹤啸九天
mathjax: true
---

* content
{:toc}

# 总结

- 【2020-6-11】阿里员工开发的[论文知识图谱](https://www.connectedpapers.com/)
   - ![](http://p1.pstatp.com/large/pgc-image/37054b2db9b64394a73feecfa9ad024d)
- 【2020-9-5】[文本摘要综述](https://github.com/xcfcode/What-I-Have-Read/blob/master/slides/presentation/%E6%96%87%E6%9C%AC%E6%91%98%E8%A6%81%E7%AE%80%E8%BF%B0.pdf)
   - ACL 2020论文：Extractive Summarization as Text Matching，[文本摘要新框架，抽取式摘要“轻松”取得SOTA](https://zhuanlan.zhihu.com/p/133096909)，打破原有的解决抽取式摘要的思路，这里提出了一个全新的范式：将抽取式摘要任务转化为一个语义匹配的问题。代码[MatchSum](https://github.com/maszhongming/MatchSum)，eight Tesla-V100-16G GPUs to train our model, the training time is about 30 hours
   - [Text Summarization on WikiHow](https://paperswithcode.com/sota/text-summarization-on-wikihow)，第一名[代码](https://github.com/nlpyang/PreSumm)
- 【2020-8-9】[NLP 中的各种不正确打开方式](https://www.bilibili.com/video/BV1ha4y1J71Q/)（反向模式 Anti-Pattern），总结的真好。[原文](https://docs.google.com/presentation/d/e/2PACX-1vSINTutKveUSVpj-FwpTz1Kkau7JC7d6ZKmGRqHyWf81Lj7dQkcr9dy9eL7HACD9NhvSH37lMtmqelh/pub?slide=id.g8a66028a04_0_50 )，也推荐作者的[博客](http://medium.com/modern-nlp)​​​​
- 【2021-1-3】Google 的AI团队在2020年底出品的《高性能自然语言处理》，综述了NLP的基础技术、核心技术、案例实践。
   - [EMNLP_2020_Tutorial__High_Performance_NLP](http://gabrielilharco.com/publications/EMNLP_2020_Tutorial__High_Performance_NLP.pdf)
- 【2021-1-23】Python Autocomplete：基于Transformer和LSTM的Python代码自动补全】’[Python Autocomplete](https://github.com/lab-ml/python_autocomplete) - Use Transformers and LSTMs to learn Python source code' by LabML
- 【2021-1-23】拼写纠错工具[autocorrect](https://github.com/fsondej/autocorrect)


## 能力总结

- 【2021-2-27】[中文NLP实话实话](https://mp.weixin.qq.com/s/vsnkFr11i4rde7ixBrsIfg)：中文自然语言处理，目前在AI泡沫之下，真假难辨，实战技术与PPT技术往往存在着很大的差异。目前关于AI或者自然语言处理，做的人与讲的人往往是两回事。
- 深度学习在自然语言处理当中，除了在**分类**问题上能够取得较好效果外（如单选问题：情感分类、文本分类、正确答案分类问题等），在信息抽取上，尤其是在**元组**抽取上基本上是一塌糊涂，在工业场景下很难达到实用水准。
- 目前各种评测集大多是人为标注的，人为标注的大多为干净环境下的较为规范的文本，而且省略了真实生产环节中的多个环节。在评测环节中达到的诸多state-of-art方法，在真实应用场景下泛化能力很差，大多仅仅是为了刷榜而刷榜。
- 目前关于知识图谱的构建环节中，数据大多数都还是来自于结构化数据，半结构化信息抽取次之，非结构化数据抽取最少。半结构化信息抽取，即表格信息抽取最为危险，一个单元格错误很有可能导致所有数据都出现错误。非结构化抽取中，实体识别和实体关系识别难度相当大。
- 工业场景下**命名实体识别**，标配的BILSTM+CRF实际上只是辅助手段，工业界还是以领域实体字典匹配为主，大厂中往往在后者有很大的用户日志，这种日志包括大量的实体信息。因此，生产环节中的实体识别工作中，基础性词性的构建和扩展工作显得尤为重要。
- 目前关于知识图谱**推理**问题，严格意义上不属于推理的范畴，最多只能相当于是知识补全问题，如评测中的知识推理任务，是三元组补全问题。
- 目前舆情分析还是处于初级阶段。目前舆情分析还停留在以表层计量为主，配以浅层句子级情感分析和主题挖掘技术的分析。对于深层次事件演化以及对象级情感分析依旧还处于初级阶段。
  - 【2021-3-14】情感分析五要素：（entity，aspect，opinion，holder，time）, entity + aspect -> target
    - 示例：我觉得华为手机拍照非常牛逼。 → （华为手机，拍照，正面，我，\）
    - ![](https://p1.pstatp.com/large/tos-cn-i-0022/8bd0117028624224811facc091b2ded2)
- Bert本质上仅仅是个编码器，是word2vec的升级版而已，不是无所不能，仅仅是编码能力强，向量表示上语义更为丰富，然而大多人都装糊涂。
- 学界和业界最大的区别在于
  - 学术界以**探索前沿**为目的，提新概念，然后搭个草图就结束，目光并不长远，打完这一战就不知道下一战打什么，下一战该去哪里打，什么时候打，或者打一枪换个阵地再打。
  - 而工业界，往往面临着生存问题，需要考虑实际问题，还是以**解决实际问题**为主，因此没必要把学界的那一套理念融入到生产环节中，要根据实际情况制定自己的方法。
- 利用结构化数据，尤其是百科类infobox数据，采集下来，存入到Neo4j图数据库中，就称自己建立了知识图谱的做法是伪知识图谱做法。基于这类知识图谱，再搞个简单的问答系统，就标榜自己是基于知识图谱的智能问答，实际上很肤浅。
- 知识图谱不是结构化知识的可视化（不是两个点几条边）那么简单，那叫知识的可视化，不是知识图谱。知识图谱的核心在于知识的图谱化，特点在于知识的表示方法和图谱存储结构，前者决定了知识的抽象表示维度，后者决定了知识运行的可行性，图算法(图遍历、联通图、最短路径)。基于图谱存储结构，进行知识的游走，进行知识表征和未知知识的预测。
- 物以稀为贵，大家都能获取到的知识，往往价值都很低。知识图谱也是这样，只有做专门性的具有数据壁垒的知识图谱，才能带来商业价值。
- 目前智能问答，大多都是人工智障，通用型的闲聊型问答大多是个智障，多轮对话缺失，答非所问等问题层出不穷。垂直性的问答才是出路，但真正用心做的太少，大多都是处于demo级别。
- 大多数微信自然语言处理软文实际上都不可不看，纯属浪费时间。尤其是在对内容的分析上，大多是抓语料，调包统计词频，提取关键词，调包情感分析，做柱状图，做折线图，做主题词云，分析方法上千篇一律。应该从根本上去做方法上的创新，这样才能有营养，从根本上来说才能有营养可言。文本分析应该从浅层分析走向深层分析，更好地挖掘文本的语义信息。
- 目前百科类知识图谱的构建工作有很多，重复性的工作不少。基于开放类百科知识图谱的数据获取接口有复旦等开放出来，可以应用到基本的概念下实体查询，实体属性查询等，但目前仅仅只能做到一度。
- 基于知识图谱的问答目前的难点在于两个方面
  - 1）多度也称为多跳问题，如姚明的老婆是谁，可以走14条回答，但姚明的老婆的女儿是谁则回答不出来，这种本质上是实体与属性以及实体与实体关系的分类问题。
  - 2）多轮问答问题。多轮分成两种，一种是**指代补全**问答， 如前一句问北京的天气，后者省略“的天气”这一词，而只说“北京”，这个需要进行意图判定并准确加载相应的问答槽。另一种是**追问式**多轮问答，典型的在天气查询或者酒店预订等垂直性问答任务上。大家要抓住这两个方面去做。
- 关系挖掘是信息抽取的重要里程碑，理解了实体与实体、实体与属性、属性与属性、实体与事件、事件与事件的关系是解决真正语义理解的基础，但目前，这方面，在工业界实际运用中，特定领域中模板的性能要比深度学习多得多，学界大多采用端到端模型进行实验，在这方面还难以超越模版性能。


## 工具汇总

- 【2020-8-27】NLP模型可视化工具LIT，为什么模型做出这样的预测？什么时候性能不佳？在输入变化可控的情况下会发生什么？LIT 将局部解释、聚合分析和反事实生成集成到一个流线型的、基于浏览器的界面中，以实现快速探索和错误分析。
   - [谷歌开源NLP模型可视化工具LIT，模型训练不再「黑箱」](https://www.toutiao.com/i6865152251150008844/)
   - 支持多种自然语言处理任务，包括探索情感分析的反事实、度量共指系统中的性别偏见，以及探索文本生成中的局部行为。
   - 此外 LIT 还支持多种模型，包括分类、seq2seq 和结构化预测模型。并且它具备高度可扩展性，可通过声明式、框架无关的 API 进行扩展。
   - [论文地址](https://arxiv.org/pdf/2008.05122.pdf)
   - [项目地址](https://github.com/PAIR-code/lit)，[模块操作细节](https://github.com/PAIR-code/lit/blob/main/docs/user_guide.md)
   - ![](https://p6-tt.byteimg.com/origin/pgc-image/a8703a8db04649b19e12ef152783bb4a?from=pc)

## 发展历史

- 微软亚洲研究院成立20周年时表示：[NLP将迎来黄金十年](https://www.msra.cn/zh-cn/news/executivebylines/tech-bylines-nlp)。
   - 比尔·盖茨曾说过，“语言理解是人工智能皇冠上的明珠”。自然语言处理（NLP，Natural Language Processing）的进步将会推动人工智能整体进展。
   - NLP的历史几乎跟计算机和人工智能（AI）的历史一样长。
- 回顾基于深度学习的NLP技术的重大进展，从时间轴来看主要包括：
   - NNLM(2003)
   - Word Embeddings(2013)
   - Seq2Seq(2014)
   - Attention(2015)
   - Memory-based networks(2015)
   - Transformer(2017)
   - BERT(2018)
   - XLNet(2019)
- 预训练语言模型发展历史
   - 摘自：[nlp中的预训练语言模型总结(单向模型、BERT系列模型、XLNet)](https://zhuanlan.zhihu.com/p/76912493)
![](https://pic3.zhimg.com/80/v2-d8fd47547f5a8230372ceaa894e52feb_720w.jpg)

- 【2021-1-17】Impressive progress of deep learning on unsupervised text corpora，[A Review of the Neural History of Natural Language Processing](https://ruder.io/a-review-of-the-recent-history-of-nlp/)
   - 2001 ： Neural language models
   - 2008 ： Multi-task learning
   - 2013 ： Word embeddings，Neural networks for NLP开启
   - 2014 ： Sequence-to-sequence models
   - 2015 ： Attention及Memory-based networks
   - 2018 ： Pretrained language models，BERT系列

# 算法理论


## 自监督学习

- 【2020-6-21】[NLP中的自监督表示学习，全是动图，很过瘾的](https://www.toutiao.com/i6839892851711541764/)，[英文原文](https://amitness.com/2020/05/self-supervised-learning-nlp/)
- 自监督的方法的核心是一个叫做 “pretext task” 的框架，它允许我们使用数据本身来生成标签，并使用监督的方法来解决非监督的问题。这些也被称为“auxiliary task”或“pre-training task“。通过执行此任务获得的表示可以用作我们的下游监督任务的起点。
   - ![](http://p3.pstatp.com/large/pgc-image/47ba10919c1440c781c0b14f1c14de82)
- 【2020-7-12】刘知远新书：[Representation Learning for Natural Language Processing](http://nlp.csai.tsinghua.edu.cn/news/%E4%B8%93%E8%91%97representation-learning-for-natural-language-processing%E6%AD%A3%E5%BC%8F%E5%87%BA%E7%89%88/)，[电子版下载](https://link.springer.com/book/10.1007%2F978-981-15-5573-2)

- 【2020-7-21】自监督学习综述（清华唐杰团队）：Self-supervised Learning: Generative or Contrastive
<iframe src="https://view.officeapps.live.com/op/embed.aspx?src=https%3A%2F%2Fstatic%2Eaminer%2Ecn%3A443%2Fupload%2Fppt%2F332%2F651%2F1679%2F5ee8986f91e011e66831c59b%5F1%2Epptx&amp;wdAr=1.4440104166666667" width="700px" height="500px" frameborder="0">这是嵌入 <a target="_blank" href="https://office.com">Microsoft Office</a> 演示文稿，由 <a target="_blank" href="https://office.com/webapps">Office</a> 提供支持。</iframe>


### 自监督的方案

- 1. 预测中心词
   - 在这个公式中，我们取一定窗口大小的一小块文本，我们的目标是根据周围的单词预测中心单词。
   - ![](http://p1.pstatp.com/large/pgc-image/b217d92b952d4601937a1629bc867642)
   - 例如，在下面的图中，我们有一个大小为1的窗口，因此我们在中间单词的两边各有一个单词。使用这些相邻的词，我们需要预测中心词。
   - ![](http://p9.pstatp.com/large/pgc-image/62516862aa444fed9885c34aaf955b05)
   - 这个方案已经在著名的Word2Vec论文的“Continuous Bag of Words”方法中使用过。

- 2. 预测邻居词
   - 在这个公式中，我们取一定窗口大小的文本张成的空间，我们的目标是在给定中心词的情况下预测周围的词。
   - ![](http://p3.pstatp.com/large/pgc-image/63df64f4f6af401db506372dc06c0b2f)
   - 这个方案已经在著名的Word2Vec论文的“skip-gram”方法中实现。

- 3. 相邻句子的预测
   - 在这个公式中，我们取三个连续的句子，设计一个任务，其中给定中心句，我们需要生成前一个句子和下一个句子。它类似于之前的skip-gram方法，但适- 用于句子而不是单词。
   - ![](http://p1.pstatp.com/large/pgc-image/6c88f0845d7e4ae38ea626bb3bfd9000)
   - 这个方案已经在Skip-Thought Vectors的论文中使用过。

- 4. 自回归语言建模
   - 在这个公式中，我们取大量未标注的文本，并设置一个任务，根据前面的单词预测下一个单词。因为我们已经知道下一个来自语料库的单词是什么，所以- 我们不需要手工标注的标签。
   - ![](http://p1.pstatp.com/large/pgc-image/bf36697cfb06464c8faf6ab3c88a2493)
   - 例如，我们可以通过预测给定前一个单词的下一个单词来将任务设置为从左到右的语言建模。
   - ![](http://p3.pstatp.com/large/pgc-image/1ddb11e50efa4a6f955bac7e14218b1b)
   - 我们也可以用这个方案来通给定未来的单词预测之前的单词，方向是从右到左。
   - ![](http://p1.pstatp.com/large/pgc-image/bbb35ee25289497a9c40e3eeee901fe0)
   - 这个方案已经使用在许多论文中，从n-gram模型到神经网络模型比如神经概率语言模型 (GPT) 。

- 5. 掩码语言建模
   - 在这个方案中，文本中的单词是随机掩码的，任务是预测它们。与自回归公式相比，我们在预测掩码单词时可以同时使用前一个词和下一个词的上下文。
   - ![](http://p1.pstatp.com/large/pgc-image/a9b530083051422f9f88c1613eff489f)
   - 这个方案已经在BERT、RoBERTa和ALBERT的论文中使用过。与自回归相比，在这个任务中，我们只预测了一小部分掩码词，因此从每句话中学到的东西更少。

- 6. 下一个句子预测
   - 在这个方案中，我们取文件中出现的两个连续的句子，以及同一文件或不同文件中随机出现的另一个句子。
   - ![](http://p3.pstatp.com/large/pgc-image/8d58e6c913dc445d895429086cc50ae9)
   - 然后，任务是区分两个句子是否是连贯的。
   - ![](http://p3.pstatp.com/large/pgc-image/f50ff4fe179a4678926ddcc3769289f0)
   - 在BERT的论文中，它被用于提高下游任务的性能，这些任务需要理解句子之间的关系，比如自然语言推理(NLI)和问题回答。然而，后来的研究对其有效性提出了质疑。

- 7. 句子顺序的预测
   - 在这个方案中，我们从文档中提取成对的连续句子。然后互换这两个句子的位置，创建出另外一对句子。
   - ![](http://p3.pstatp.com/large/pgc-image/0638806390584a488e06c82a11832455)
   - 我们的目标是对一对句子进行分类，看它们的顺序是否正确。
   - ![](http://p3.pstatp.com/large/pgc-image/2ba2388f8dfa4778ba4684a8bbbc57fe)
   - 在ALBERT的论文中，它被用来取代“下一个句子预测”任务。

- 8. 句子重排
   - 在这个方案中，我们从语料库中取出一个连续的文本，并破开的句子。然后，对句子的位置进行随机打乱，任务是恢复句子的原始顺序。
   - ![](http://p1.pstatp.com/large/pgc-image/451d926ff39a4ce6b3f2ef4b97d18469)
   - 它已经在BART的论文中被用作预训练的任务之一。

- 9. 文档旋转
   - 在这个方案中，文档中的一个随机token被选择为旋转点。然后，对文档进行旋转，使得这个token成为开始词。任务是从这个旋转的版本中恢复原来的句子。
   - ![](http://p3.pstatp.com/large/pgc-image/8095d77a0fc440978eb825289a83acf6)
   - 它已经在BART的论文中被用作预训练的任务之一。直觉上，这将训练模型开始识别文档。

- 10. 表情符号预测
   - 这个方案被用在了DeepMoji的论文中，并利用了我们使用表情符号来表达我们所发推文的情感这一想法。如下所示，我们可以使用推特上的表情符号作为标签，并制定一个监督任务，在给出文本时预测表情符号。
   - ![](http://p1.pstatp.com/large/pgc-image/501d4092a3c4433298824a230c07eeb1)
   - DeepMoji的作者们使用这个概念对一个模型进行了12亿条推文的预训练，然后在情绪分析、仇恨语言检测和侮辱检测等与情绪相关的下游任务上对其进行微调。

## 机器翻译

- 【2018-10】[独家：“论文致谢刷屏”博士黄国平演讲干货](https://mp.weixin.qq.com/s/RYnJnkz-55qj94hyy4zm2Q),QCon 全球软件开发大会 2018 上海站的演讲[视频](https://time.geekbang.org/dailylesson/detail/100020790)
- 【2020-6-5】[机器翻译：统计建模与深度学习方法](https://opensource.niutrans.com/mtbook/index.html)，[ppt地址](https://github.com/NiuTrans/MTBook/blob/master/slides)
- ![](https://opensource.niutrans.com/guideline.png)
- 【2020-6-10】Google官方示例：[基于注意力的神经机器翻译](https://www.tensorflow.org/tutorials/text/nmt_with_attention?hl=zh-cn)
   - ![](https://tensorflow.org/images/spanish-english.png)
- 【2021-1-13】翻车的机器翻译
  - 大数据文摘：[机器翻译古文也翻车？读了20次“苟富贵勿相忘”后，谷歌：没钱的人总会被遗忘](https://mp.weixin.qq.com/s/E2VESXhJLaNmJMlp84sXaA)
  - [谷歌翻译20次鲁迅《狂人日记》中的经典“吃人”片段！极度生草](https://www.bilibili.com/video/BV1nK4y1r75x/?spm_id_from=333.788.recommend_more_video.1)
  - [谷歌翻译20次司马迁《陈涉世家》！ 清朝，瑞士，东罗马，曹魏竟在同一时代](https://www.bilibili.com/video/BV1Jf4y1C7oP?from=search&seid=7681248349324754656)

<iframe src="//player.bilibili.com/player.html?aid=288370813&bvid=BV1Jf4y1C7oP&cid=271241642&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"  height="600" width="100%"> </iframe>

- 【2021-1-22】【LibreTranslate：可完全本地化部署的开源机器翻译API服务，基于Argos Translate】’[LibreTranslate](https://github.com/uav4geo/LibreTranslate) - Free and Open Source Machine Translation API. 100% self-hosted, no limits, no ties to proprietary services. Built on top of Argos Translate.' by UAV4GEO，[在线体验Demo](https://libretranslate.com/)
  - windows下安装失败，错误信息
    - ERROR: Could not find a version that satisfies the requirement ctranslate2 (from argostranslate==1.0) (from versions: none)；ERROR: No matching distribution found for ctranslate2 (from argostranslate==1.0)


## NER 命名实体识别

- 目标：识别序列中的人名、地名、组织机构名等实体。属于序列标注问题。
- [工业界如何解决NER问题？12个trick](https://zhuanlan.zhihu.com/p/152463745)
   - Q1、如何快速有效地提升NER性能（非模型迭代）？
   - Q2、如何在模型层面提升NER性能？
   - Q3、如何构建引入词汇信息（词向量）的NER？
   - Q4、如何解决NER实体span过长的问题？
   - Q5、如何客观看待BERT在NER中的作用？
   - Q6、如何冷启动NER任务？
   - Q7、如何有效解决低资源NER问题？
   - Q8、如何缓解NER标注数据的噪声问题？
   - Q9、如何克服NER中的类别不平衡问题？
   - Q10、如何对NER任务进行领域迁移？
   - Q11、如何让NER系统变得“透明”且健壮？
   - Q12、如何解决低耗时场景下的NER任务？


### 序列标注的几种模式

- 在序列标注中，我们想对一个序列的每一个元素(token)标注一个标签。一般来说，一个序列指的是一个句子，而一个元素(token)指的是句子中的一个词语或者一个字。比如信息提取问题可以认为是一个序列标注问题，如提取出会议时间、地点等。
- 不同的序列标注任务就是将目标句中的字或者词按照需求的方式标记，不同的结果取决于对样本数据的标注，一般序列的标注是要符合一定的标注标准的如([PKU数据标注规范](http://sighan.cs.uchicago.edu/bakeoff2005/data/pku_spec.pdf))。
- 另外, 词性标注、分词都属于同一类问题，他们的区别主要是对序列中的token的标签标注的方式不同。

下面以命名实体识别来举例说明. 我们在进行命名实体识别时，通常对每个字进行标注。中文为单个字，英文为单词，空格分割。

标签类型的定义一般如下：

|定义|	全称|	备注|
|---|---|---|
|B	|Begin	|实体片段的开始|
|I	|Intermediate|	实体片段的中间|
|E	|End	|实体片段的结束|
|S	|Single	|单个字的实体|
|O	|Other/Outside	|其他不属于任何实体的字符(包括标点等)|

### 常见的标签方案

常用的较为流行的标签方案有如下几种：
- IOB1: 标签I用于文本块中的字符，标签O用于文本块之外的字符，标签B用于在该文本块前面接续则一个同类型的文本块情况下的第一个字符。
- IOB2: 每个文本块都以标签B开始，除此之外，跟IOB1一样。
- IOE1: 标签I用于独立文本块中，标签E仅用于同类型文本块连续的情况，假如有两个同类型的文本块，那么标签E会被打在第一个文本块的最后一个字符。
- IOE2: 每个文本块都以标签E结尾，无论该文本块有多少个字符，除此之外，跟IOE1一样。
- START/END （也叫SBEIO、IOBES）: 包含了全部的5种标签，文本块由单个字符组成的时候，使用S标签来表示，由一个以上的字符组成时，首字符总是使用B标签，尾字符总是使用E标签，中间的字符使用I标签。
- IO: 只使用I和O标签，显然，如果文本中有连续的同种类型实体的文本块，使用该标签方案不能够区分这种情况。

其中最常用的是IOB2、IOBS、IOBES。

#### BIO标注模式

将每个元素标注为“B-X”、“I-X”或者“O”。其中，“B-X”表示此元素所在的片段属于X类型并且此元素在此片段的开头，“I-X”表示此元素所在的片段属于X类型并且此元素在此片段的中间位置，“O”表示不属于任何类型。

命名实体识别中每个token对应的标签集合如下:
>LabelSet = {O, B-PER, I-PER, B-LOC, I-LOC, B-ORG, I-ORG}

其中，PER代表人名， LOC代表位置， ORG代表组织. B-PER、I-PER代表人名首字、人名非首字，B-LOC、I-LOC代表地名(位置)首字、地名(位置)非首字，B-ORG、I-ORG代表组织机构名首字、组织机构名非首字，O代表该字不属于命名实体的一部分。

![](https://sthsf.github.io/2020/02/18/NLP--%E5%91%BD%E5%90%8D%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB/BIO.jpg)

对于词性标注, 则可以用{B-NP, I-NP}给序列中的名词token打标签

#### BIOES标注模式

BIOES标注模式就是在BIO的基础上增加了单字符实体和字符实体的结束标识, 即

>LabelSet = {O, B-PER, I-PER, E-PER, S-PER, B-LOC, I-LOC, E-LOC, S-LOC, B-ORG, I-ORG, E-ORG, S-ORG}

![](https://sthsf.github.io/2020/02/18/NLP--%E5%91%BD%E5%90%8D%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB/BIOES.jpg)

根据标注的复杂度, 还有会在其中添加其他的比如MISC之类的实体, 如

>LabelSet = {O ,B-MISC, I-MISC, B-ORG ,I-ORG, B-PER ,I-PER, B-LOC ,I-LOC]。
其中，一般一共分为四大类：PER（人名），LOC（位置[地名]），ORG（组织）以及MISC(杂项)，而且B表示开始，I表示中间，O表示不是实体。

其他类似的标注方式:

标注方式1:
>LabelSet = {BA, MA, EA, BO, MO, EO, BP, MP, EP, O}
其中，
- BA代表这个汉字是地址首字，MA代表这个汉字是地址中间字，EA代表这个汉字是地址的尾字；
- BO代表这个汉字是机构名的首字，MO代表这个汉字是机构名称的中间字，EO代表这个汉字是机构名的尾字；
- BP代表这个汉字是人名首字，MP代表这个汉字是人名中间字，EP代表这个汉字是人名尾字，而O代表这个汉字不属于命名实体。

标注方式2:
> LabelSet = {NA, SC, CC, SL, CL, SP, CP}
其中 NA = No entity, SC = Start Company, CC = Continue Company, SL = Start Location, CL = Continue Location, SP = Start Person, CP = Continue Person

上面两种标注方式与BIO和BIEOS类似, 只是使用不同的标签字符来标识而已.

### 参考资料

- [命名实体识别(Name Entity Recognition)综述](https://sthsf.github.io/2020/02/18/NLP--%E5%91%BD%E5%90%8D%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB/)


## 短语挖掘

### 关键词提取方案

- 知乎话题：[关键词提取都有哪些方案](https://www.zhihu.com/question/21104071)

- 刘知远的博士论文：[基于文档主题结构的关键词抽取方法研究](http://nlp.csai.tsinghua.edu.cn/~lzy/publications/phd_thesis.pdf)
- 关键词挖掘的方法：（2014年，作者：[zibuyu9](https://www.zhihu.com/question/21104071/answer/24556905)）
   - 1. TFIDF是很强的baseline，具有较强的普适性，如果没有太多经验的话，可以实现该算法基本能应付大部分关键词抽取的场景了。
   - 2. 对于中文而言，中文分词和词性标注的性能对关键词抽取的效果至关重要。
   - 3. 较复杂的算法各自有些问题，如Topic Model，它的主要问题是抽取的关键词一般过于宽泛，不能较好反映文章主题。这在我的博士论文中有专门实验和论述；TextRank实际应用效果并不比TFIDF有明显优势，而且由于涉及网络构建和随机游走的迭代算法，效率极低。这些复杂算法集中想要解决的问题，是如何利用更丰富的文档外部和内部信息进行抽取。如果有兴趣尝试更复杂的算法，我认为我们提出的基于SMT（统计机器翻译）的模型，可以较好地兼顾效率和效果。
      - TextRank源于page-rank，page-rank是谷歌提出的对网页按照影响力进行排序的算法。同样的，text-rank认为文档或句子中相邻的词语重要性是相互影响的，所以text-rank引入了词语的顺序信息。
      - ![](https://pic4.zhimg.com/80/v2-192504d9afdc37816ceb8e6c8e7649f2_720w.jpg)
   - 4. 以上都是无监督算法，即没有事先标注好的数据集合。而如果我们有事先标注好的数据集合的话，就可以将关键词抽取问题转换为有监督的分类问题。这在我博士论文中的相关工作介绍中均有提到。
   - 从性能上来讲，利用有监督模型的效果普遍要优于无监督模型，对关键词抽取来讲亦是如此。在Web 2.0时代的社会标签推荐问题，就是典型的有监督的关键词推荐问题，也是典型的多分类、多标签的分类问题，有很多高效算法可以使用。

- 作者：[小Fan](https://www.zhihu.com/question/21104071/answer/291420205)
- 分两步走：
   - `候选词匹配`：基于关键词词库的多模式匹配得到候选，这里最重要的工作是词库构建，往往会融合多种方法：垂直站点专有名词，百科词条，输入法细胞词库，广告主购买词，基于大规模语料库的自动词库挖掘（推荐韩家炜团队的 shangjingbo1226/SegPhrase ，shangjingbo1226/AutoPhrase 方法）等。这里会涉及大量的数据清洗工作，甚至还可以有一个质量分类器决定哪些词条可以进入词库。
   - `候选词相关性排序`：包括无监督和有监督方法，如下：
      - `无监督方法`：常见的有 TFIDF（需要统计 phrase 级别的 DF）， textrank（优势不明显，计算量大，慎用），topic 相似度（参见 baidu/Familia），embedding 相似度（需要训练或计算 keyword 和 doc embedding），TWE 相似度（参见 baidu/Familia）
      - `有监督方法`：常见的有基于统计机器翻译 SMT 的方法（转换成翻译问题，可以采用 IBM Model 1），基于序列标注模型的方法（转换成核心成分识别问题，类似 NER，状态只有0和1，即是否是核心成分，较适用于短文本），基于排序学习LTR的方法（转换成候选词排序问题，采用 pairwise 方法，或者深度语义匹配方法，如 DSSM），基于传统机器学习分类方法（转换成二元或多元分类问题）。有监督方法依赖一定规模的标注数据，效果通常会显著好于无监督方法。


### [韩家炜团队开源工具](https://www.jianshu.com/p/76197e116835)

- 具体工具如下：
   - `TopMine`：频率模式挖掘+统计分析
   - `SegPhrase`：SegPhrase：弱监督、高质量的 Phrase Mining
      - 论文：Mining Quality Phrases from Massive Text Corpora
      - TopMine 的方法完全是无监督的，如果有少量的 Label 数据可能会在很大程度上提高 Topic Model 的结果。
      - SegPhrase框架只需要有限的培训，但是生成的短语的质量却接近于人类的判断力。而且，该方法具有可伸缩性：随着语料库大小的增加，计算时间和所需空间都会线性增长。论文在大型文本语料库上的实验证明了该新方法的质量和效率。
      - segphrase已有[GitHub开源工具](https://github.com/shangjingbo1226/SegPhrase)
   - `AutoPhrase`：自动的 Phrase Mining
      - 论文：Automated Phrase Mining from Massive Text Corpora
      - AutoPhrase支持多种语言（包含简体中文和繁体中文）基本思想是通过分别在训练和解析过程中添加编码/解码层来重用英语实现。在训练阶段，在中文单词片段和英文字母之间创建一个字典，将输入数据编码为英文伪单词。在解析阶段，在识别出编码后的新文档中的优质短语之后，进行解码以恢复可读的中文。
      - AutoPhrase已有[GitHub开源工具](https://github.com/shangjingbo1226/AutoPhrase)

### AutoPhrase

- 【2020-7-3】数据挖掘之父韩家炜团队的自动短语挖掘论文：[Automated Phrase Mining from Massive Text Corpora]()，提出自动挖掘短语的方法：AutoPhrase，[C++代码](https://github.com/shangjingbo1226/AutoPhrase)，[Python包](https://github.com/CS512-Autophrase-Demo/AutophrasePy), [解说](https://blog.csdn.net/weixin_42363527/article/details/102884088)
- 思路：使用通用知识库（KB）的来构造正样本（应该就是用完全匹配的方式），然后训练一个NER模型（非神经网络的），然后用这个NER模型的预测结果来减少负样本噪声，引入词性信息
- 先从KB里匹配出正样本，其他的词是负样本，然后训练NER/CRF模型，再卡个阈值，筛掉分低的实体，最后出的作为抽取出的短语。

对于自动短语挖掘任务，
- 输入：语料库（特定语言和特定领域的文本单词序列，长度任意）和知识库
- 输出：一个按质量递减排列的短语列表

短语质量 定义为一个单词序列成为一个完整语义单元的概率，满足以下条件：
- `流行度`: 在给定的文档集合中，质量短语应该出现的频率足够高。
- `一致性`: 由于偶然因素，令牌在高质量短语中的搭配出现的概率明显高于预期。
- `信息性`: 如果一个短语表达了一个特定的主题或概念，那么这个短语就是信息性的。
- `完备性`: 长频繁短语及其子序列均满足上述3个条件。当一个短语可以在特定的文档上下文中解释为一个完整的语义单元时，它就被认为是完整的

流程图
- ![](https://img-blog.csdnimg.cn/2020042217323277.png)
- AutoPhrase会根据正池和负池对短语质量进行两次评估，一次在短语分割前，一次在短语分割后。也就是说，POS-Guided短语分割需要一组初始的短语质量分数;我们预先根据原始频率估计分数;然后，一旦特征值被纠正，我们重新估计分数。

- `AutoPhrase`超越了分段短语，进一步摆脱了额外的手工标注工作，提高了性能。
- 主要使用以下两种新技术：
   - `Robust Positive-Only Distant Training`（鲁棒正向远程训练）
      - 即：利用已有的知识库（Wikipedia）做远程监督训练
      - 公共知识库（如维基百科）中的高质量短语，免费并且数量很多。在远程训练中，使用一般知识库中高质量短语，可以避免手工标注。
      - 具体做法是：
         - 从一般知识库中构建积极标签的样本
         - 从给定的领域语料库中构建消极标签的样本
         - 训练大量的基本分类器
         - 将这些分类器的预测聚合在一起
   - `POS-Guided Phrasal Segmentation`. （POS-Guided短语分割）
      - 即：利用词性信息来增加抽取的准确性
      - 语言处理器应该权衡：`性能` 和 `领域独立能力`
         - 对于领域独立能力，如果没有语言领域知识，准确性会受限制
         - 对于准确性，依赖复杂的、训练有素的语言分析器，就会降低领域独立能力
      - 解决办法： 在文档集合的语言中加入一个预先训练的词性标记，以进一步提高性能

## 文本分类


### fasttext

- 【2021-5-27】fasttext, [简介](https://blog.csdn.net/qq_32023541/article/details/80839800?spm=1001.2014.3001.5501) [安装使用](https://blog.csdn.net/qq_32023541/article/details/80844036)，部署
  - fasttext 是一个有效的学习字词表达和句子分类的库。建立在现代 Mac OS 和 Linux 发行版上。因为它使用了 C++11 的特性，所以需要一个支持 C++11 的编译器，这些包括：gcc-4.8 或更高版本, clang-3.3 或更高版本

FastText是Facebook开发的一款快速文本分类器，提供简单而高效的文本分类和表征学习的方法，性能比肩深度学习而且速度更快。

fastText 方法包含三部分：模型架构、层次 Softmax 和 N-gram 特征。
- 模型架构:fastText 模型输入一个词的序列（一段文本或者一句话)，输出这个词序列属于不同类别的概率。序列中的词和词组组成特征向量，特征向量通过线性变换映射到中间层，中间层再映射到标签。fastText 在预测标签时使用了非线性激活函数，但在中间层不使用非线性激活函数。
- 层次softmax:在某些文本分类任务中类别很多，计算线性分类器的复杂度高。为了改善运行时间，fastText 模型使用了层次 Softmax 技巧。层次 Softmax 技巧建立在哈夫曼编码的基础上，对标签进行编码，能够极大地缩小模型预测目标的数量。
- N-gram 特征：fastText 可以用于文本分类和句子分类。不管是文本分类还是句子分类，我们常用的特征是词袋模型。但词袋模型不能考虑词之间的顺序，因此 fastText 还加入了 N-gram 特征。

（2）分类过程

fasttext在进行文本分类时，huffmax树叶子节点处是每一个类别标签的词向量。在训练过程中，训练语料的每一个词也会得到响应的词向量。输入为一个window 内的词对应的词向量，隐藏层为这几个词的线性相加。相加的结果作为该文档的向量。再通过softmax层得到预测标签。结合文档真实标签计算 loss，梯度与迭代更新词向量（优化词向量的表达）。

参数方面的建议：
1. loss function 选用 hs（hierarchical softmax）要比 ns（negative sampling）训练速度更快，准确率也更高
2. wordNgram 默认为1，建议设置为 2 或以上更好
3. 如果词数不是很多，可以把 bucket 设置小一些，否则会预留太多的 bucket 使模型太大


```shell
# 下载后安装
pip install fasttext
# 直接安装（可能失败）
git clone https://github.com/facebookresearch/fastText.git
cd fastText
pip install .
```

示例代码
- data.txt 是一个以 UTF-8 编码的训练文本文件，在默认情况下，单词向量将会考虑到 n-grams 的3 到 6 个字符。在优化结束时，程序将保存 2 个文件： model.bin 和 model.vec

```python
import fasttext
# 训练文件，skipgram model
model = fasttext.skipgram('data.txt','model')
print model.words # 字典中的词汇列表
# 单词 'king' 的词向量
print model['king']
# CBOW model
model = fasttext.cbow('data.txt','model')
print model.words

# 加载前面训练好的模型 model.bin
model = fasttext.load_model("model.bin")
# ---- 文本分类 ----
# 训练文本分类器
classifier = fasttext.supervised('data.train.txt','model')
# 还可以用 label_prefix 指定标签前缀
classifier = fasttext.supervised('data.train.txt','model',label_prefix = '__label__')
# 输出两个文件: model.bin 和 model.vec
# 加载模型
classifier = fasttext.load_model("model.bin",label_prefix = "__label__")
# 使用 classifier.test 方法在测试数据集上评估模型
result = classifier.test("test.txt")
print "准确率：" , result.precision
print "召回率：" , result.recall
print "Number of examples:", result.nexamples
# 预测
texts = ["example very long text 1","example very longtext 2"]
labels = classifier.predict(texts)
labels = classifier.predict_proba(texts) # 含概率
labels = classifier.predict(texts,k = 3) # top k的预测结果
print labels
```

## 文本生成

- 【2021-1-9】[现代自然语言生成](https://item.jd.com/12785661.html) 黄民烈

### 综述

- 文本生成被称为NLG，目标是根据输入数据生成自然语言的文本。
   - NLP领域使用更多的一般是NLU（Nature Language Understanding 自然语言理解）类任务，如文本分类、命名实体识别等，NLU的目标则是将自然语言文本转化成结构化数据。
   - NLU和NLG两者表向上是一对相反的过程，但其实是紧密相连的，甚至目前很多NLU的任务都受到了生成式模型中表示方法的启发，更多只在最终任务上有所区别
- 文本生成，广义上只要输出是自然语言文本的各类任务都属于这个范畴
   - 端到端文本生成应用领域
   ![](https://p0.meituan.net/travelcube/5166cd647d076e3cea26972cbe9a332e75935.png)

### 技术方案

- 文本生成包含文本表示和文本生成两个关键的部分，既可以独立建模，也可以通过框架完成端到端的训练

#### 文本生成
- (1) seq2seq
   - 2014年提出的Seq2Seq Model，是解决这类问题一个非常通用的思路，本质是将输入句子或其中的词Token做Embedding后，输入循环神经网络中作为源句的表示，这一部分称为Encoder；另一部分生成端在每一个位置同样通过循环神经网络，循环输出对应的Token，这一部分称为Decoder。通过两个循环神经网络连接Encoder和Decoder，可以将两个平行表示连接起来。
   - ![](https://p1.meituan.net/travelcube/71ee857a5f58390785c10738d57d5c7970847.png)
- (2) attention
   - 本质思想是获取两端的某种权重关系，即在Decoder端生成的词和Encoder端的某些信息更相关。它也同样可以处理多模态的问题，比如Image2Text任务，通过CNN等将图片做一个关键特征的向量表示，将这个表示输出到类似的Decoder中去解码输出文本，视频语音等也使用同样的方式
   - ![](https://p0.meituan.net/travelcube/c64908b07137477135f9b7aa2927daea170277.png)

- Encoder-Decoder是一个非常通用的框架，它同样深入应用到了文本生成的三种主流方法，分别是规划式、抽取式和生成式，下面看下这几类方法各自的优劣势：
   - 规划式：根据结构化的信息，通过语法规则、树形规则等方式规划生成进文本中，可以抽象为三个阶段。宏观规划解决“说什么内容”，微观规划解决“怎么说”，包括语法句子粒度的规划，以及最后的表层优化对结果进行微调。
   - 抽取式：顾名思义，在原文信息中抽取一部分作为输出。可以通过编码端的表征在解码端转化为多种不同的分类任务，来实现端到端的优化。
      - 优势: 控制力极强、准确率较高，特别适合新闻播报等模版化场景。控制力极强，对源内容相关性好，改变用户行文较少，也不容易造成体验问题，可以直接在句子级别做端到端优化
      - 劣势是很难做到端到端的优化，损失信息上限也不高。
         - 在优化评估上，首先标题创意衡量的主观性很强，线上Feeds的标注数据也易受到其他因素的影响，比如推荐排序本身；其次，训练预测数据量差异造成OOV问题非常突出，分类任务叠加噪音效果提升非常困难。对此，我们重点在语义＋词级的方向上来对点击/转化率做建模，同时辅以线上E&E选优的机制来持续获取标注对，并提升在线自动纠错的能力。
         - 在受限上，抽取式虽然能直接在Seq级别对业务目标做优化，但有时候也须兼顾阅读体验，否则会形成一些“标题党”，亦或造成与原文相关性差的问题。对此，我们抽象了预处理和质量模型，来通用化处理文本创意内容的质控，独立了一个召回模块负责体验保障。并在模型结构上来对原文做独立表示，后又引入了Topic Feature Context来做针对性控制。
      - ![](https://p0.meituan.net/travelcube/94790c9b54cf6ad34fc800c1579416c3130763.png)
   - 生成式：通过编码端的表征，在解码端完成序列生成的任务，可以实现完全的端到端优化，可以完成多模态的任务。其在泛化能力上具有压倒性优势，但劣势是控制难度极大，建模复杂度也很高。
- 目前的主流的评估方法主要基于数据和人工评测。基于数据可以从不同角度衡量和训练目标文本的相近程度，如基于N-Gram匹配的BLUE和ROUGE等，基于字符编辑距离（Edit Distance）等，以及基于内容Coverage率的Jarcard距离等。基于数据的评测，在机器翻译等有明确标注的场景下具有很大的意义，这也是机器翻译领域最先有所突破的重要原因。但对于我们创意优化的场景来说，意义并不大，我们更重要的是优化业务目标，多以线上的实际效果为导向，并辅以人工评测。
- 另外，值得一提的是，近两年也逐渐涌现了很多利用GAN（Generative Adversarial Networks，生成对抗网络）的相关方法，来解决文本生成泛化性多样性的问题。有不少思路非常有趣，也值得尝试，只是GAN对于NLP的文本生成这类离散输出任务在效果评测指标层面，与传统的Seq2Seq模型还存在一定的差距，可视为一类具有潜力的技术方向。

#### 文本表示

- 整个2018年有两方面非常重要的工作进展：
   - Contextual Embedding：该方向包括一系列工作，如最佳论文Elmo(Embeddings from Language Models)，OpenAI的GPT(Generative Pre-Training)，以及谷歌大力出奇迹的BERT(Bidirectional Encoder Representations from Transformers)。解决的核心问题，是如何利用大量的没标注的文本数据学到一个预训练的模型，并通过通过这个模型辅助在不同的有标注任务上更好地完成目标。传统NLP任务深度模型，往往并不能通过持续增加深度来获取效果的提升，但是在表示层面增加深度，却往往可以对句子做更好的表征，它的核心思想是利用Embedding来表征上下文的的信息。但是这个想法可以通过很多种方式来实现，比如ELMo，通过双向的LSTM拼接后，可以同时得到含上下文信息的Embedding。而Transformer则在Encoder和Decoder两端，都将Attention机制都应用到了极致，通过序列间全位置的直连，可以高效叠加多层（12层），来完成句子的表征。这类方法可以将不同的终端任务做一个统一的表示，大大简化了建模抽象的复杂度。我们的表示也经历了从RNN到拥抱Attention的过程。
   
![](https://p1.meituan.net/travelcube/5f1b394d23c4fe5be6dfe885028a8668332821.png)

图5 GPT ELMo BERT模型结构
   - Tree-Based Embedding：另外一个流派则是通过树形结构进行建模，包括很多方式如传统的语法树，在语法结构上做Tree Base的RNN，用根结点的Embedding即可作为上下文的表征。Tree本身可以通过构造的方式，也可以通过学习的方式（比如强化学习）来进行构建。最终Task效果，既和树的结构（包括深度）有关，也受“表示”学习的能力影响，调优难度比较大。在我们的场景中，人工评测效果并不是很好，仍有很大继续探索的空间。

### 信息流中的应用

- 【2020-5-26】[大众点评信息流基于文本生成的创意优化实践](https://tech.meituan.com/2019/03/14/information-flow-creative-optimization-practices.html)
- 核心目标与推荐问题相似，提升包括点击率、转化率在内的通用指标，同时需要兼顾考量产品的阅读体验包括内容的导向性等
- 信息流中落地重点包括三个方向：
   - 文本创意：在文本方面，既包括了面向内容的摘要标题、排版改写等，也包括面向商户的推荐文案及内容化聚合页。它们都广泛地应用了文本表示和文本生成等技术，也是本文的主要方向。
   - 图像创意：图像方面涉及到首图或首帧的优选、图像的动态裁剪，以及图像的二次生成等。
   - 其他创意：包括多类展示理由（如社交关系等）、元素创意在内的额外补充信息。
![](https://p0.meituan.net/travelcube/a94ee4867832c8b91a9bae495d09eaf084451.png)

- 文本创意优化，在业务和技术上分别面临着不同的挑战。
   - 首先业务侧，启动创意优化需要两个基础前提：
      - 第一，衔接好创意优化与业务目标，因为并不是所有的创意都能优化，也不是所有创意优化都能带来预期的业务价值，方向不对则易蹚坑。
      - 第二，创意优化转化为最优化问题，有一定的Gap。其不同于很多分类排序问题，本身相对主观，所谓“一千个人眼中有一千个哈姆雷特”，创意优化能不能达到预期的业务目标，这个转化非常关键。
   - 其次，在技术层面，业界不同的应用都面临不一样的挑战，并且尝试和实践对应的解决方案。对文本创意生成来说，我们面临的最大的挑战包括以下三点：
      - 带受限的生成 生成一段流畅的文本并非难事，关键在于根据不同的场景和目标能控制它说什么、怎么说。这是目前挑战相对较大的一类问题，在我们的应用场景中都面临这个挑战。
      - 业务导向 生成能够提升业务指标、贴合业务目标的内容。为此，对内容源、内容表示与建模上提出了更高的要求。
      - 高效稳定 这里有两层含义，第一层是高效，即模型训练预测的效果和效率；第二层是稳定，线上系统应用，需要具备很高的准确率和一套完善的质量提升方案。

## 指针网络

- 【2021-3-24】Pointer-Generator模型，解决了摘要中存在事实性错误的问题。然后作者又向前走了一步，就是加入了Coverage机制来解决重复问题
- Pointer Networks天生具备输出元素来自输入元素这样的特点，于是它非常适合用来实现“复制”这个功能。Pointer Networks其实特别适合用于解决OOV（out of vocabulary）问题
- 演进路线
![](https://p1.pstatp.com/large/tos-cn-i-0022/f032ede80be342db930db99361fb9337)

## 多轮对话改写

【2021-3-31】指代消解
- 代词是用来代替重复出现的名词
- 例句：
  1. Ravi is a boy. He often donates money to the poor.
    - 先出现主语，后出现代词，所以流动的方向从左到右，这类句子叫**回指**(Anaphora)
  2. He was already on his way to airport.Realized Ravi.
    - 这种句子表达的方式的逆序的，这类句子叫**预指**(Cataphora)

- 【2020-6-19】[浅谈多轮对话改写](https://zhuanlan.zhihu.com/p/145057649)
   - 2000条日常对话数据进行了简单地统计，发现其中33.5%存在指代，52.7%存在省略的情况
   - 人脑具有记忆的能力，能够很好地重建对话历史的重要信息，自动补全或者替换对方当前轮的回复，来理解回复的意思。同样，为了让对话系统具备记忆的能力，通常会通过对话状态跟踪（DST），来存储对话历史中的一些重要信息。
   - DST的主要问题有：
      - 存储的信息量有限，对于长对话的历史信息无法感知；
      - 存储信息不会太丰富，否则会导致冗余信息太多；
      - 存储的信息不太便于对话系统中其他模块的任务使用。
   - rewrite模型分为Pipeline的方式和End2End的方式
      - ![](https://pic4.zhimg.com/80/v2-ba681d520212275b177bd76d845cb2bf_1440w.jpg)
      - Pipeline方法将rewrite任务分成两个子任务，即指代和零指代的检测以及指代消解
      - 基于模型复杂度和性能的考虑，End2End方法的探索一直没有停止，直到2019年出现了几篇比较优秀的工作，本文将其划分为3种类型：基于联合训练的方法、基于序列标注的方法和基于指针网络的方法。
- [ACL 2019 使用表达改写提升多轮对话系统效果](https://www.aminer.cn/research_report/5d527dd4d5e908133c946b07)
   - [代码实现](https://github.com/chin-gyou/dialogue-utterance-rewriter/issues/11#issuecomment-627871399)
   - 【2020-12-02】[优化版](https://github.com/liu-nlper/dialogue-utterance-rewriter)，使用transformer，ACL 2019论文复现，多轮对话重写：[Improving Multi-turn Dialogue Modelling with Utterance ReWriter](https://www.aclweb.org/anthology/P19-1003.pdf)
      - 作者开源的代码是基于LSTM的，论文中基于Transformer的代码并未公布；
      - 论文实验所使用数据与公开的数据不一致，所以给出新的指标以供参考。
![](http://zhengwen.aminer.cn/a1.png)
- 模型框架
![](http://zhengwen.aminer.cn/a2.png)
- 对于任务导向型对话系统和闲聊型对话系统均有效果提升，实现了用更成熟的单轮对话技术解决多轮对话问题。
![](http://zhengwen.aminer.cn/a10.png)
- [Transformer多轮对话改写实践](https://zhuanlan.zhihu.com/p/137127209)
   - 介绍了多轮对话存在指代和信息省略的问题，同时提出了一种新方法-抽取式多轮对话改写，可以更加实用的部署于线上对话系统，并且提升对话效果
- 日常的交流对话中
   - 30%的对话会包含指代词。比如“它”用来指代物，“那边”用来指代地址
   - 同时有50%以上的对话会有信息省略
- 对话改写的任务有两个：1这句话要不要改写；2 把信息省略和指代识别出来。对于baseline论文放出的数据集，有90%的数据都是简单改写，也就是满足任务2，只有信息省略或者指代词。少数改写语句比较复杂，本文训练集剔除他们，但是验证集保留。
![](https://pic1.zhimg.com/80/v2-d80efd57b81c6ece955a247ca7247db4_1440w.jpg)
- Transformers结构可以通过attention机制有效提取指代词和上下文中关键信息的配对，最近也有一篇很好的工作专门用Bert来做指代消岐[2]。经过transformer结构提取文本特征后，模型结构及输出如下图。
![](https://pic4.zhimg.com/80/v2-0c4a789b68c60c8279dbd98fc18b5b2b_1440w.jpg)
- 输出五个指针中：
   - 关键信息的start和end专门用来识别需要为下文做信息补全或者指代的词；
   - 补全位置用来预测关键信息(start-end)插入在待改写语句的位置，实验中用插入位置的下一个token来表示；
   - 指代start和end用来识别带改写语句出现的指代词。
   - 当待改写语句中不存在指代词或者关键信息的补全时，指代的start和end将会指向cls，同理补全位置也这样。如同阅读理解任务中不存在答案一样，这样的操作在做预测任务时，当指代和补全位置的预测最大概率都位于cls时就可以避免改写，从而保证了改写的稳定性。
- 效果评估
   - 准确度
   ![](https://pic3.zhimg.com/80/v2-75faf2bed618cf5170efa56d65cd88e2_1440w.jpg)
   - 性能
   ![](https://pic3.zhimg.com/80/v2-75faf2bed618cf5170efa56d65cd88e2_1440w.jpg)
   - 对数据集的依赖
   ![](https://pic2.zhimg.com/80/v2-b22df166f10a6716b3db01e303f1a721_1440w.jpg)
   - 对负样本(不需要改写样本)的识别.基于指针抽取的方法对负样本的识别效果会更好。同时根据对长文本的改写效果观察，生成式改写效果较差。
   ![](https://pic3.zhimg.com/80/v2-b653c7da5923236991b6b0c5f973703a_1440w.jpg)
- 示例

|A1|B1|A2|算法改写结果|用户标注label|
|---|---|---|---|---|---|
|你知道板泉井水吗 | 知道 | 她是歌手 | 板泉井水是歌手 | 板泉井水是歌手|
|乌龙茶 | 乌龙茶好喝吗 | 嗯好喝 | 嗯乌龙茶好喝 | 嗯乌龙茶好喝|
|武林外传 | 超爱武林外传的 | 它的导演是谁 | 武林外传的导演是谁 | 武林外传的导演是谁|
|李文雯你爱我吗 | 李文雯是哪位啊 | 她是我女朋友 | 李文雯是我女朋友 | 李文雯是我女朋友|
|舒马赫 | 舒马赫看球了么 | 看了 | 舒马赫看了 | 舒马赫看球了|

- 结论
   - 抽取式文本改写和生成式改写效果相当
   - 抽取式文本改写速度上绝对优于生成式
   - 抽取式文本改写对训练数据依赖少
   - 抽取式文本改写对负样本识别准确率略高于生成式
- 参考
  - 2019.07，ACL，微信AI Lab [Improving Multi-turn Dialogue Modelling with Utterance ReWriter](https://www.aclweb.org/anthology/P19-1003.pdf)
  - 2020.12.21，AAAI 2021，Netease Games AI Lab，[SARG: A Novel Semi Autoregressive Generator for Multi-turn Incomplete Utterance Restoration](https://arxiv.org/abs/2008.01474)，
- 几种模型：
   - CopyNet：基于LSTM 的Seq2Seq模型，使用attention 和 copy 机制 （基本的指针网络）
   - PAC：首先使用bert fintune 的 H，U 词向量，然后直接喂给标准的指针生成网络
   - **T-Ptr-λ**：就上面讲的那一篇腾讯论文的方法，主要是利用6层-6层 的Transformer Encoder Decoder结构
   - Seq2Seq-Uni: 这个baseline 是将Transformer 的各个block 层的输出做了统一结合，然后应用到 seq2seq 结构
   - **SARG**：论文提出的模型，超参数使用 α=3，λ=1 

## 小样本学习

- 从「文本增强」和「半监督学习」这两个角度出发，谈一谈如何解决少样本困境
   - ![](https://pic4.zhimg.com/v2-ad143542b083a12c46673589de4f136f_b.jpg)
- NLP中文本增强方法
   - ![](https://pic4.zhimg.com/v2-938b727e9572bb86dce68e37b68db40f_b.jpg)
- 详情参考：
   - [标注样本少怎么办？「文本增强+半监督学习」总结（从PseudoLabel到UDA/FixMatch） - JayLou娄杰的文章](https://zhuanlan.zhihu.com/p/146777068)
   - [一文了解NLP中的数据增强方法-简枫的文章](https://zhuanlan.zhihu.com/p/145521255)
   - 【2020-7-7】【一键中文数据增强工具】'NLP Chinese Data Augmentation' by 425776024 [GitHub](https://github.com/425776024/nlpcda),支持如下功能：
      - 1.随机实体替换
      - 2.近义词
      - 3.近义近音字替换
      - 4.随机字删除（内部细节：数字时间日期片段，内容不会删）
      - 5.新增：NER类 BIO 数据增强
      - 6.新增 随机置换邻近的字：研表究明，汉字序顺并不定一影响文字的阅读理解<<是乱序的
      - 7.新增百度中英翻译互转实现的增强
      - 8.新增中文等价字替换（1 一 壹 ①，2 二 贰 ②）

- 【2020-7-7】创新奇智有关少样本学习（Few-shot Learning）的研究论文《Prototype Rectification for Few-Shot Learning》被全球计算机视觉顶会ECCV 2020接收为Oral论文，入选率仅2%。[参考地址](https://www.toutiao.com/i6846586199994270222/)
- 本文提出一种简单有效的少样本学习方法，通过减小类内偏差和跨类偏差进行原型校正，从而显著提高少样本分类结果，并且给出理论推导证明本文所提方法可以提高理论下界，最终通过实验表明本方法在通用数据集中达到了最优结果，论文被ECCV 2020 接收为Oral。

- 【2020-8-19】[少样本学习中的深度学习模型综述](https://arxiv.org/abs/2008.06365v2)

- 【2020-11-12】Few-shot Learning最新进展调研
- 【2021-1-20】**少样本学习**可以使一个模型快速承接各种任务。但是为每个任务更新整个模型的权重是很浪费的。最好进行局部更新，让更改集中在一小部分参数里。有一些方法让这些微调变得更加有效和实用，包括：
   - 使用 adapter（Houlsby et al., 2019、Pfeiffer et al., 2020a、Üstün et al., 2020）
   - 加入稀疏参数向量（Guo et al., 2020）
   - 仅修改偏差值（Ben-Zaken et al., 2020）
- 能够仅基于几个范例就可以让模型学会完成任务的方法，大幅度降低了机器学习、NLP 模型应用的门槛。
- 这让模型可以适应新领域，在数据昂贵的情况下为应用的可能性开辟了道路。
- 对于现实世界的情况，我们可以收集上千个训练样本。模型同样也应该可以在少样本学习和大训练集学习之间无缝切换，不应受到例如文本长度这样的限制。在整个训练集上微调过的模型已经在 SuperGLUE 等很多流行任务中实现了超越人类的性能，但如何增强其少样本学习能力是改进的关键所在。

# Demo集合

- 【2020-5-19】NLP各类应用Demo汇总（Colab实现），[The Super Duper NLP Repo](https://notebooks.quantumstat.com/)，如文本生成、对话、问答、翻译等
![](https://image.jiqizhixin.com/uploads/editor/de27f3bd-2740-4097-b6d6-7340aeeb32cd/302.png)

<iframe src='https://notebooks.quantumstat.com/' frameborder='0' scrolling='no' allowfullscreen="true"></iframe>


# nlp工具集合

## web平台

- 【2021-7-15】[nlp-web-demo](https://github.com/lonly197/nlp-web-demo)页面集成了Stanford, Hanlp, FNLP, Thulc, FudanDNN, Boson NLP and Jieba等方法，对比分词、词性标注、句法、摘要等效果
- watson的[Natural Language Understanding](https://natural-language-understanding-demo.ng.bluemix.net/)工具包 demo，支持中文，可以实测。

## 开源nlp工具包

[nlp工具包汇总推荐](https://www.biaodianfu.com/nlp-tools.html)

### 词性解释

- n/名词 np/人名 ns/地名 ni/机构名 nz/其它专名
- m/数词 q/量词 mq/数量词 t/时间词 f/方位词 s/处所词
- v/动词 a/形容词 d/副词 h/前接成分 k/后接成分
- i/习语 j/简称 r/代词 c/连词 p/介词 u/助词 y/语气助词
- e/叹词 o/拟声词 g/语素 w/标点 x/其它

### Jieba结巴

结巴中文分词采用的算法
- 基于Trie树结构实现高效的词图扫描，生成句子中汉字所有可能成词情况所构成的有向无环图（DAG)
- 采用了动态规划查找最大概率路径, 找出基于词频的最大切分组合
- 对于未登录词，采用了基于汉字成词能力的HMM模型，使用了Viterbi算法

三种分词模式：
- 精确模式，试图将句子最精确地切开，适合文本分析；
- 全模式，把句子中所有的可以成词的词语都扫描出来, 速度非常快，但是不能解决歧义；
- 搜索引擎模式，在精确模式的基础上，对长词再次切分，提高召回率，适合用于搜索引擎分词。

```python
# -*- coding:utf-8 -*-
import jieba

text = '我来到北京清华大学'
# 添加或管理自定义词典

# 分词
default_mode = jieba.cut(text) # 精确
full_mode = jieba.cut(text,cut_all=True) # 全模式
search_mode = jieba.cut_for_search(text) # 搜索引擎

print("精确模式:","/".join(default_mode))
print("全模式:","/".join(full_mode))
print("搜索引擎模式:","/".join(search_mode))
# 关键词提取
tags = jieba.analyse.extract_tags(text,2)
print("关键词抽取:","/".join(tags))

```


### 北大pkuseg —— 超过jieba+THULAC

北京大学语言计算与机器学习研究组研制推出的一套全新的中文分词工具包。pkuseg具有如下几个特点：
- **多领域**分词。不同于以往的通用中文分词工具，此工具包同时致力于为不同领域的数据提供个性化的预训练模型。根据待分词文本的领域特点，用户可以自由地选择不同的模型。 我们目前支持了**新闻**领域，**网络**领域，**医药**领域，**旅游**领域，以及混合领域的分词预训练模型。在使用中，如果用户明确待分词的领域，可加载对应的模型进行分词。如果用户无法确定具体领域，推荐使用在混合领域上训练的通用模型。各领域分词样例可参考txt。
- 更高的分词准确率。相比于其他的分词工具包，当使用相同的训练数据和测试数据，pkuseg可以取得更高的分词准确率。
- 支持用户**自训练**模型。支持用户使用全新的标注数据进行训练。
- 支持词性标注。

```python
import pkuseg

seg = pkuseg.pkuseg()   # 以默认配置加载模型
seg = pkuseg.pkuseg(model_name='medicine')  # 程序会自动下载所对应的细领域模型, news/web/tourism/medicine
seg = pkuseg.pkuseg(postag=True)  # 开启词性标注功能
seg = pkuseg.pkuseg(user_dict='my_dict.txt')  # 给定用户词典为当前目录下的"my_dict.txt"

text = seg.cut('我爱北京天安门')  # 进行分词
print(text)

# 对input.txt的文件分词输出到output.txt中, 开8个进程
pkuseg.test('input.txt', 'output.txt', nthread=8)
# 模型训练
pkuseg.train(trainFile, testFile, savedir, train_iter = 20, init_model)
```

[pkuseg](https://github.com/lancopku/pkuseg-python)分词示例，及词云可视化

```python
import pkuseg
from collections import Counter
import pprint
from wordcloud import WordCloud
import matplotlib.pyplot as plt

with open("data/santisanbuqu_liucixin.txt", encoding="utf-8") as f:
    content = f.read()

with open("data/CNENstopwords.txt", encoding="utf-8") as f:
    stopwords = f.read()

lexicon = ['章北海', '汪淼', '叶文洁']
seg = pkuseg.pkuseg(user_dict=lexicon)
text = seg.cut(content)

new_text = []
for w in text:
    if w not in stopwords:
        new_text.append(w)
counter = Counter(new_text)
pprint.pprint(counter.most_common(50))

cut_text = " ".join(new_text)

wordcloud = WordCloud(font_path="font/FZYingXueJW.TTF", background_color="white", width=800, height=600).generate(
    cut_text)

plt.imshow(wordcloud, interpolation="bilinear")
plt.axis("off")
plt.show()
```

[img](https://www.biaodianfu.com/wp-content/uploads/2020/10/wordcloud.png) ![](https://www.biaodianfu.com/wp-content/uploads/2020/10/wordcloud.png)

### 哈工大LTP

- [LTP](https://github.com/HIT-SCIR/ltp), pyltp是python下对ltp(c++)的封装。
- 安装
  - pip install pyltp
  - 安装完毕后，还需要下载模型文件，文件[下载地址](http://ltp.ai/download.html)

代码

```python
from ltp import LTP

ltp = LTP()  # 默认加载 Small 模型
seg, hidden = ltp.seg(["他叫汤姆去拿外衣。"])
pos = ltp.pos(hidden)
ner = ltp.ner(hidden)
srl = ltp.srl(hidden)
dep = ltp.dep(hidden)
sdp = ltp.sdp(hidden)
```
pyltp示例

```python
import os
from pyltp import SentenceSplitter
from pyltp import Segmentor
from pyltp import Postagger
from pyltp import NamedEntityRecognizer
from pyltp import Parser
from pyltp import SementicRoleLabeller

LTP_DATA_DIR = 'D:\CodeHub\CutSeg\ltp_data_v3.4.0'  # ltp模型目录的路径

# 分句
sents = SentenceSplitter.split('元芳你怎么看？我就趴窗口上看呗！')  # 分句
print('\n'.join(sents))

# 分词
cws_model_path = os.path.join(LTP_DATA_DIR, 'cws.model')  # 分词模型路径，模型名称为`cws.model`
segmentor = Segmentor()  # 初始化实例
segmentor.load(cws_model_path)  # 加载模型
words = segmentor.segment('元芳你怎么看')  # 分词
print('/'.join(words))
segmentor.release()  # 释放模型

# 词性标注
pos_model_path = os.path.join(LTP_DATA_DIR, 'pos.model')  # 词性标注模型路径，模型名称为`pos.model`
postagger = Postagger()  # 初始化实例
postagger.load(pos_model_path)  # 加载模型
postags = postagger.postag(words)  # 词性标注
print('/'.join(postags))
postagger.release()  # 释放模型

# 命名实体识别
ner_model_path = os.path.join(LTP_DATA_DIR, 'ner.model')  # 命名实体识别模型路径，模型名称为`ner.model`
recognizer = NamedEntityRecognizer()  # 初始化实例
recognizer.load(ner_model_path)  # 加载模型
netags = recognizer.recognize(words, postags)  # 命名实体识别
print('/'.join(netags))
recognizer.release()  # 释放模型

# 依存句法分析
par_model_path = os.path.join(LTP_DATA_DIR, 'parser.model')  # 依存句法分析模型路径，模型名称为`parser.model`
parser = Parser()  # 初始化实例
parser.load(par_model_path)  # 加载模型
arcs = parser.parse(words, postags)  # 句法分析
print("/".join("%d:%s" % (arc.head, arc.relation) for arc in arcs))
parser.release()  # 释放模型

# 语义角色标注
# srl_model_path = os.path.join(LTP_DATA_DIR, 'pisrl.model')  # 语义角色标注模型目录路径，模型目录为`pisrl.model`
srl_model_path = os.path.join(LTP_DATA_DIR,
                              'pisrl_win.model')  # windows系统要用不同的SRL模型文件，http://ospm9rsnd.bkt.clouddn.com/server/3.4.0/pisrl_win.model或http://model.scir.yunfutech.com/server/3.4.0/pisrl_win.model
labeller = SementicRoleLabeller()  # 初始化实例
labeller.load(srl_model_path)  # 加载模型
roles = labeller.label(words, postags, arcs)  # 语义角色标注
for role in roles:
    print(role.index, "".join(
        ["%s:(%d,%d)" % (arg.name, arg.range.start, arg.range.end) for arg in role.arguments]))
labeller.release()  # 释放模型
```



### 清华THULAC

[THULAC](https://github.com/thunlp/THULAC-Python)（THU Lexical Analyzer for Chinese）由清华大学自然语言处理与社会人文计算实验室研制推出的一套中文词法分析工具包，具有中文分词和词性标注功能。THULAC具有如下几个特点：
- 能力强。利用我们集成的目前世界上规模最大的人工分词和词性标注中文语料库（约含5800万字）训练而成，模型标注能力强大。
- 准确率高。该工具包在标准数据集Chinese Treebank（CTB5）上分词的F1值可达3％，词性标注的F1值可达到9％，与该数据集上最好方法效果相当。
- 速度较快。同时进行分词和词性标注速度为300KB/s，每秒可处理约15万字。只进行分词速度可达到3MB/s。

安装
- pip install thulac
- 安装时会默认下载模型文件。

示例：

```shell
#===========命令行============
#从input.txt读入，并将分词和词性标注结果输出到ouptut.txt中
python -m thulac input.txt output.txt
#如果只需要分词功能，可在增加参数"seg_only" 
python -m thulac input.txt output.txt seg_only
```
Python接口

```python
#=======================
import thulac

# 示例1
thu1 = thulac.thulac()  # 默认模式
text = thu1.cut("我爱北京天安门", text=True)  # 进行一句话分词
print(text)

# 示例2
thu2 = thulac.thulac(seg_only=True)  # 只进行分词，不进行词性标注
thu2.cut_f("input.txt", "output.txt")  # 对input.txt文件内容进行分词，输出到output.txt
# 完整格式
thulac(user_dict = None, model_path = None, T2S = False, seg_only = False, filt = False, max_length = 50000, deli='_', rm_space=False)

```
参数：
- user_dict：设置用户词典，用户词典中的词会被打上uw标签。词典中每一个词一行，UTF8编码
- model_path：设置模型文件所在文件夹，默认为models/
- T2S：默认False, 是否将句子从繁体转化为简体
- seg_only：默认False, 时候只进行分词，不进行词性标注
- filt：默认False, 是否使用过滤器去除一些没有意义的词语，例如“可以”。
- max_length：最大长度
- deli：默认为‘_’, 设置词与词性之间的分隔符
- rm_space：默认为False, 是否去掉原文本中的空格后再进行分词
- text：默认为False, 是否返回文本，不返回文本则返回一个二维数组([[word, tag]..]),seg_only模式下tag为空字符。


### 斯坦福stanfordNLP

多个版本：
- Stanford CoreNLP的源代码[stanfordnlp](https://stanfordnlp.github.io/stanfordnlp/)是使用Java写的，提供了Server方式进行交互。
  - pip install stanfordnlp
- [stanfordcorenlp](https://stanfordnlp.github.io/CoreNLP/)（推荐使用）：一个对Stanford CoreNLP进行了封装的Python工具包。

安装流程：
- 安装Python包：pip install stanfordcorenlp
- 下载安装JDK（版本要求JDK 1.8以上）
- 下载安装[Stanford CoreNLP文件即语言包](https://github.com/stanfordnlp/CoreNLP)。解压CoreNLP文件，并将中文包放在解压文件的根目录。

python调用

```python
from stanfordcorenlp import StanfordCoreNLP

nlp = StanfordCoreNLP('D:\\stanford-corenlp-full-2018-02-27', lang='zh')
sentence = '斯坦福大学自然语言处理包StanfordNLP'
print(nlp.word_tokenize(sentence))  # 分词, ['斯坦福', '大学', '自然', '语言', '处理', '包', 'StanfordNLP']
print(nlp.pos_tag(sentence))  # 词性标注, [('斯坦福', 'NR'), ('大学', 'NN'), ('自然', 'AD'), ('语言', 'NN'), ('处理', 'VV'), ('包', 'NN'), ('StanfordNLP', 'NN')]
print(nlp.ner(sentence))  # 实体识别, [('斯坦福', 'ORGANIZATION'), ('大学', 'ORGANIZATION'), ('自然', 'O'), ('语言', 'O'), ('处理', 'O'), ('包', 'O'), ('StanfordNLP', 'O')]
print(nlp.parse(sentence))  # 语法树, 略
print(nlp.dependency_parse(sentence))  # 依存句法, [('ROOT', 0, 5), ('compound:nn', 2, 1), ('nsubj', 5, 2), ('advmod', 5, 3), ('nsubj', 5, 4), ('compound:nn', 7, 6), ('dobj', 5, 7)]

#========服务方式调用=========
# 启动服务：java -mx6g -cp "*" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -timeout 5000
from stanfordcorenlp import StanfordCoreNLP
nlp = StanfordCoreNLP('http://localhost', port=9000, lang='zh')
sentence = '斯坦福大学自然语言处理包StanfordNLP'
print(nlp.word_tokenize(sentence))  # 分词
print(nlp.pos_tag(sentence))  # 词性标注
print(nlp.ner(sentence))  # 实体识别
print(nlp.parse(sentence))  # 语法树
print(nlp.dependency_parse(sentence))  # 依存句法
```

### TextBlob——nltk加工

[TextBlob](https://github.com/sloria/TextBlob)是一个用Python编写的开源的文本处理库。是自然语言工具包[NLTK](https://www.biaodianfu.com/nltk.html)的一个包装器，目的是抽象其复杂性。它可以用来执行很多自然语言处理的任务，比如，词性标注，名词性成分提取，情感分析，文本翻译，等等。

主要特性：
- 名词短语提取
- 词性标记
- 情绪分析
- 分类
- 由 Google 翻译提供的翻译和检测
- 标记（将文本分割成单词和句子）
- 词句、短语频率
- 解析
- n-gram
- 词变化（复数和单数化）和词形化
- 拼写校正
- 通过扩展添加新模型或语言
- WordNet 集成

安装
- pip install textblob

```python
from textblob import TextBlob
text = 'I love natural language processing! I am not like fish!'
blob = TextBlob(text) # 可能报错，此时需要安装nltk
```
完整示例

```python
from textblob import TextBlob
from textblob import Word

text = 'I love natural language processing! I am not like fish!'
blob = TextBlob(text)

print(blob.words)  # 分词
print(blob.tags)  # 词性标注
print(blob.noun_phrases)  # 短语抽取

# 分句+计算句子情感值
# 使用TextBlob情感分析的结果，以元组的方式进行返回，形式如(polarity, subjectivity).
# polarity 是一个范围为 [-1.0 , 1.0 ] 的浮点数, 正数表示积极，负数表示消极。
# subjectivity 是一个 范围为 [0.0 , 1.0 ] 的浮点数，其中 0.0 表示 客观，1.0表示主观的。
for sentence in blob.sentences:
    print(sentence + '------>' + str(sentence.sentiment.polarity))

# 词语变形(Words Inflection)
w = Word("apple")
print(w.pluralize())  # 变复数
print(w.pluralize().singularize())  # 变单数

# 词干化(Words Lemmatization)
w = Word('went')
print(w.lemmatize('v'))
w = Word('octopi')
print(w.lemmatize())

# 拼写纠正(Spelling Correction)
sen = 'I lvoe naturl language processing!'
sen = TextBlob(sen)
print(sen.correct())

# 句法分析(Parsing)
text = TextBlob('I lvoe naturl language processing!')
print(text.parse())

# N-Grams
text = TextBlob('I lvoe naturl language processing!')
print(text.ngrams(n=2))
```


### SnowNLP——繁体转换、idf，相似度

[SnowNLP](https://github.com/isnowfy/snownlp)是一个python写的类库，可以方便的处理中文文本内容，是受到了TextBlob的启发而写的，由于现在大部分的自然语言处理库基本都是针对英文的，于是写了一个方便处理中文的类库，并且和TextBlob不同的是，这里没有用NLTK，所有的算法都是自己实现的，并且自带了一些训练好的字典。

主要特性：
- 中文分词（Character-Based Generative Model）
- 词性标注（TnT3-gram 隐马）
- 情感分析（现在训练数据主要是买卖东西时的评价，所以对其他的一些可能效果不是很好，待解决）
- 文本分类（Naive Bayes）
- **转换成拼音**（Trie树实现的最大匹配）
- **繁体转简体**（Trie树实现的最大匹配）
- 提取文本关键词（TextRank算法）
- 提取**文本摘要**（TextRank算法）
- tf，idf
- Tokenization（分割成句子）
- **文本相似**（BM25）

安装：
- pip install snownlp

代码
```python
from snownlp import SnowNLP

s1 = SnowNLP('这个东西真心很赞')
print(s1.words) # ['这个', '东西', '真心', '很', '赞']
print(list(s1.tags)) # [('这个', 'r'), ('东西', 'n'), ('真心', 'd'), ('很', 'd'), ('赞', 'Vg')]
print(s1.sentiments) # 0.9769551298267365
print(s1.pinyin) # ['zhe', 'ge', 'dong', 'xi', 'zhen', 'xin', 'hen', 'zan']

s2 = SnowNLP('「繁體字」「繁體中文」的叫法在臺灣亦很常見。')
print(s2.han) # 「繁体字」「繁体中文」的叫法在台湾亦很常见。

text = '''
自然语言处理是计算机科学领域与人工智能领域中的一个重要方向。
它研究能实现人与计算机之间用自然语言进行有效通信的各种理论和方法。
自然语言处理是一门融语言学、计算机科学、数学于一体的科学。
因此，这一领域的研究将涉及自然语言，即人们日常使用的语言，
所以它与语言学的研究有着密切的联系，但又有重要的区别。
自然语言处理并不是一般地研究自然语言，
而在于研制能有效地实现自然语言通信的计算机系统，
特别是其中的软件系统。因而它是计算机科学的一部分。
'''
s3 = SnowNLP(text)
print(s3.keywords(3)) # ['语言', '自然', '计算机']
print(s3.summary(3)) # ['因而它是计算机科学的一部分', '自然语言处理是计算机科学领域与人工智能领域中的一个重要方向', '自然语言处理是一门融语言学、计算机科学、数学于一体的科学']
print(s3.sentences) # ['自然语言处理是计算机科学领域与人工智能领域中的一个重要方向', '它研究能实现人与计算机之间用自然语言进行有效通信的各种理论和方法', '自然语言处理是一门融语言学、计算机科学、数学于一体的科学', '因此', '这一领域的研究将涉及自然语言', '即人们日常使用的语言', '所以它与语言学的研究有着密切的联系', '但又有重要的区别', '自然语言处理并不是一般地研究自然语言', '而在于研制能有效地实现自然语言通信的计算机系统', '特别是其中的软件系统', '因而它是计算机科学的一部分']

s4 = SnowNLP([['这篇', '文章'], ['那篇', '论文'], ['这个']])
print(s4.tf) # [{'这篇': 1, '文章': 1}, {'那篇': 1, '论文': 1}, {'这个': 1}]
print(s4.idf) # {'这篇': 0.5108256237659907, '文章': 0.5108256237659907, '那篇': 0.5108256237659907, '论文': 0.5108256237659907, '这个': 0.5108256237659907}
print(s4.sim([u'文章'])) # [0.4686473612532025, 0, 0]
```


### DeepNLP——神经网络版

[DeepNLP](https://github.com/rockingdingo/deepnlp)项目是基于Tensorflow平台的一个python版本的NLP套装, 目的在于将Tensorflow深度学习平台上的模块，结合 最新的一些算法，提供NLP基础模块的支持，并支持其他更加复杂的任务的拓展，如生成式文摘等等。

NLP 套装模块
- 分词 Word Segmentation/Tokenization：线性链条件随机场 Linear Chain CRF, 基于CRF++包来实现
- 词性标注 Part-of-speech (POS)：单向LSTM/ 双向BI-LSTM, 基于Tensorflow实现
- 命名实体识别 Named-entity-recognition(NER)：单向LSTM/ 双向BI-LSTM/ LSTM-CRF 结合网络, 基于Tensorflow实现
- 依存句法分析 Dependency Parsing (Parse)：基于arc-standard system的神经网络的parser
- 自动生成式文摘 Textsum (Seq2Seq-Attention)
- 关键句子抽取 Textrank
- 文本分类 Textcnn (WIP)
- 预训练模型：
  - 中文: 基于人民日报语料和微博混合语料: 分词, 词性标注, 实体识别
- 可调用 Web Restful API
- 计划中: 句法分析 Parsing

```python
# pip install deepnlp
import deepnlp
# Download all the modules
deepnlp.download()
# Download specific module
deepnlp.download('segment')
deepnlp.download('pos')
deepnlp.download('ner')
deepnlp.download('parse')
# Download module and domain-specific model
deepnlp.download(module = 'pos', name = 'en') 
deepnlp.download(module = 'ner', name = 'zh_entertainment')
```

示例

```python
from deepnlp import segmenter, pos_tagger, ner_tagger, nn_parser
from deepnlp import pipeline

# 分词模块
tokenizer = segmenter.load_model(name='zh')
text = "我爱吃北京烤鸭"
seg_list = tokenizer.seg(text)
text_seg = " ".join(seg_list)
print(text_seg)

# 词性标注
p_tagger = pos_tagger.load_model(name='zh')
tagging = p_tagger.predict(seg_list)
for (w, t) in tagging:
    pair = w + "/" + t
print(pair)

# 命名实体识别
n_tagger = ner_tagger.load_model(name='zh')  # Base LSTM Based Model
tagset_entertainment = ['city', 'district', 'area']
tagging = n_tagger.predict(seg_list, tagset=tagset_entertainment)
for (w, t) in tagging:
    pair = w + "/" + t
    print(pair)

# 依存句法分析
parser = nn_parser.load_model(name='zh')
words = ['它', '熟悉', '一个', '民族', '的', '历史']
tags = ['r', 'v', 'm', 'n', 'u', 'n']
dep_tree = parser.predict(words, tags)
num_token = dep_tree.count()
print("id\tword\tpos\thead\tlabel")
for i in range(num_token):
    cur_id = int(dep_tree.tree[i + 1].id)
    cur_form = str(dep_tree.tree[i + 1].form)
    cur_pos = str(dep_tree.tree[i + 1].pos)
    cur_head = str(dep_tree.tree[i + 1].head)
    cur_label = str(dep_tree.tree[i + 1].deprel)
    print("%d\t%s\t%s\t%s\t%s" % (cur_id, cur_form, cur_pos, cur_head, cur_label))

# Pipeline
p = pipeline.load_model('zh')
text = "我爱吃北京烤鸭"
res = p.analyze(text)
print(res[0])
print(res[1])
print(res[2])
words = p.segment(text)
pos_tagging = p.tag_pos(words)
ner_tagging = p.tag_ner(words)
print(list(pos_tagging))
print(ner_tagging)
```

### 小明NLP——拼写检查、偏旁部首

小明NLP [xmnlp](https://github.com/SeanLee97/xmnlp)的主要功能：
- 中文分词 & 词性标注
- 支持繁體
- 支持自定义词典
- 中文拼写检查
- 文本摘要 & 关键词提取
- 情感分析
- 文本转拼音
- 获取汉字偏旁部首

```python
mport xmnlp
print(xmnlp.radical('自然语言处理'))
print(xmnlp.radical('自然語言處理'))
```

### spaCy——（工业界）

[spaCy](https://spacy.io/) 是一个Python自然语言处理工具包，诞生于2014年年中，号称“Industrial-Strength Natural Language Processing in Python”，是具有工业级强度的Python NLP工具包。spaCy里大量使用了 Cython 来提高相关模块的性能，这个区别于学术性质更浓的NLTK，因此具有了业界应用的实际价值。

主要特性：
- 分词
- 命名实体识别
- 多语言支持（号称支持53种语言）
- 针对11种语言的23种统计模型
- 预训练词向量
- 高性能
- 轻松的整合深度学习
- 词性标注
- 依存句法分析
- 句法驱动的句子切分
- 用于语法和命名实体识别的内置可视化工具
- 方便的字符串到哈希映射
- 导出到numpy数据数组
- 高效的二进制序列化
- 易于模型打包和部署
- 稳健，精确评估

先执行包的安装：pip install spacy，再执行数据集和[模型](https://spacy.io/models)的下载。
- 英文：python -m spacy download en_core_web_sm
- 无官方中文，小道中文版
  - 模型：https://github.com/howl-anderson/Chinese_models_for_SpaCy
  - 执行：pip install ./zh_core_web_sm-2.0.5.tar.gz

```python
import spacy

nlp = spacy.load("en_core_web_sm") # 英文
nlp = spacy.load("zh_core_web_sm") # 中文

doc = nlp("王小明在北京的清华大学读书")
# 分词 词性标注
for token in doc:
    print(token, token.pos_, token.pos)
# 命名实体识别（NER）
for ent in doc.ents:
    print(ent, ent.label_, ent.label)
# 名词短语提取
for np in doc.noun_chunks:
    print(np)
# 依存关系
for token in doc:
    print(token.text, token.dep_, token.head)
# 文本相似度
doc1 = nlp(u"my fries were super gross")
doc2 = nlp(u"such disgusting fries")
similarity = doc1.similarity(doc2)
print(similarity)
# 句法树展示
for token in doc:
    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,
          token.shape_, token.is_alpha, token.is_stop, token.has_vector,
          token.ent_iob_, token.ent_type_,
          token.vector_norm, token.is_oov)
spacy.displacy.serve(doc)
```

同时启动web服务

![](https://www.biaodianfu.com/wp-content/uploads/2020/10/displatcy-768x461.png)


### 复旦FoolNLTK——最准（非最快）的开源中文工具包

[FoolNLTK](https://github.com/rockyzhengwu/FoolNLTK) [文章](FoolNLTK：一个便捷的中文处理工具包)介绍, 复旦大学，一个使用双向 LSTM （BiLSTM 模型）构建的便捷的中文处理工具包，该工具不仅可以实现分词、词性标注和命名实体识别，同时还能使用用户自定义字典加强分词的效果。根据该项目所述，这个中文工具包可能不是最快的开源中文分词，但很可能是最准的开源中文分词。
- 基于神经网络的方法，往往使用「字向量 + 双向 LSTM + CRF」模型，利用神经网络来学习特征，将传统 CRF 中的人工特征工程量将到最低。
- ![](https://www.biaodianfu.com/wp-content/uploads/2020/10/BiLSTM.png)

使用示例：
```python
# pip install tensorflow==1.15 # tf版本限制
import fool
sentence = "中文分词测试：我爱北京天安门"
print(fool.cut(sentence))  # 分词, [['中文', '分词', '测试', '：', '我', '爱', '北京', '天安', '门']]
print(fool.pos_cut(sentence))  # 词性标注, [[('中文', 'nz'), ('分词', 'n'), ('测试', 'n'), ('：', 'wm'), ('我', 'r'), ('爱', 'v'), ('北京', 'ns'), ('天安', 'nz'), ('门', 'n')]]
print(fool.analysis(sentence)[1]) # 明名实体识别, [[(9, 15, 'company', '北京天安门')]]
```

### HanLP——支持TensorFlow 2

[HanLP](https://www.hanlp.com/)原先是一个JAVA版本的自然语言处理工具包，在目前的升级中已经支持了Python。当前已经支持基于 TensorFlow 2.x。HanLP具备功能完善、性能高效、架构清晰、语料时新、可自定义的特点。特殊功能：
- 短语提取
- 拼音转换
- 繁简转换
- 文本推荐

## CRF工具包

CRF++工具包，[CRF++: Yet Another CRF toolkit](https://taku910.github.io/crfpp/)

### （1）编译安装

步骤：[python调用CRF++工具包](https://www.jianshu.com/p/9a8425110f43)

```shell
# 拉取github上的源文件
git clone https://github.com/taku910/crfpp.git
cd crfpp

# 去除找不到winmain.h的错误 [2021-7-15] sed命令有误，手工删除winmain引用即可
sed -i '/#include "winmain.h"/d' crf_test.cpp
sed -i '/#include "winmain.h"/d' crf_learn.cpp

# 编译安装
./configure
make && make install

# 配置文件并导入，消除【错误2】 【2021-7-15】mac下没有ld.so.conf文件
echo "include /usr/local/lib" >> /etc/ld.so.conf
/sbin/ldconfig -v
# 导入python
cd python
python3 setup.py install

```

###  （2）直接安装

命令：
- pip install crfpy

### 使用

- 训练模型
  - crf_learn template train.data model 
- 测试使用

```python
import CRFPP
import codecs

# 加载模型
tagger=CRFPP.Tagger(r'-m C:\Users\secoo\Desktop\CRF++\model')

# 加载测试文件
input_data = codecs.open(r'C:\Users\secoo\Desktop\CRF++\shangde.txt', 'r', 'utf-8')
word_str=input_data.readlines()
word_str=''.join(word_str)
# 测试
print(tagger.parse(word_str))

```


## python工具包

- 【2021-1-19】[python高效使用统计语言模型kenlm：新词发现、分词、智能纠错等](https://linux.ctolib.com/mattzheng-py-kenlm-model.html)
- kenlm的优点（关于kenlm工具训练统计语言模型）： 训练语言模型用的是传统的“统计+平滑”的方法，使用kenlm这个工具来训练。它快速，节省内存，最重要的是，允许在开源许可下使用多核处理器。 kenlm是一个C++编写的语言模型工具，具有速度快、占用内存小的特点，也提供了Python接口。
- 额外需要加载的库：
   - kenlm
   - pypinyin
   - pycorrector
- 粗略整理代码: 
   - kenlm：[mattzheng/py-kenlm-model](https://github.com/mattzheng/py-kenlm-model)
      - 安装：pip install https://github.com/kpu/kenlm/archive/master.zip
   - 新词发现：[mattzheng/word-discovery](https://github.com/mattzheng/word-discovery)
- kenlm模型训练，详见[地址](https://www.cnblogs.com/zidiancao/p/6067147.html)
  - 文件必须是分词以后的文件。
  - -o后面的5表示的是5-gram,一般取到3即可，但可以结合自己实际情况判断

```shell
# 训练命令
bin/lmplz -o 5 --verbose_header --text data/chat_log.txt --arpa result/log.arpa --vocab_file result/log.vocab
# 结果后得到arpa文件
```

### kenlm模式

- 【2021-4-20】[Kenlm文本流畅度检测](https://zhuanlan.zhihu.com/p/265677864)

```python
import kenlm
model = kenlm.Model('lm/test.arpa')
print(model.score('this is a sentence .', bos = True, eos = True))
# 豆瓣语料模型
model = kenlm.Model('build/my_model/douban.arpa')
test_list = ["我是一名程序员",    "是一名程序员我",    "我 是 一名 程序员",    "是 一名 程序员 我",    "一名 是 程序员 我",    "一名 程序员 是 我",    "我 我 我 啊 嗯 你 一名 是 吗 噢噢 程序员 是 吗 我",    "我 等一下 去 交 进去 的 觉得 我 不卡 我 要 能 给我 媳妇 上 了 取得 这样的 他 方便 一点 他 就 觉得 9个人 要 办个 手续 这样 的"]
for i in test_list:
   time_start = time.time()
   print(i, ': ', model.score(i, bos=True, eos=True), 'time: ', time.time() - time_start)
# 结果见下图

#Stateful query 状态转移概率
state = kenlm.State()
state2 = kenlm.State()
#Use <s> as context.  If you don't want <s>, use model.NullContextWrite(state).
model.BeginSentenceWrite(state)
```

示例：
![](https://pic1.zhimg.com/80/v2-f9277d33e552b3ab71ffc950ad3aa2ac_720w.png)

### pypinyin拼音模块

- 拼音模块涉及到了pypinyin，用来识别汉字的拼音，还有非常多种的模式：

```python
from pypinyin import lazy_pinyin, Style
	# Python 中拼音库 PyPinyin 的用法
	# https://blog.csdn.net/devcloud/article/details/95066038

tts = ['BOPOMOFO', 'BOPOMOFO_FIRST', 'CYRILLIC', 'CYRILLIC_FIRST', 'FINALS', 'FINALS_TONE',
 'FINALS_TONE2', 'FINALS_TONE3', 'FIRST_LETTER', 'INITIALS', 'NORMAL', 'TONE', 'TONE2', 'TONE3']
for tt in tts:
    print(tt,lazy_pinyin('聪明的小兔子吃', style=eval('Style.{}'.format(tt))   ))
```

其中结果为：
```shell
BOPOMOFO ['ㄘㄨㄥ', 'ㄇㄧㄥˊ', 'ㄉㄜ˙', 'ㄒㄧㄠˇ', 'ㄊㄨˋ', 'ㄗ˙', 'ㄔ']
BOPOMOFO_FIRST ['ㄘ', 'ㄇ', 'ㄉ', 'ㄒ', 'ㄊ', 'ㄗ', 'ㄔ']
CYRILLIC ['цун1', 'мин2', 'дэ', 'сяо3', 'ту4', 'цзы', 'чи1']
CYRILLIC_FIRST ['ц', 'м', 'д', 'с', 'т', 'ц', 'ч']
FINALS ['ong', 'ing', 'e', 'iao', 'u', 'i', 'i']
FINALS_TONE ['ōng', 'íng', 'e', 'iǎo', 'ù', 'i', 'ī']
FINALS_TONE2 ['o1ng', 'i2ng', 'e', 'ia3o', 'u4', 'i', 'i1']
FINALS_TONE3 ['ong1', 'ing2', 'e', 'iao3', 'u4', 'i', 'i1']
FIRST_LETTER ['c', 'm', 'd', 'x', 't', 'z', 'c']
INITIALS ['c', 'm', 'd', 'x', 't', 'z', 'ch']
NORMAL ['cong', 'ming', 'de', 'xiao', 'tu', 'zi', 'chi']
TONE ['cōng', 'míng', 'de', 'xiǎo', 'tù', 'zi', 'chī']
TONE2 ['co1ng', 'mi2ng', 'de', 'xia3o', 'tu4', 'zi', 'chi1']
TONE3 ['cong1', 'ming2', 'de', 'xiao3', 'tu4', 'zi', 'chi1']
```

可以看出不同的style可以得到不同拼音形式。

### pycorrector纠错模块

- pycorrector的detect，可以返回，错误字的信息

```python
import pycorrector
sentence = '这瓶洗棉奶用着狠不错'
idx_errors = pycorrector.detect(sentence)
# [['这瓶', 0, 2, 'word'], ['棉奶', 3, 5, 'word']]

#correct是专门用来纠正：
pycorrector.correct(sentence)
```

-  pycorrector与kenlm纠错对比
- 来对比一下pycorrector自带的纠错和本次实验的纠错：

```python
import pycorrector
sentence = '这瓶洗棉奶用着狠不错'
idx_errors = pycorrector.detect(sentence)

correct = []
for ide in idx_errors:
    right_word = km.find_best_word(ide[0],ngrams_,freqs = 0)
    if right_word != ide[0]:
        correct.append([right_word] + ide)

print('错误：',idx_errors)
print('pycorrector的结果：',pycorrector.correct(sentence))
print('kenlm的结果：',correct)
```

> 错误： [['这瓶', 0, 2, 'word'], ['棉奶', 3, 5, 'word']]
> pycorrector的结果： ('这瓶洗面奶用着狠不错', [['棉奶', '面奶', 3, 5]])
> kenlm的结果： [['面奶', '棉奶', 3, 5, 'word']]

其他类似的案例：

sentence =  '少先队员因该给老人让坐'

> 错误： [['因该', 4, 6, 'word'], ['坐', 10, 11, 'char']]
> pycorrector的结果： ('少先队员应该给老人让座', [['因该', '应该', 4, 6], ['坐', '座', 10, 11]])
> kenlm的结果： [['应该', '因该', 4, 6, 'word']]
这里笔者的简陋规则暴露问题了，只能对2个字以上的进行判定。

sentence = '绿茶净华可以舒缓痘痘机肤'

> 错误： [['净华', 2, 4, 'word'], ['机肤', 10, 12, 'word']]
> pycorrector的结果： ('绿茶净化可以舒缓痘痘肌肤', [['净华', '净化', 2, 4], ['机肤', '肌肤', 10, 12]])
> kenlm的结果： [['精华', '净华', 2, 4, 'word'], ['肌肤', '机肤', 10, 12, 'word']]


## [nlp-tutorial](https://github.com/graykode/nlp-tutorial)

<p align="center"><img width="100" src="https://upload.wikimedia.org/wikipedia/commons/thumb/1/11/TensorFlowLogo.svg/225px-TensorFlowLogo.svg.png" />  <img width="100" src="https://media-thumbs.golden.com/OLqzmrmwAzY1P7Sl29k2T9WjJdM=/200x200/smart/golden-storage-production.s3.amazonaws.com/topic_images/e08914afa10a4179893eeb07cb5e4713.png" /></p>

`nlp-tutorial` is a tutorial for who is studying NLP(Natural Language Processing) using **TensorFlow** and **Pytorch**. Most of the models in NLP were implemented with less than **100 lines** of code.(except comments or blank lines)



## Curriculum - (Example Purpose)

#### 1. Basic Embedding Model

- 1-1. [NNLM(Neural Network Language Model)](https://github.com/graykode/nlp-tutorial/tree/master/1-1.NNLM) - **Predict Next Word**
  - Paper -  [A Neural Probabilistic Language Model(2003)](http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)
  - Colab - [NNLM_Tensor.ipynb](https://colab.research.google.com/github/graykode/nlp-tutorial/blob/master/1-1.NNLM/NNLM_Tensor.ipynb), [NNLM_Torch.ipynb](https://colab.research.google.com/github/graykode/nlp-tutorial/blob/master/1-1.NNLM/NNLM_Torch.ipynb)
- 1-2. [Word2Vec(Skip-gram)](https://github.com/graykode/nlp-tutorial/tree/master/1-2.Word2Vec) - **Embedding Words and Show Graph**
  - Paper - [Distributed Representations of Words and Phrases
    and their Compositionality(2013)](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)
  - Colab - [Word2Vec_Tensor(NCE_loss).ipynb](https://colab.research.google.com/github/graykode/nlp-tutorial/blob/master/1-2.Word2Vec/Word2Vec_Skipgram_Tensor(NCE_loss).ipynb), [Word2Vec_Tensor(Softmax).ipynb](https://colab.research.google.com/github/graykode/nlp-tutorial/blob/master/1-2.Word2Vec/Word2Vec_Skipgram_Tensor(Softmax).ipynb), [Word2Vec_Torch(Softmax).ipynb](https://colab.research.google.com/github/graykode/nlp-tutorial/blob/master/1-2.Word2Vec/Word2Vec_Skipgram_Torch(Softmax).ipynb)
- 1-3. [FastText(Application Level)](https://github.com/graykode/nlp-tutorial/tree/master/1-3.FastText) - **Sentence Classification**
  - Paper - [Bag of Tricks for Efficient Text Classification(2016)](https://arxiv.org/pdf/1607.01759.pdf)
  - Colab - [FastText.ipynb](https://colab.research.google.com/github/graykode/nlp-tutorial/blob/master/1-3.FastText/FastText.ipynb)

#### 2. CNN(Convolutional Neural Network)

- 2-1. [TextCNN](https://github.com/graykode/nlp-tutorial/tree/master/2-1.TextCNN) - **Binary Sentiment Classification**
  - Paper - [Convolutional Neural Networks for Sentence Classification(2014)](http://www.aclweb.org/anthology/D14-1181)
  - Colab - [TextCNN_Tensor.ipynb](https://colab.research.google.com/github/graykode/nlp-tutorial/blob/master/2-1.TextCNN/TextCNN_Tensor.ipynb), [TextCNN_Torch.ipynb](https://colab.research.google.com/github/graykode/nlp-tutorial/blob/master/2-1.TextCNN/TextCNN_Torch.ipynb)
- 2-2. DCNN(Dynamic Convolutional Neural Network)

#### 3. RNN(Recurrent Neural Network)

- 3-1. [TextRNN](https://github.com/graykode/nlp-tutorial/tree/master/3-1.TextRNN) - **Predict Next Step**
  - Paper - [Finding Structure in Time(1990)](http://psych.colorado.edu/~kimlab/Elman1990.pdf)
  - Colab - [TextRNN_Tensor.ipynb](https://colab.research.google.com/github/graykode/nlp-tutorial/blob/master/3-1.TextRNN/TextRNN_Tensor.ipynb), [TextRNN_Torch.ipynb](https://colab.research.google.com/github/graykode/nlp-tutorial/blob/master/3-1.TextRNN/TextRNN_Torch.ipynb)
- 3-2. [TextLSTM](https://github.com/graykode/nlp-tutorial/tree/master/3-2.TextLSTM) - **Autocomplete**
  - Paper - [LONG SHORT-TERM MEMORY(1997)](https://www.bioinf.jku.at/publications/older/2604.pdf)
  - Colab - [TextLSTM_Tensor.ipynb](https://colab.research.google.com/github/graykode/nlp-tutorial/blob/master/3-2.TextLSTM/TextLSTM_Tensor.ipynb), [TextLSTM_Torch.ipynb](https://colab.research.google.com/github/graykode/nlp-tutorial/blob/master/3-2.TextLSTM/TextLSTM_Torch.ipynb)
- 3-3. [Bi-LSTM](https://github.com/graykode/nlp-tutorial/tree/master/3-3.Bi-LSTM) - **Predict Next Word in Long Sentence**
  - Colab - [Bi_LSTM_Tensor.ipynb](https://colab.research.google.com/github/graykode/nlp-tutorial/blob/master/3-3.Bi-LSTM/Bi_LSTM_Tensor.ipynb), [Bi_LSTM_Torch.ipynb](https://colab.research.google.com/github/graykode/nlp-tutorial/blob/master/3-3.Bi-LSTM/Bi_LSTM_Torch.ipynb)

#### 4. Attention Mechanism

- 4-1. [Seq2Seq](https://github.com/graykode/nlp-tutorial/tree/master/4-1.Seq2Seq) - **Change Word**
  - Paper - [Learning Phrase Representations using RNN Encoder–Decoder
    for Statistical Machine Translation(2014)](https://arxiv.org/pdf/1406.1078.pdf)
  - Colab - [Seq2Seq_Tensor.ipynb](https://colab.research.google.com/github/graykode/nlp-tutorial/blob/master/4-1.Seq2Seq/Seq2Seq_Tensor.ipynb), [Seq2Seq_Torch.ipynb](https://colab.research.google.com/github/graykode/nlp-tutorial/blob/master/4-1.Seq2Seq/Seq2Seq_Torch.ipynb)
- 4-2. [Seq2Seq with Attention](https://github.com/graykode/nlp-tutorial/tree/master/4-2.Seq2Seq(Attention)) - **Translate**
  - Paper - [Neural Machine Translation by Jointly Learning to Align and Translate(2014)](https://arxiv.org/abs/1409.0473)
  - Colab - [Seq2Seq(Attention)_Tensor.ipynb](https://colab.research.google.com/github/graykode/nlp-tutorial/blob/master/4-2.Seq2Seq(Attention)/Seq2Seq(Attention)_Tensor.ipynb), [Seq2Seq(Attention)_Torch.ipynb](https://colab.research.google.com/github/graykode/nlp-tutorial/blob/master/4-2.Seq2Seq(Attention)/Seq2Seq(Attention)_Torch.ipynb)
- 4-3. [Bi-LSTM with Attention](https://github.com/graykode/nlp-tutorial/tree/master/4-3.Bi-LSTM(Attention)) - **Binary Sentiment Classification**
  - Colab - [Bi_LSTM(Attention)_Tensor.ipynb](https://colab.research.google.com/github/graykode/nlp-tutorial/blob/master/4-3.Bi-LSTM(Attention)/Bi_LSTM(Attention)_Tensor.ipynb), [Bi_LSTM(Attention)_Torch.ipynb](https://colab.research.google.com/github/graykode/nlp-tutorial/blob/master/4-3.Bi-LSTM(Attention)/Bi_LSTM(Attention)_Torch.ipynb)


#### 5. Model based on Transformer

- 5-1.  [The Transformer](https://github.com/graykode/nlp-tutorial/tree/master/5-1.Transformer) - **Translate**
  - Paper - [Attention Is All You Need(2017)](https://arxiv.org/abs/1706.03762)
  - Colab - [Transformer_Torch.ipynb](https://colab.research.google.com/github/graykode/nlp-tutorial/blob/master/5-1.Transformer/Transformer_Torch.ipynb), [Transformer(Greedy_decoder)_Torch.ipynb](https://colab.research.google.com/github/graykode/nlp-tutorial/blob/master/5-1.Transformer/Transformer(Greedy_decoder)_Torch.ipynb)
- 5-2. [BERT](https://github.com/graykode/nlp-tutorial/tree/master/5-2.BERT) - **Classification Next Sentence & Predict Masked Tokens**
  - Paper - [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding(2018)](https://arxiv.org/abs/1810.04805)
  - Colab - [BERT_Torch.ipynb](https://colab.research.google.com/github/graykode/nlp-tutorial/blob/master/5-2.BERT/BERT_Torch.ipynb)

|           Model            |              Example               |   Framework   | Lines(torch/tensor) |
| :------------------------: | :--------------------------------: | :-----------: | :-----------------: |
|            NNLM            |         Predict Next Word          | Torch, Tensor |        67/83        |
|     Word2Vec(Softmax)      |   Embedding Words and Show Graph   | Torch, Tensor |        77/94        |
|          TextCNN           |      Sentence Classification       | Torch, Tensor |        94/99        |
|          TextRNN           |         Predict Next Step          | Torch, Tensor |        70/88        |
|          TextLSTM          |            Autocomplete            | Torch, Tensor |        73/78        |
|          Bi-LSTM           | Predict Next Word in Long Sentence | Torch, Tensor |        73/78        |
|          Seq2Seq           |            Change Word             | Torch, Tensor |       93/111        |
|   Seq2Seq with Attention   |             Translate              | Torch, Tensor |       108/118       |
|   Bi-LSTM with Attention   |  Binary Sentiment Classification   | Torch, Tensor |       92/104        |
|        Transformer         |             Translate              |     Torch     |        222/0        |
| Greedy Decoder Transformer |             Translate              |     Torch     |        246/0        |
|            BERT            |            how to train            |     Torch     |        242/0        |


## Dependencies

- Python 3.5+
- Tensorflow 1.12.0+
- Pytorch 0.4.1+
- Plan to add Keras Version



# 应用

## 项目实战

- 【2020-6-2】[京东AI项目实战课](http://finance.sina.com.cn/wm/2020-06-02/doc-iircuyvi6299111.shtml)：五个阶段从理论到实践、从项目实战到面试准备的一站式教学，涵盖NLP领域核心技能（特征工程、分类模型、语法树等），前沿技术（BERT、XLNet、Seq2Seq、Transformer、ALBERT、模型蒸馏、模型压缩等）；
- ![](http://n.sinaimg.cn/sinakd202062s/263/w1000h4063/20200602/f2ae-iumkapw2054484.png)
- ![](http://n.sinaimg.cn/sinakd202062s/612/w1000h5212/20200602/0036-iumkapw2054582.jpg)

## 资料

- 更多[Demo地址](http://wqw547243068.github.io/demo)

# 结束


