---
layout: post
title:  "自然语言处理-NLP"
date:   2020-05-01 21:50:00
categories: 自然语言处理
tags: 深度学习 NLP 自然语言处理 文本摘要 唐杰
excerpt: NLP各类知识点及工程落地方法汇总
author: 鹤啸九天
mathjax: true
---

* content
{:toc}

# 总结

- 【2020-6-11】阿里员工开发的[论文知识图谱](https://www.connectedpapers.com/)
   - ![](http://p1.pstatp.com/large/pgc-image/37054b2db9b64394a73feecfa9ad024d)
- 【2020-9-5】[文本摘要综述](https://github.com/xcfcode/What-I-Have-Read/blob/master/slides/presentation/%E6%96%87%E6%9C%AC%E6%91%98%E8%A6%81%E7%AE%80%E8%BF%B0.pdf)
   - ACL 2020论文：Extractive Summarization as Text Matching，[文本摘要新框架，抽取式摘要“轻松”取得SOTA](https://zhuanlan.zhihu.com/p/133096909)，打破原有的解决抽取式摘要的思路，这里提出了一个全新的范式：将抽取式摘要任务转化为一个语义匹配的问题。代码[MatchSum](https://github.com/maszhongming/MatchSum)，eight Tesla-V100-16G GPUs to train our model, the training time is about 30 hours
   - [Text Summarization on WikiHow](https://paperswithcode.com/sota/text-summarization-on-wikihow)，第一名[代码](https://github.com/nlpyang/PreSumm)
- 【2020-8-9】[NLP 中的各种不正确打开方式](https://www.bilibili.com/video/BV1ha4y1J71Q/)（反向模式 Anti-Pattern），总结的真好。[原文](https://docs.google.com/presentation/d/e/2PACX-1vSINTutKveUSVpj-FwpTz1Kkau7JC7d6ZKmGRqHyWf81Lj7dQkcr9dy9eL7HACD9NhvSH37lMtmqelh/pub?slide=id.g8a66028a04_0_50 )，也推荐作者的[博客](http://medium.com/modern-nlp)​​​​
- 【2021-1-3】Google 的AI团队在2020年底出品的《高性能自然语言处理》，综述了NLP的基础技术、核心技术、案例实践。
   - [EMNLP_2020_Tutorial__High_Performance_NLP](http://gabrielilharco.com/publications/EMNLP_2020_Tutorial__High_Performance_NLP.pdf)

## 工具汇总

- 【2020-8-27】NLP模型可视化工具LIT，为什么模型做出这样的预测？什么时候性能不佳？在输入变化可控的情况下会发生什么？LIT 将局部解释、聚合分析和反事实生成集成到一个流线型的、基于浏览器的界面中，以实现快速探索和错误分析。
   - [谷歌开源NLP模型可视化工具LIT，模型训练不再「黑箱」](https://www.toutiao.com/i6865152251150008844/)
   - 支持多种自然语言处理任务，包括探索情感分析的反事实、度量共指系统中的性别偏见，以及探索文本生成中的局部行为。
   - 此外 LIT 还支持多种模型，包括分类、seq2seq 和结构化预测模型。并且它具备高度可扩展性，可通过声明式、框架无关的 API 进行扩展。
   - [论文地址](https://arxiv.org/pdf/2008.05122.pdf)
   - [项目地址](https://github.com/PAIR-code/lit)，[模块操作细节](https://github.com/PAIR-code/lit/blob/main/docs/user_guide.md)
   - ![](https://p6-tt.byteimg.com/origin/pgc-image/a8703a8db04649b19e12ef152783bb4a?from=pc)

# 发展历史

- 微软亚洲研究院成立20周年时表示：[NLP将迎来黄金十年](https://www.msra.cn/zh-cn/news/executivebylines/tech-bylines-nlp)。
   - 比尔·盖茨曾说过，“语言理解是人工智能皇冠上的明珠”。自然语言处理（NLP，Natural Language Processing）的进步将会推动人工智能整体进展。
   - NLP的历史几乎跟计算机和人工智能（AI）的历史一样长。
- 回顾基于深度学习的NLP技术的重大进展，从时间轴来看主要包括：
   - NNLM(2003)
   - Word Embeddings(2013)
   - Seq2Seq(2014)
   - Attention(2015)
   - Memory-based networks(2015)
   - Transformer(2017)
   - BERT(2018)
   - XLNet(2019)
- 预训练语言模型发展历史
   - 摘自：[nlp中的预训练语言模型总结(单向模型、BERT系列模型、XLNet)](https://zhuanlan.zhihu.com/p/76912493)
![](https://pic3.zhimg.com/80/v2-d8fd47547f5a8230372ceaa894e52feb_720w.jpg)

- 【2021-1-17】Impressive progress of deep learning on unsupervised text corpora，[A Review of the Neural History of Natural Language Processing](https://ruder.io/a-review-of-the-recent-history-of-nlp/)
   - 2001 - Neural language models
   - 2008 - Multi-task learning
   - 2013 - Word embeddings
   - 2013 - Neural networks for NLP
   - 2014 - Sequence-to-sequence models
   - 2015 - Attention
   - 2015 - Memory-based networks
   - 2018 - Pretrained language models

# 算法理论


## 自监督学习

- 【2020-6-21】[NLP中的自监督表示学习，全是动图，很过瘾的](https://www.toutiao.com/i6839892851711541764/)，[英文原文](https://amitness.com/2020/05/self-supervised-learning-nlp/)
- 自监督的方法的核心是一个叫做 “pretext task” 的框架，它允许我们使用数据本身来生成标签，并使用监督的方法来解决非监督的问题。这些也被称为“auxiliary task”或“pre-training task“。通过执行此任务获得的表示可以用作我们的下游监督任务的起点。
   - ![](http://p3.pstatp.com/large/pgc-image/47ba10919c1440c781c0b14f1c14de82)
- 【2020-7-12】刘知远新书：[Representation Learning for Natural Language Processing](http://nlp.csai.tsinghua.edu.cn/news/%E4%B8%93%E8%91%97representation-learning-for-natural-language-processing%E6%AD%A3%E5%BC%8F%E5%87%BA%E7%89%88/)，[电子版下载](https://link.springer.com/book/10.1007%2F978-981-15-5573-2)

- 【2020-7-21】自监督学习综述（清华唐杰团队）：Self-supervised Learning: Generative or Contrastive
<iframe src="https://view.officeapps.live.com/op/embed.aspx?src=https%3A%2F%2Fstatic%2Eaminer%2Ecn%3A443%2Fupload%2Fppt%2F332%2F651%2F1679%2F5ee8986f91e011e66831c59b%5F1%2Epptx&amp;wdAr=1.4440104166666667" width="350px" height="266px" frameborder="0">这是嵌入 <a target="_blank" href="https://office.com">Microsoft Office</a> 演示文稿，由 <a target="_blank" href="https://office.com/webapps">Office</a> 提供支持。</iframe>


### 自监督的方案

- 1. 预测中心词
   - 在这个公式中，我们取一定窗口大小的一小块文本，我们的目标是根据周围的单词预测中心单词。
   - ![](http://p1.pstatp.com/large/pgc-image/b217d92b952d4601937a1629bc867642)
   - 例如，在下面的图中，我们有一个大小为1的窗口，因此我们在中间单词的两边各有一个单词。使用这些相邻的词，我们需要预测中心词。
   - ![](http://p9.pstatp.com/large/pgc-image/62516862aa444fed9885c34aaf955b05)
   - 这个方案已经在著名的Word2Vec论文的“Continuous Bag of Words”方法中使用过。

- 2. 预测邻居词
   - 在这个公式中，我们取一定窗口大小的文本张成的空间，我们的目标是在给定中心词的情况下预测周围的词。
   - ![](http://p3.pstatp.com/large/pgc-image/63df64f4f6af401db506372dc06c0b2f)
   - 这个方案已经在著名的Word2Vec论文的“skip-gram”方法中实现。

- 3. 相邻句子的预测
   - 在这个公式中，我们取三个连续的句子，设计一个任务，其中给定中心句，我们需要生成前一个句子和下一个句子。它类似于之前的skip-gram方法，但适- 用于句子而不是单词。
   - ![](http://p1.pstatp.com/large/pgc-image/6c88f0845d7e4ae38ea626bb3bfd9000)
   - 这个方案已经在Skip-Thought Vectors的论文中使用过。

- 4. 自回归语言建模
   - 在这个公式中，我们取大量未标注的文本，并设置一个任务，根据前面的单词预测下一个单词。因为我们已经知道下一个来自语料库的单词是什么，所以- 我们不需要手工标注的标签。
   - ![](http://p1.pstatp.com/large/pgc-image/bf36697cfb06464c8faf6ab3c88a2493)
   - 例如，我们可以通过预测给定前一个单词的下一个单词来将任务设置为从左到右的语言建模。
   - ![](http://p3.pstatp.com/large/pgc-image/1ddb11e50efa4a6f955bac7e14218b1b)
   - 我们也可以用这个方案来通给定未来的单词预测之前的单词，方向是从右到左。
   - ![](http://p1.pstatp.com/large/pgc-image/bbb35ee25289497a9c40e3eeee901fe0)
   - 这个方案已经使用在许多论文中，从n-gram模型到神经网络模型比如神经概率语言模型 (GPT) 。

- 5. 掩码语言建模
   - 在这个方案中，文本中的单词是随机掩码的，任务是预测它们。与自回归公式相比，我们在预测掩码单词时可以同时使用前一个词和下一个词的上下文。
   - ![](http://p1.pstatp.com/large/pgc-image/a9b530083051422f9f88c1613eff489f)
   - 这个方案已经在BERT、RoBERTa和ALBERT的论文中使用过。与自回归相比，在这个任务中，我们只预测了一小部分掩码词，因此从每句话中学到的东西更少。

- 6. 下一个句子预测
   - 在这个方案中，我们取文件中出现的两个连续的句子，以及同一文件或不同文件中随机出现的另一个句子。
   - ![](http://p3.pstatp.com/large/pgc-image/8d58e6c913dc445d895429086cc50ae9)
   - 然后，任务是区分两个句子是否是连贯的。
   - ![](http://p3.pstatp.com/large/pgc-image/f50ff4fe179a4678926ddcc3769289f0)
   - 在BERT的论文中，它被用于提高下游任务的性能，这些任务需要理解句子之间的关系，比如自然语言推理(NLI)和问题回答。然而，后来的研究对其有效性提出了质疑。

- 7. 句子顺序的预测
   - 在这个方案中，我们从文档中提取成对的连续句子。然后互换这两个句子的位置，创建出另外一对句子。
   - ![](http://p3.pstatp.com/large/pgc-image/0638806390584a488e06c82a11832455)
   - 我们的目标是对一对句子进行分类，看它们的顺序是否正确。
   - ![](http://p3.pstatp.com/large/pgc-image/2ba2388f8dfa4778ba4684a8bbbc57fe)
   - 在ALBERT的论文中，它被用来取代“下一个句子预测”任务。

- 8. 句子重排
   - 在这个方案中，我们从语料库中取出一个连续的文本，并破开的句子。然后，对句子的位置进行随机打乱，任务是恢复句子的原始顺序。
   - ![](http://p1.pstatp.com/large/pgc-image/451d926ff39a4ce6b3f2ef4b97d18469)
   - 它已经在BART的论文中被用作预训练的任务之一。

- 9. 文档旋转
   - 在这个方案中，文档中的一个随机token被选择为旋转点。然后，对文档进行旋转，使得这个token成为开始词。任务是从这个旋转的版本中恢复原来的句子。
   - ![](http://p3.pstatp.com/large/pgc-image/8095d77a0fc440978eb825289a83acf6)
   - 它已经在BART的论文中被用作预训练的任务之一。直觉上，这将训练模型开始识别文档。

- 10. 表情符号预测
   - 这个方案被用在了DeepMoji的论文中，并利用了我们使用表情符号来表达我们所发推文的情感这一想法。如下所示，我们可以使用推特上的表情符号作为标签，并制定一个监督任务，在给出文本时预测表情符号。
   - ![](http://p1.pstatp.com/large/pgc-image/501d4092a3c4433298824a230c07eeb1)
   - DeepMoji的作者们使用这个概念对一个模型进行了12亿条推文的预训练，然后在情绪分析、仇恨语言检测和侮辱检测等与情绪相关的下游任务上对其进行微调。

## 机器翻译

- 【2020-6-5】[机器翻译：统计建模与深度学习方法](https://opensource.niutrans.com/mtbook/index.html)，[ppt地址](https://github.com/NiuTrans/MTBook/blob/master/slides)
- ![](https://opensource.niutrans.com/guideline.png)

- 【2020-6-10】Google官方示例：[基于注意力的神经机器翻译](https://www.tensorflow.org/tutorials/text/nmt_with_attention?hl=zh-cn)
   - ![](https://tensorflow.org/images/spanish-english.png)

## NER 命名实体识别

- 目标：识别序列中的人名、地名、组织机构名等实体。属于序列标注问题。
- [工业界如何解决NER问题？12个trick](https://zhuanlan.zhihu.com/p/152463745)
   - Q1、如何快速有效地提升NER性能（非模型迭代）？
   - Q2、如何在模型层面提升NER性能？
   - Q3、如何构建引入词汇信息（词向量）的NER？
   - Q4、如何解决NER实体span过长的问题？
   - Q5、如何客观看待BERT在NER中的作用？
   - Q6、如何冷启动NER任务？
   - Q7、如何有效解决低资源NER问题？
   - Q8、如何缓解NER标注数据的噪声问题？
   - Q9、如何克服NER中的类别不平衡问题？
   - Q10、如何对NER任务进行领域迁移？
   - Q11、如何让NER系统变得“透明”且健壮？
   - Q12、如何解决低耗时场景下的NER任务？


### 序列标注的几种模式

- 在序列标注中，我们想对一个序列的每一个元素(token)标注一个标签。一般来说，一个序列指的是一个句子，而一个元素(token)指的是句子中的一个词语或者一个字。比如信息提取问题可以认为是一个序列标注问题，如提取出会议时间、地点等。
- 不同的序列标注任务就是将目标句中的字或者词按照需求的方式标记，不同的结果取决于对样本数据的标注，一般序列的标注是要符合一定的标注标准的如([PKU数据标注规范](http://sighan.cs.uchicago.edu/bakeoff2005/data/pku_spec.pdf))。
- 另外, 词性标注、分词都属于同一类问题，他们的区别主要是对序列中的token的标签标注的方式不同。

下面以命名实体识别来举例说明. 我们在进行命名实体识别时，通常对每个字进行标注。中文为单个字，英文为单词，空格分割。

标签类型的定义一般如下：

|定义|	全称|	备注|
|---|---|---|
|B	|Begin	|实体片段的开始|
|I	|Intermediate|	实体片段的中间|
|E	|End	|实体片段的结束|
|S	|Single	|单个字的实体|
|O	|Other/Outside	|其他不属于任何实体的字符(包括标点等)|

### 常见的标签方案

常用的较为流行的标签方案有如下几种：
- IOB1: 标签I用于文本块中的字符，标签O用于文本块之外的字符，标签B用于在该文本块前面接续则一个同类型的文本块情况下的第一个字符。
- IOB2: 每个文本块都以标签B开始，除此之外，跟IOB1一样。
- IOE1: 标签I用于独立文本块中，标签E仅用于同类型文本块连续的情况，假如有两个同类型的文本块，那么标签E会被打在第一个文本块的最后一个字符。
- IOE2: 每个文本块都以标签E结尾，无论该文本块有多少个字符，除此之外，跟IOE1一样。
- START/END （也叫SBEIO、IOBES）: 包含了全部的5种标签，文本块由单个字符组成的时候，使用S标签来表示，由一个以上的字符组成时，首字符总是使用B标签，尾字符总是使用E标签，中间的字符使用I标签。
- IO: 只使用I和O标签，显然，如果文本中有连续的同种类型实体的文本块，使用该标签方案不能够区分这种情况。

其中最常用的是IOB2、IOBS、IOBES。

#### BIO标注模式

将每个元素标注为“B-X”、“I-X”或者“O”。其中，“B-X”表示此元素所在的片段属于X类型并且此元素在此片段的开头，“I-X”表示此元素所在的片段属于X类型并且此元素在此片段的中间位置，“O”表示不属于任何类型。

命名实体识别中每个token对应的标签集合如下:
>LabelSet = {O, B-PER, I-PER, B-LOC, I-LOC, B-ORG, I-ORG}

其中，PER代表人名， LOC代表位置， ORG代表组织. B-PER、I-PER代表人名首字、人名非首字，B-LOC、I-LOC代表地名(位置)首字、地名(位置)非首字，B-ORG、I-ORG代表组织机构名首字、组织机构名非首字，O代表该字不属于命名实体的一部分。

![](https://sthsf.github.io/2020/02/18/NLP--%E5%91%BD%E5%90%8D%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB/BIO.jpg)

对于词性标注, 则可以用{B-NP, I-NP}给序列中的名词token打标签

#### BIOES标注模式

BIOES标注模式就是在BIO的基础上增加了单字符实体和字符实体的结束标识, 即

>LabelSet = {O, B-PER, I-PER, E-PER, S-PER, B-LOC, I-LOC, E-LOC, S-LOC, B-ORG, I-ORG, E-ORG, S-ORG}

![](https://sthsf.github.io/2020/02/18/NLP--%E5%91%BD%E5%90%8D%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB/BIOES.jpg)

根据标注的复杂度, 还有会在其中添加其他的比如MISC之类的实体, 如

>LabelSet = {O ,B-MISC, I-MISC, B-ORG ,I-ORG, B-PER ,I-PER, B-LOC ,I-LOC]。
其中，一般一共分为四大类：PER（人名），LOC（位置[地名]），ORG（组织）以及MISC(杂项)，而且B表示开始，I表示中间，O表示不是实体。

其他类似的标注方式:

标注方式1:
>LabelSet = {BA, MA, EA, BO, MO, EO, BP, MP, EP, O}
其中，
- BA代表这个汉字是地址首字，MA代表这个汉字是地址中间字，EA代表这个汉字是地址的尾字；
- BO代表这个汉字是机构名的首字，MO代表这个汉字是机构名称的中间字，EO代表这个汉字是机构名的尾字；
- BP代表这个汉字是人名首字，MP代表这个汉字是人名中间字，EP代表这个汉字是人名尾字，而O代表这个汉字不属于命名实体。

标注方式2:
> LabelSet = {NA, SC, CC, SL, CL, SP, CP}
其中 NA = No entity, SC = Start Company, CC = Continue Company, SL = Start Location, CL = Continue Location, SP = Start Person, CP = Continue Person

上面两种标注方式与BIO和BIEOS类似, 只是使用不同的标签字符来标识而已.

### 参考资料

- [命名实体识别(Name Entity Recognition)综述](https://sthsf.github.io/2020/02/18/NLP--%E5%91%BD%E5%90%8D%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB/)


## 短语挖掘

### 关键词提取方案

- 知乎话题：[关键词提取都有哪些方案](https://www.zhihu.com/question/21104071)

- 刘知远的博士论文：[基于文档主题结构的关键词抽取方法研究](http://nlp.csai.tsinghua.edu.cn/~lzy/publications/phd_thesis.pdf)
- 关键词挖掘的方法：（2014年，作者：[zibuyu9](https://www.zhihu.com/question/21104071/answer/24556905)）
   - 1. TFIDF是很强的baseline，具有较强的普适性，如果没有太多经验的话，可以实现该算法基本能应付大部分关键词抽取的场景了。
   - 2. 对于中文而言，中文分词和词性标注的性能对关键词抽取的效果至关重要。
   - 3. 较复杂的算法各自有些问题，如Topic Model，它的主要问题是抽取的关键词一般过于宽泛，不能较好反映文章主题。这在我的博士论文中有专门实验和论述；TextRank实际应用效果并不比TFIDF有明显优势，而且由于涉及网络构建和随机游走的迭代算法，效率极低。这些复杂算法集中想要解决的问题，是如何利用更丰富的文档外部和内部信息进行抽取。如果有兴趣尝试更复杂的算法，我认为我们提出的基于SMT（统计机器翻译）的模型，可以较好地兼顾效率和效果。
      - TextRank源于page-rank，page-rank是谷歌提出的对网页按照影响力进行排序的算法。同样的，text-rank认为文档或句子中相邻的词语重要性是相互影响的，所以text-rank引入了词语的顺序信息。
      - ![](https://pic4.zhimg.com/80/v2-192504d9afdc37816ceb8e6c8e7649f2_720w.jpg)
   - 4. 以上都是无监督算法，即没有事先标注好的数据集合。而如果我们有事先标注好的数据集合的话，就可以将关键词抽取问题转换为有监督的分类问题。这在我博士论文中的相关工作介绍中均有提到。
   - 从性能上来讲，利用有监督模型的效果普遍要优于无监督模型，对关键词抽取来讲亦是如此。在Web 2.0时代的社会标签推荐问题，就是典型的有监督的关键词推荐问题，也是典型的多分类、多标签的分类问题，有很多高效算法可以使用。

- 作者：[小Fan](https://www.zhihu.com/question/21104071/answer/291420205)
- 分两步走：
   - `候选词匹配`：基于关键词词库的多模式匹配得到候选，这里最重要的工作是词库构建，往往会融合多种方法：垂直站点专有名词，百科词条，输入法细胞词库，广告主购买词，基于大规模语料库的自动词库挖掘（推荐韩家炜团队的 shangjingbo1226/SegPhrase ，shangjingbo1226/AutoPhrase 方法）等。这里会涉及大量的数据清洗工作，甚至还可以有一个质量分类器决定哪些词条可以进入词库。
   - `候选词相关性排序`：包括无监督和有监督方法，如下：
      - `无监督方法`：常见的有 TFIDF（需要统计 phrase 级别的 DF）， textrank（优势不明显，计算量大，慎用），topic 相似度（参见 baidu/Familia），embedding 相似度（需要训练或计算 keyword 和 doc embedding），TWE 相似度（参见 baidu/Familia）
      - `有监督方法`：常见的有基于统计机器翻译 SMT 的方法（转换成翻译问题，可以采用 IBM Model 1），基于序列标注模型的方法（转换成核心成分识别问题，类似 NER，状态只有0和1，即是否是核心成分，较适用于短文本），基于排序学习LTR的方法（转换成候选词排序问题，采用 pairwise 方法，或者深度语义匹配方法，如 DSSM），基于传统机器学习分类方法（转换成二元或多元分类问题）。有监督方法依赖一定规模的标注数据，效果通常会显著好于无监督方法。


### [韩家炜团队开源工具](https://www.jianshu.com/p/76197e116835)

- 具体工具如下：
   - `TopMine`：频率模式挖掘+统计分析
   - `SegPhrase`：SegPhrase：弱监督、高质量的 Phrase Mining
      - 论文：Mining Quality Phrases from Massive Text Corpora
      - TopMine 的方法完全是无监督的，如果有少量的 Label 数据可能会在很大程度上提高 Topic Model 的结果。
      - SegPhrase框架只需要有限的培训，但是生成的短语的质量却接近于人类的判断力。而且，该方法具有可伸缩性：随着语料库大小的增加，计算时间和所需空间都会线性增长。论文在大型文本语料库上的实验证明了该新方法的质量和效率。
      - segphrase已有[GitHub开源工具](https://github.com/shangjingbo1226/SegPhrase)
   - `AutoPhrase`：自动的 Phrase Mining
      - 论文：Automated Phrase Mining from Massive Text Corpora
      - AutoPhrase支持多种语言（包含简体中文和繁体中文）基本思想是通过分别在训练和解析过程中添加编码/解码层来重用英语实现。在训练阶段，在中文单词片段和英文字母之间创建一个字典，将输入数据编码为英文伪单词。在解析阶段，在识别出编码后的新文档中的优质短语之后，进行解码以恢复可读的中文。
      - AutoPhrase已有[GitHub开源工具](https://github.com/shangjingbo1226/AutoPhrase)

### AutoPhrase

- 【2020-7-3】数据挖掘之父韩家炜团队的自动短语挖掘论文：[Automated Phrase Mining from Massive Text Corpora]()，提出自动挖掘短语的方法：AutoPhrase，[C++代码](https://github.com/shangjingbo1226/AutoPhrase)，[Python包](https://github.com/CS512-Autophrase-Demo/AutophrasePy), [解说](https://blog.csdn.net/weixin_42363527/article/details/102884088)
- 思路：使用通用知识库（KB）的来构造正样本（应该就是用完全匹配的方式），然后训练一个NER模型（非神经网络的），然后用这个NER模型的预测结果来减少负样本噪声，引入词性信息
- 先从KB里匹配出正样本，其他的词是负样本，然后训练NER/CRF模型，再卡个阈值，筛掉分低的实体，最后出的作为抽取出的短语。

对于自动短语挖掘任务，
- 输入：语料库（特定语言和特定领域的文本单词序列，长度任意）和知识库
- 输出：一个按质量递减排列的短语列表

短语质量 定义为一个单词序列成为一个完整语义单元的概率，满足以下条件：
- `流行度`: 在给定的文档集合中，质量短语应该出现的频率足够高。
- `一致性`: 由于偶然因素，令牌在高质量短语中的搭配出现的概率明显高于预期。
- `信息性`: 如果一个短语表达了一个特定的主题或概念，那么这个短语就是信息性的。
- `完备性`: 长频繁短语及其子序列均满足上述3个条件。当一个短语可以在特定的文档上下文中解释为一个完整的语义单元时，它就被认为是完整的

流程图
- ![](https://img-blog.csdnimg.cn/2020042217323277.png)
- AutoPhrase会根据正池和负池对短语质量进行两次评估，一次在短语分割前，一次在短语分割后。也就是说，POS-Guided短语分割需要一组初始的短语质量分数;我们预先根据原始频率估计分数;然后，一旦特征值被纠正，我们重新估计分数。

- `AutoPhrase`超越了分段短语，进一步摆脱了额外的手工标注工作，提高了性能。
- 主要使用以下两种新技术：
   - `Robust Positive-Only Distant Training`（鲁棒正向远程训练）
      - 即：利用已有的知识库（Wikipedia）做远程监督训练
      - 公共知识库（如维基百科）中的高质量短语，免费并且数量很多。在远程训练中，使用一般知识库中高质量短语，可以避免手工标注。
      - 具体做法是：
         - 从一般知识库中构建积极标签的样本
         - 从给定的领域语料库中构建消极标签的样本
         - 训练大量的基本分类器
         - 将这些分类器的预测聚合在一起
   - `POS-Guided Phrasal Segmentation`. （POS-Guided短语分割）
      - 即：利用词性信息来增加抽取的准确性
      - 语言处理器应该权衡：`性能` 和 `领域独立能力`
         - 对于领域独立能力，如果没有语言领域知识，准确性会受限制
         - 对于准确性，依赖复杂的、训练有素的语言分析器，就会降低领域独立能力
      - 解决办法： 在文档集合的语言中加入一个预先训练的词性标记，以进一步提高性能

## 文本生成

- 【2021-1-9】[现代自然语言生成](https://item.jd.com/12785661.html) 黄民烈

### 综述

- 文本生成被称为NLG，目标是根据输入数据生成自然语言的文本。
   - NLP领域使用更多的一般是NLU（Nature Language Understanding 自然语言理解）类任务，如文本分类、命名实体识别等，NLU的目标则是将自然语言文本转化成结构化数据。
   - NLU和NLG两者表向上是一对相反的过程，但其实是紧密相连的，甚至目前很多NLU的任务都受到了生成式模型中表示方法的启发，更多只在最终任务上有所区别
- 文本生成，广义上只要输出是自然语言文本的各类任务都属于这个范畴
   - 端到端文本生成应用领域
   ![](https://p0.meituan.net/travelcube/5166cd647d076e3cea26972cbe9a332e75935.png)

### 技术方案

- 文本生成包含文本表示和文本生成两个关键的部分，既可以独立建模，也可以通过框架完成端到端的训练

#### 文本生成
- (1) seq2seq
   - 2014年提出的Seq2Seq Model，是解决这类问题一个非常通用的思路，本质是将输入句子或其中的词Token做Embedding后，输入循环神经网络中作为源句的表示，这一部分称为Encoder；另一部分生成端在每一个位置同样通过循环神经网络，循环输出对应的Token，这一部分称为Decoder。通过两个循环神经网络连接Encoder和Decoder，可以将两个平行表示连接起来。
   - ![](https://p1.meituan.net/travelcube/71ee857a5f58390785c10738d57d5c7970847.png)
- (2) attention
   - 本质思想是获取两端的某种权重关系，即在Decoder端生成的词和Encoder端的某些信息更相关。它也同样可以处理多模态的问题，比如Image2Text任务，通过CNN等将图片做一个关键特征的向量表示，将这个表示输出到类似的Decoder中去解码输出文本，视频语音等也使用同样的方式
   - ![](https://p0.meituan.net/travelcube/c64908b07137477135f9b7aa2927daea170277.png)

- Encoder-Decoder是一个非常通用的框架，它同样深入应用到了文本生成的三种主流方法，分别是规划式、抽取式和生成式，下面看下这几类方法各自的优劣势：
   - 规划式：根据结构化的信息，通过语法规则、树形规则等方式规划生成进文本中，可以抽象为三个阶段。宏观规划解决“说什么内容”，微观规划解决“怎么说”，包括语法句子粒度的规划，以及最后的表层优化对结果进行微调。
   - 抽取式：顾名思义，在原文信息中抽取一部分作为输出。可以通过编码端的表征在解码端转化为多种不同的分类任务，来实现端到端的优化。
      - 优势: 控制力极强、准确率较高，特别适合新闻播报等模版化场景。控制力极强，对源内容相关性好，改变用户行文较少，也不容易造成体验问题，可以直接在句子级别做端到端优化
      - 劣势是很难做到端到端的优化，损失信息上限也不高。
         - 在优化评估上，首先标题创意衡量的主观性很强，线上Feeds的标注数据也易受到其他因素的影响，比如推荐排序本身；其次，训练预测数据量差异造成OOV问题非常突出，分类任务叠加噪音效果提升非常困难。对此，我们重点在语义＋词级的方向上来对点击/转化率做建模，同时辅以线上E&E选优的机制来持续获取标注对，并提升在线自动纠错的能力。
         - 在受限上，抽取式虽然能直接在Seq级别对业务目标做优化，但有时候也须兼顾阅读体验，否则会形成一些“标题党”，亦或造成与原文相关性差的问题。对此，我们抽象了预处理和质量模型，来通用化处理文本创意内容的质控，独立了一个召回模块负责体验保障。并在模型结构上来对原文做独立表示，后又引入了Topic Feature Context来做针对性控制。
      - ![](https://p0.meituan.net/travelcube/94790c9b54cf6ad34fc800c1579416c3130763.png)
   - 生成式：通过编码端的表征，在解码端完成序列生成的任务，可以实现完全的端到端优化，可以完成多模态的任务。其在泛化能力上具有压倒性优势，但劣势是控制难度极大，建模复杂度也很高。
- 目前的主流的评估方法主要基于数据和人工评测。基于数据可以从不同角度衡量和训练目标文本的相近程度，如基于N-Gram匹配的BLUE和ROUGE等，基于字符编辑距离（Edit Distance）等，以及基于内容Coverage率的Jarcard距离等。基于数据的评测，在机器翻译等有明确标注的场景下具有很大的意义，这也是机器翻译领域最先有所突破的重要原因。但对于我们创意优化的场景来说，意义并不大，我们更重要的是优化业务目标，多以线上的实际效果为导向，并辅以人工评测。
- 另外，值得一提的是，近两年也逐渐涌现了很多利用GAN（Generative Adversarial Networks，生成对抗网络）的相关方法，来解决文本生成泛化性多样性的问题。有不少思路非常有趣，也值得尝试，只是GAN对于NLP的文本生成这类离散输出任务在效果评测指标层面，与传统的Seq2Seq模型还存在一定的差距，可视为一类具有潜力的技术方向。

#### 文本表示

- 整个2018年有两方面非常重要的工作进展：
   - Contextual Embedding：该方向包括一系列工作，如最佳论文Elmo(Embeddings from Language Models)，OpenAI的GPT(Generative Pre-Training)，以及谷歌大力出奇迹的BERT(Bidirectional Encoder Representations from Transformers)。解决的核心问题，是如何利用大量的没标注的文本数据学到一个预训练的模型，并通过通过这个模型辅助在不同的有标注任务上更好地完成目标。传统NLP任务深度模型，往往并不能通过持续增加深度来获取效果的提升，但是在表示层面增加深度，却往往可以对句子做更好的表征，它的核心思想是利用Embedding来表征上下文的的信息。但是这个想法可以通过很多种方式来实现，比如ELMo，通过双向的LSTM拼接后，可以同时得到含上下文信息的Embedding。而Transformer则在Encoder和Decoder两端，都将Attention机制都应用到了极致，通过序列间全位置的直连，可以高效叠加多层（12层），来完成句子的表征。这类方法可以将不同的终端任务做一个统一的表示，大大简化了建模抽象的复杂度。我们的表示也经历了从RNN到拥抱Attention的过程。
   
![](https://p1.meituan.net/travelcube/5f1b394d23c4fe5be6dfe885028a8668332821.png)

图5 GPT ELMo BERT模型结构
   - Tree-Based Embedding：另外一个流派则是通过树形结构进行建模，包括很多方式如传统的语法树，在语法结构上做Tree Base的RNN，用根结点的Embedding即可作为上下文的表征。Tree本身可以通过构造的方式，也可以通过学习的方式（比如强化学习）来进行构建。最终Task效果，既和树的结构（包括深度）有关，也受“表示”学习的能力影响，调优难度比较大。在我们的场景中，人工评测效果并不是很好，仍有很大继续探索的空间。

### 信息流中的应用

- 【2020-5-26】[大众点评信息流基于文本生成的创意优化实践](https://tech.meituan.com/2019/03/14/information-flow-creative-optimization-practices.html)
- 核心目标与推荐问题相似，提升包括点击率、转化率在内的通用指标，同时需要兼顾考量产品的阅读体验包括内容的导向性等
- 信息流中落地重点包括三个方向：
   - 文本创意：在文本方面，既包括了面向内容的摘要标题、排版改写等，也包括面向商户的推荐文案及内容化聚合页。它们都广泛地应用了文本表示和文本生成等技术，也是本文的主要方向。
   - 图像创意：图像方面涉及到首图或首帧的优选、图像的动态裁剪，以及图像的二次生成等。
   - 其他创意：包括多类展示理由（如社交关系等）、元素创意在内的额外补充信息。
![](https://p0.meituan.net/travelcube/a94ee4867832c8b91a9bae495d09eaf084451.png)

- 文本创意优化，在业务和技术上分别面临着不同的挑战。
   - 首先业务侧，启动创意优化需要两个基础前提：
      - 第一，衔接好创意优化与业务目标，因为并不是所有的创意都能优化，也不是所有创意优化都能带来预期的业务价值，方向不对则易蹚坑。
      - 第二，创意优化转化为最优化问题，有一定的Gap。其不同于很多分类排序问题，本身相对主观，所谓“一千个人眼中有一千个哈姆雷特”，创意优化能不能达到预期的业务目标，这个转化非常关键。
   - 其次，在技术层面，业界不同的应用都面临不一样的挑战，并且尝试和实践对应的解决方案。对文本创意生成来说，我们面临的最大的挑战包括以下三点：
      - 带受限的生成 生成一段流畅的文本并非难事，关键在于根据不同的场景和目标能控制它说什么、怎么说。这是目前挑战相对较大的一类问题，在我们的应用场景中都面临这个挑战。
      - 业务导向 生成能够提升业务指标、贴合业务目标的内容。为此，对内容源、内容表示与建模上提出了更高的要求。
      - 高效稳定 这里有两层含义，第一层是高效，即模型训练预测的效果和效率；第二层是稳定，线上系统应用，需要具备很高的准确率和一套完善的质量提升方案。

## 多轮对话改写

- 【2020-6-19】[浅谈多轮对话改写](https://zhuanlan.zhihu.com/p/145057649)
   - 2000条日常对话数据进行了简单地统计，发现其中33.5%存在指代，52.7%存在省略的情况
   - 人脑具有记忆的能力，能够很好地重建对话历史的重要信息，自动补全或者替换对方当前轮的回复，来理解回复的意思。同样，为了让对话系统具备记忆的能力，通常会通过对话状态跟踪（DST），来存储对话历史中的一些重要信息。
   - DST的主要问题有：
      - 存储的信息量有限，对于长对话的历史信息无法感知；
      - 存储信息不会太丰富，否则会导致冗余信息太多；
      - 存储的信息不太便于对话系统中其他模块的任务使用。
   - rewrite模型分为Pipeline的方式和End2End的方式
      - ![](https://pic4.zhimg.com/80/v2-ba681d520212275b177bd76d845cb2bf_1440w.jpg)
      - Pipeline方法将rewrite任务分成两个子任务，即指代和零指代的检测以及指代消解
      - 基于模型复杂度和性能的考虑，End2End方法的探索一直没有停止，直到2019年出现了几篇比较优秀的工作，本文将其划分为3种类型：基于联合训练的方法、基于序列标注的方法和基于指针网络的方法。
- [ACL 2019 | 使用表达改写提升多轮对话系统效果](https://www.aminer.cn/research_report/5d527dd4d5e908133c946b07)
   - [代码实现](https://github.com/chin-gyou/dialogue-utterance-rewriter/issues/11#issuecomment-627871399)
   - 【2020-12-02】[优化版](https://github.com/liu-nlper/dialogue-utterance-rewriter)，使用transformer，ACL 2019论文复现，多轮对话重写：[Improving Multi-turn Dialogue Modelling with Utterance ReWriter](https://www.aclweb.org/anthology/P19-1003.pdf)
      - 作者开源的代码是基于LSTM的，论文中基于Transformer的代码并未公布；
      - 论文实验所使用数据与公开的数据不一致，所以给出新的指标以供参考。
![](http://zhengwen.aminer.cn/a1.png)
- 模型框架
![](http://zhengwen.aminer.cn/a2.png)
- 对于任务导向型对话系统和闲聊型对话系统均有效果提升，实现了用更成熟的单轮对话技术解决多轮对话问题。
![](http://zhengwen.aminer.cn/a10.png)
- [Transformer多轮对话改写实践](https://zhuanlan.zhihu.com/p/137127209)
   - 介绍了多轮对话存在指代和信息省略的问题，同时提出了一种新方法-抽取式多轮对话改写，可以更加实用的部署于线上对话系统，并且提升对话效果
- 日常的交流对话中
   - 30%的对话会包含指代词。比如“它”用来指代物，“那边”用来指代地址
   - 同时有50%以上的对话会有信息省略
- 对话改写的任务有两个：1这句话要不要改写；2 把信息省略和指代识别出来。对于baseline论文放出的数据集，有90%的数据都是简单改写，也就是满足任务2，只有信息省略或者指代词。少数改写语句比较复杂，本文训练集剔除他们，但是验证集保留。
![](https://pic1.zhimg.com/80/v2-d80efd57b81c6ece955a247ca7247db4_1440w.jpg)
- Transformers结构可以通过attention机制有效提取指代词和上下文中关键信息的配对，最近也有一篇很好的工作专门用Bert来做指代消岐[2]。经过transformer结构提取文本特征后，模型结构及输出如下图。
![](https://pic4.zhimg.com/80/v2-0c4a789b68c60c8279dbd98fc18b5b2b_1440w.jpg)
- 输出五个指针中：
   - 关键信息的start和end专门用来识别需要为下文做信息补全或者指代的词；
   - 补全位置用来预测关键信息(start-end)插入在待改写语句的位置，实验中用插入位置的下一个token来表示；
   - 指代start和end用来识别带改写语句出现的指代词。
   - 当待改写语句中不存在指代词或者关键信息的补全时，指代的start和end将会指向cls，同理补全位置也这样。如同阅读理解任务中不存在答案一样，这样的操作在做预测任务时，当指代和补全位置的预测最大概率都位于cls时就可以避免改写，从而保证了改写的稳定性。
- 效果评估
   - 准确度
   ![](https://pic3.zhimg.com/80/v2-75faf2bed618cf5170efa56d65cd88e2_1440w.jpg)
   - 性能
   ![](https://pic3.zhimg.com/80/v2-75faf2bed618cf5170efa56d65cd88e2_1440w.jpg)
   - 对数据集的依赖
   ![](https://pic2.zhimg.com/80/v2-b22df166f10a6716b3db01e303f1a721_1440w.jpg)
   - 对负样本(不需要改写样本)的识别.基于指针抽取的方法对负样本的识别效果会更好。同时根据对长文本的改写效果观察，生成式改写效果较差。
   ![](https://pic3.zhimg.com/80/v2-b653c7da5923236991b6b0c5f973703a_1440w.jpg)
- 示例
   - 从左到右依次是A1|B1|A2|算法改写结果|用户标注label
>- 你知道板泉井水吗 | 知道 | 她是歌手 | 板泉井水是歌手 | 板泉井水是歌手
>- 乌龙茶 | 乌龙茶好喝吗 | 嗯好喝 | 嗯乌龙茶好喝 | 嗯乌龙茶好喝
>- 武林外传 | 超爱武林外传的 | 它的导演是谁 | 武林外传的导演是谁 | 武林外传的导演是谁
>- 李文雯你爱我吗 | 李文雯是哪位啊 | 她是我女朋友 | 李文雯是我女朋友 | 李文雯是我女朋友
>- 舒马赫 | 舒马赫看球了么 | 看了 | 舒马赫看了 | 舒马赫看球了

- 结论
   - 抽取式文本改写和生成式改写效果相当
   - 抽取式文本改写速度上绝对优于生成式
   - 抽取式文本改写对训练数据依赖少
   - 抽取式文本改写对负样本识别准确率略高于生成式

## 阅读理解

- [在阅读理解这件事上，AI已甩人类几条街？](https://www.huxiu.com/article/233577.html)
   - 机器在阅读理解的评分上超过人类，也许是NLP发展历程上的一次重大突破，意味着机器在“指标”上对人类的胜利，机器也确实可以在限定场景下有超过人类的表现。但这终究是一场“指标性”胜利，想要做到能理解会思考，机器还有“万里长征路”要走。
- [斯坦福陈丹琦博士论文 | 156页PDF讲解【神经网络阅读理解】](https://zhuanlan.zhihu.com/p/52622693)
- 【2021-1-14】[美团智能问答技术探索与实践](https://mp.weixin.qq.com/s?__biz=MzU1NTMyOTI4Mw==&mid=2247517833&idx=1&sn=0cb67429fa434d3dcd5afd6167754313&chksm=fbd734e5cca0bdf3f0cf43b588153d8117dec25d130240dcb9c42d5219cd94b972e463b55063&mpshare=1&scene=1&srcid=1221TKOk0XWVMxG3wT9wowUP&sharer_sharetime=1610600327445&sharer_shareid=b8d409494a5439418f4a89712efcd92a&version=3.1.0.6189&platform=mac#rd)
   - 近年来基于深度神经网络的机器阅读理解 ( Machine Reading Comprehension，MRC ) 技术得到了快速的发展，逐渐成为问答和对话系统中的关键技术。MRC模型以问题和文档为输入，通过阅读文档内容预测问题的答案。根据需要预测的答案形式不同，阅读理解任务可以分为填空式 ( Cloze-style )、多项选择式 ( Multi-choice )、片段提取式 ( Span-extraction ) 和自由文本 ( Free-form )。在实际问答系统中最常使用的是片段提取式阅读理解 ( MRC )，该任务需要从文档中提取连续的一段文字作为答案。最具影响力的片段提取式MRC公开数据集有SQuAD和MSMARCO等，这些数据集的出现促进了MRC模型的发展。在模型方面，深度神经网络结构被较早的应用到了机器阅读理解任务中，并采用基于边界预测(boundary-based prediction)方式解决片段提取式阅读理解任务。这些模型采用多层循环神经网络+注意力机制的结构获得问题和文档中每个词的上下文向量表示，在输出层预测答案片段的起始位置和终止位置。近年来预训练语言模型如BERT，RoBERTa和XLNet等在众多NLP任务上取得突破性进展，尤其是在阅读理解任务上。这些工作在编码阶段采用Transformer结构获得问题和文档向量表示，在输出层同样采用边界预测方式预测答案在文档中的位置。目前在单文档阅读理解任务SQuAD上，深度神经网络模型的预测EM/F1指标已经超越了人类标注者的水平，说明了模型在答案预测上的有效性。
   - Document QA借助机器阅读理解 ( MRC ) 技术，从非结构化文档中抽取片段回答用户问题。在问答场景中，当用户输入问题后，问答系统首先采用信息检索方式从商户详情或诸多UGC评论中查找到相关文档，再利用MRC模型从文档中摘取能够确切回答问题的一段文本。美团和大众点评上商户的简介攻略、UGC评论均有专业内容运营团队，可以产生内容优质、高可信度的答案，应用机器阅读理解技术直接从文档中提取答案，不需要人工维护意图类目，可以在不同业务领域灵活迁移，人工维护成本小。
   - 机器阅读理解模型
   - 深度神经网络结构较早的应用到机器阅读理解任务，代表性的包括Bi-DAF、R-NET、QANet、BERT等。这些模型均采用多层循环神经网络或Transformer加注意力机制等方式来解决问题和文档的上下文向量表示，最后通过边界预测来获取答案片段的起始和结束位置。我们选择表现最好的BERT模型进行相应任务的建模，将问题和文档作为输入，预测在文档中的起始位置和结束位置，将最大可能的起始位置和结束位置之间的片段抽取出来，作为答案。
   - 文档问答系统的答案预测流程包含三个步骤：
      - (1) **文档检索与选择** ( Retriever )：根据Query关键字检索景点等商户下的相关详情和UGC评论，根据相关性排序，筛选出相关的评论用于提取候选答案；
      - (2) **候选答案提取** ( Reader )：利用MRC模型在每个相关评论上提取一段文字作为候选答案，同时判断当前评论是否有答案，预测有答案和无答案的概率；
      - (3) **答案排序** ( Ranker )：根据候选答案的预测得分排序。这样能够同时处理多篇相关评论，比较并选择最优答案，同时根据无答案概率和阈值判断是否拒绝回答，避免无答案时错误回答。
   - 模型框架首先建模成Multi-Task架构，所有领域数据训练出一个共享参数，解决新领域与冷启动的问题，同时不同的领域，也会得到各自领域的参数，提升各自领域效果。除此之外，也发现只计算用户的Query和问答对里问题的相似度，是不太够的。答案往往也能帮助我们更好的去理解问题。上图中"还营业吗现在"的问题，语义上"正常营业吗？"比"关门了吗？"更相关，但从答案"肺炎期间闭园不营业"和"没去过"中很容易辨识出第一条答案更相关。因此建模时我们将答案也考虑进去，采用Multi-Field框架。最终我们的模型为Multi-Field Multi-Task RoBERTa模型。
   - 主流的KBQA解决方案包括基于查询图方法 ( Semantic Parser )、基于搜索排序方法 ( Information Retrieval )。查询图方案核心思路就是将自然语言问题经过一些语义分析方式转化成中间的语义表示 ( Logical Forms )，然后再将其转化为可以在 KG 中执行的描述性语言 ( 如 SPARQL 语言 ) 在图谱中查询，这种方式优势就是可解释强，符合知识图谱的显示推理过程。搜索排序方案首先会确定用户Query中的实体提及词 ( Entity Mention )，然后链接到 KG 中的主题实体 ( Topic Entity )，并将与Topic Entity相关的子图 ( Subgraph ) 提取出来作为候选答案集合，通过对Query以及Subgraph进行向量表示并映射到同一向量空间，通过两者相似度排序得到答案。这类方法更偏向于端到端的解决问题，但在扩展性和可解释性上不如查询图方案。在美团场景里我们采用以Semantic Parser方法为主的解决方案。


### 简介

- 机器阅读理解包括 4 种比较常见的任务：完形填空，多项选择，片段抽取，自由作答。
   - `完形填空`：在文章中隐藏一些单词，然后预测被隐藏的单词。
   - `多项选择`：给定一个问题、一段背景文字和一些候选答案，让模型判断哪些是正确答案。
   - `片段抽取`：给定一个问题、一段背景文字，从文本中抽取出一些单词，作为答案。
   - `自由作答`：给定一个问题、一段背景文字，生成单词序列作为答案。

- [A Gentle Introduction to Text Summarization in Machine Learning](https://blog.floydhub.com/gentle-introduction-to-text-summarization-in-machine-learning/)
- 阅读理解技术方案分类：
   - （1）提取式 Extraction-based summarization
      - ![](https://blog.floydhub.com/content/images/2019/04/extractive.gif)
   - （2）生成式 Abstraction-based summarization
      - ![](https://blog.floydhub.com/content/images/2019/04/Screenshot-2019-04-12-at-17.45.04.png)
   - （3）组合 ——综合以上两种方法
      - the pointer-generator network that gets the best of both worlds by combining both extraction(pointing) and abstraction(generating)
      - ![](https://blog.floydhub.com/content/images/2019/04/coverage.gif)


### 数据集

- 数据集分为四个类别：
   - 完形填空式；
   - 多选式；
   - 原文片段；
   - 答案由人类总结而来

- [阅读理解进阶三部曲——关键知识、模型性能提升、产品化落地](https://www.toutiao.com/i6630964969640821262/?tt_from=mobile_qq&utm_campaign=client_share&timestamp=1544680034&app=news_article&utm_source=mobile_qq&iid=53104048203&utm_medium=toutiao_android&group_id=6630964969640821262)


### 经典模型

- 阅读理解的经典 Model，主要包括：
   - Allen AI 提出的 BIDAF 
   - 微软提出的 R-NET 
   - Google 提出的 QANet 
   - 最近刷榜的 GPT & BERT 

### 代码实战

- 《[机器阅读理解：算法与实践](https://github.com/zcgzcgzcg1/MRC_book)》书籍配套代码

![](https://camo.githubusercontent.com/82ba584320339627f8207666b8ee6c53f19cfee8/68747470733a2f2f63732e7374616e666f72642e6564752f7e63677a68752f7069632f636f7665722e706e67)

## 小样本学习

- 从「文本增强」和「半监督学习」这两个角度出发，谈一谈如何解决少样本困境
   - ![](https://pic4.zhimg.com/v2-ad143542b083a12c46673589de4f136f_b.jpg)
- NLP中文本增强方法
   - ![](https://pic4.zhimg.com/v2-938b727e9572bb86dce68e37b68db40f_b.jpg)
- 详情参考：
   - [标注样本少怎么办？「文本增强+半监督学习」总结（从PseudoLabel到UDA/FixMatch） - JayLou娄杰的文章](https://zhuanlan.zhihu.com/p/146777068)
   - [一文了解NLP中的数据增强方法-简枫的文章](https://zhuanlan.zhihu.com/p/145521255)
   - 【2020-7-7】【一键中文数据增强工具】'NLP Chinese Data Augmentation' by 425776024 [GitHub](https://github.com/425776024/nlpcda),支持如下功能：
      - 1.随机实体替换
      - 2.近义词
      - 3.近义近音字替换
      - 4.随机字删除（内部细节：数字时间日期片段，内容不会删）
      - 5.新增：NER类 BIO 数据增强
      - 6.新增 随机置换邻近的字：研表究明，汉字序顺并不定一影响文字的阅读理解<<是乱序的
      - 7.新增百度中英翻译互转实现的增强
      - 8.新增中文等价字替换（1 一 壹 ①，2 二 贰 ②）

- 【2020-7-7】创新奇智有关少样本学习（Few-shot Learning）的研究论文《Prototype Rectification for Few-Shot Learning》被全球计算机视觉顶会ECCV 2020接收为Oral论文，入选率仅2%。[参考地址](https://www.toutiao.com/i6846586199994270222/)
- 本文提出一种简单有效的少样本学习方法，通过减小类内偏差和跨类偏差进行原型校正，从而显著提高少样本分类结果，并且给出理论推导证明本文所提方法可以提高理论下界，最终通过实验表明本方法在通用数据集中达到了最优结果，论文被ECCV 2020 接收为Oral。

- 【2020-8-19】[少样本学习中的深度学习模型综述](https://arxiv.org/abs/2008.06365v2)

- 【2020-11-12】Few-shot Learning最新进展调研


# Demo集合

- 【2020-5-19】NLP各类应用Demo汇总（Colab实现），[The Super Duper NLP Repo](https://notebooks.quantumstat.com/)，如文本生成、对话、问答、翻译等
![](https://image.jiqizhixin.com/uploads/editor/de27f3bd-2740-4097-b6d6-7340aeeb32cd/302.png)

<iframe src='https://notebooks.quantumstat.com/' frameborder='0' scrolling='no' allowfullscreen="true"></iframe>


# 工程落地

## python工具包

- 【2021-1-19】[python高效使用统计语言模型kenlm：新词发现、分词、智能纠错等](https://linux.ctolib.com/mattzheng-py-kenlm-model.html)
- kenlm的优点（关于kenlm工具训练统计语言模型）： 训练语言模型用的是传统的“统计+平滑”的方法，使用kenlm这个工具来训练。它快速，节省内存，最重要的是，允许在开源许可下使用多核处理器。 kenlm是一个C++编写的语言模型工具，具有速度快、占用内存小的特点，也提供了Python接口。
- 额外需要加载的库：
   - kenlm
   - pypinyin
   - pycorrector
- 粗略整理代码: 
   - kenlm：[mattzheng/py-kenlm-model](https://github.com/mattzheng/py-kenlm-model)
      - 安装：pip install https://github.com/kpu/kenlm/archive/master.zip
   - 新词发现：[mattzheng/word-discovery](https://github.com/mattzheng/word-discovery)
### kenlm模式

```python
import kenlm
model = kenlm.Model('lm/test.arpa')
print(model.score('this is a sentence .', bos = True, eos = True))

#Stateful query 状态转移概率
state = kenlm.State()
state2 = kenlm.State()
#Use <s> as context.  If you don't want <s>, use model.NullContextWrite(state).
model.BeginSentenceWrite(state)
```


### pypinyin拼音模块

- 拼音模块涉及到了pypinyin，用来识别汉字的拼音，还有非常多种的模式：

```python
from pypinyin import lazy_pinyin, Style
	# Python 中拼音库 PyPinyin 的用法
	# https://blog.csdn.net/devcloud/article/details/95066038

tts = ['BOPOMOFO', 'BOPOMOFO_FIRST', 'CYRILLIC', 'CYRILLIC_FIRST', 'FINALS', 'FINALS_TONE',
 'FINALS_TONE2', 'FINALS_TONE3', 'FIRST_LETTER', 'INITIALS', 'NORMAL', 'TONE', 'TONE2', 'TONE3']
for tt in tts:
    print(tt,lazy_pinyin('聪明的小兔子吃', style=eval('Style.{}'.format(tt))   ))
```

其中结果为：
```shell
BOPOMOFO ['ㄘㄨㄥ', 'ㄇㄧㄥˊ', 'ㄉㄜ˙', 'ㄒㄧㄠˇ', 'ㄊㄨˋ', 'ㄗ˙', 'ㄔ']
BOPOMOFO_FIRST ['ㄘ', 'ㄇ', 'ㄉ', 'ㄒ', 'ㄊ', 'ㄗ', 'ㄔ']
CYRILLIC ['цун1', 'мин2', 'дэ', 'сяо3', 'ту4', 'цзы', 'чи1']
CYRILLIC_FIRST ['ц', 'м', 'д', 'с', 'т', 'ц', 'ч']
FINALS ['ong', 'ing', 'e', 'iao', 'u', 'i', 'i']
FINALS_TONE ['ōng', 'íng', 'e', 'iǎo', 'ù', 'i', 'ī']
FINALS_TONE2 ['o1ng', 'i2ng', 'e', 'ia3o', 'u4', 'i', 'i1']
FINALS_TONE3 ['ong1', 'ing2', 'e', 'iao3', 'u4', 'i', 'i1']
FIRST_LETTER ['c', 'm', 'd', 'x', 't', 'z', 'c']
INITIALS ['c', 'm', 'd', 'x', 't', 'z', 'ch']
NORMAL ['cong', 'ming', 'de', 'xiao', 'tu', 'zi', 'chi']
TONE ['cōng', 'míng', 'de', 'xiǎo', 'tù', 'zi', 'chī']
TONE2 ['co1ng', 'mi2ng', 'de', 'xia3o', 'tu4', 'zi', 'chi1']
TONE3 ['cong1', 'ming2', 'de', 'xiao3', 'tu4', 'zi', 'chi1']
```

可以看出不同的style可以得到不同拼音形式。

### pycorrector纠错模块

- pycorrector的detect，可以返回，错误字的信息

```python
import pycorrector
sentence = '这瓶洗棉奶用着狠不错'
idx_errors = pycorrector.detect(sentence)
# [['这瓶', 0, 2, 'word'], ['棉奶', 3, 5, 'word']]

#correct是专门用来纠正：
pycorrector.correct(sentence)
```

-  pycorrector与kenlm纠错对比
- 来对比一下pycorrector自带的纠错和本次实验的纠错：

```python
import pycorrector
sentence = '这瓶洗棉奶用着狠不错'
idx_errors = pycorrector.detect(sentence)

correct = []
for ide in idx_errors:
    right_word = km.find_best_word(ide[0],ngrams_,freqs = 0)
    if right_word != ide[0]:
        correct.append([right_word] + ide)

print('错误：',idx_errors)
print('pycorrector的结果：',pycorrector.correct(sentence))
print('kenlm的结果：',correct)
```

> 错误： [['这瓶', 0, 2, 'word'], ['棉奶', 3, 5, 'word']]
> pycorrector的结果： ('这瓶洗面奶用着狠不错', [['棉奶', '面奶', 3, 5]])
> kenlm的结果： [['面奶', '棉奶', 3, 5, 'word']]

其他类似的案例：

sentence =  '少先队员因该给老人让坐'

> 错误： [['因该', 4, 6, 'word'], ['坐', 10, 11, 'char']]
> pycorrector的结果： ('少先队员应该给老人让座', [['因该', '应该', 4, 6], ['坐', '座', 10, 11]])
> kenlm的结果： [['应该', '因该', 4, 6, 'word']]
这里笔者的简陋规则暴露问题了，只能对2个字以上的进行判定。

sentence = '绿茶净华可以舒缓痘痘机肤'

> 错误： [['净华', 2, 4, 'word'], ['机肤', 10, 12, 'word']]
> pycorrector的结果： ('绿茶净化可以舒缓痘痘肌肤', [['净华', '净化', 2, 4], ['机肤', '肌肤', 10, 12]])
> kenlm的结果： [['精华', '净华', 2, 4, 'word'], ['肌肤', '机肤', 10, 12, 'word']]


## [nlp-tutorial](https://github.com/graykode/nlp-tutorial)

<p align="center"><img width="100" src="https://upload.wikimedia.org/wikipedia/commons/thumb/1/11/TensorFlowLogo.svg/225px-TensorFlowLogo.svg.png" />  <img width="100" src="https://media-thumbs.golden.com/OLqzmrmwAzY1P7Sl29k2T9WjJdM=/200x200/smart/golden-storage-production.s3.amazonaws.com/topic_images/e08914afa10a4179893eeb07cb5e4713.png" /></p>

`nlp-tutorial` is a tutorial for who is studying NLP(Natural Language Processing) using **TensorFlow** and **Pytorch**. Most of the models in NLP were implemented with less than **100 lines** of code.(except comments or blank lines)



## Curriculum - (Example Purpose)

#### 1. Basic Embedding Model

- 1-1. [NNLM(Neural Network Language Model)](https://github.com/graykode/nlp-tutorial/tree/master/1-1.NNLM) - **Predict Next Word**
  - Paper -  [A Neural Probabilistic Language Model(2003)](http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)
  - Colab - [NNLM_Tensor.ipynb](https://colab.research.google.com/github/graykode/nlp-tutorial/blob/master/1-1.NNLM/NNLM_Tensor.ipynb), [NNLM_Torch.ipynb](https://colab.research.google.com/github/graykode/nlp-tutorial/blob/master/1-1.NNLM/NNLM_Torch.ipynb)
- 1-2. [Word2Vec(Skip-gram)](https://github.com/graykode/nlp-tutorial/tree/master/1-2.Word2Vec) - **Embedding Words and Show Graph**
  - Paper - [Distributed Representations of Words and Phrases
    and their Compositionality(2013)](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)
  - Colab - [Word2Vec_Tensor(NCE_loss).ipynb](https://colab.research.google.com/github/graykode/nlp-tutorial/blob/master/1-2.Word2Vec/Word2Vec_Skipgram_Tensor(NCE_loss).ipynb), [Word2Vec_Tensor(Softmax).ipynb](https://colab.research.google.com/github/graykode/nlp-tutorial/blob/master/1-2.Word2Vec/Word2Vec_Skipgram_Tensor(Softmax).ipynb), [Word2Vec_Torch(Softmax).ipynb](https://colab.research.google.com/github/graykode/nlp-tutorial/blob/master/1-2.Word2Vec/Word2Vec_Skipgram_Torch(Softmax).ipynb)
- 1-3. [FastText(Application Level)](https://github.com/graykode/nlp-tutorial/tree/master/1-3.FastText) - **Sentence Classification**
  - Paper - [Bag of Tricks for Efficient Text Classification(2016)](https://arxiv.org/pdf/1607.01759.pdf)
  - Colab - [FastText.ipynb](https://colab.research.google.com/github/graykode/nlp-tutorial/blob/master/1-3.FastText/FastText.ipynb)

#### 2. CNN(Convolutional Neural Network)

- 2-1. [TextCNN](https://github.com/graykode/nlp-tutorial/tree/master/2-1.TextCNN) - **Binary Sentiment Classification**
  - Paper - [Convolutional Neural Networks for Sentence Classification(2014)](http://www.aclweb.org/anthology/D14-1181)
  - Colab - [TextCNN_Tensor.ipynb](https://colab.research.google.com/github/graykode/nlp-tutorial/blob/master/2-1.TextCNN/TextCNN_Tensor.ipynb), [TextCNN_Torch.ipynb](https://colab.research.google.com/github/graykode/nlp-tutorial/blob/master/2-1.TextCNN/TextCNN_Torch.ipynb)
- 2-2. DCNN(Dynamic Convolutional Neural Network)

#### 3. RNN(Recurrent Neural Network)

- 3-1. [TextRNN](https://github.com/graykode/nlp-tutorial/tree/master/3-1.TextRNN) - **Predict Next Step**
  - Paper - [Finding Structure in Time(1990)](http://psych.colorado.edu/~kimlab/Elman1990.pdf)
  - Colab - [TextRNN_Tensor.ipynb](https://colab.research.google.com/github/graykode/nlp-tutorial/blob/master/3-1.TextRNN/TextRNN_Tensor.ipynb), [TextRNN_Torch.ipynb](https://colab.research.google.com/github/graykode/nlp-tutorial/blob/master/3-1.TextRNN/TextRNN_Torch.ipynb)
- 3-2. [TextLSTM](https://github.com/graykode/nlp-tutorial/tree/master/3-2.TextLSTM) - **Autocomplete**
  - Paper - [LONG SHORT-TERM MEMORY(1997)](https://www.bioinf.jku.at/publications/older/2604.pdf)
  - Colab - [TextLSTM_Tensor.ipynb](https://colab.research.google.com/github/graykode/nlp-tutorial/blob/master/3-2.TextLSTM/TextLSTM_Tensor.ipynb), [TextLSTM_Torch.ipynb](https://colab.research.google.com/github/graykode/nlp-tutorial/blob/master/3-2.TextLSTM/TextLSTM_Torch.ipynb)
- 3-3. [Bi-LSTM](https://github.com/graykode/nlp-tutorial/tree/master/3-3.Bi-LSTM) - **Predict Next Word in Long Sentence**
  - Colab - [Bi_LSTM_Tensor.ipynb](https://colab.research.google.com/github/graykode/nlp-tutorial/blob/master/3-3.Bi-LSTM/Bi_LSTM_Tensor.ipynb), [Bi_LSTM_Torch.ipynb](https://colab.research.google.com/github/graykode/nlp-tutorial/blob/master/3-3.Bi-LSTM/Bi_LSTM_Torch.ipynb)

#### 4. Attention Mechanism

- 4-1. [Seq2Seq](https://github.com/graykode/nlp-tutorial/tree/master/4-1.Seq2Seq) - **Change Word**
  - Paper - [Learning Phrase Representations using RNN Encoder–Decoder
    for Statistical Machine Translation(2014)](https://arxiv.org/pdf/1406.1078.pdf)
  - Colab - [Seq2Seq_Tensor.ipynb](https://colab.research.google.com/github/graykode/nlp-tutorial/blob/master/4-1.Seq2Seq/Seq2Seq_Tensor.ipynb), [Seq2Seq_Torch.ipynb](https://colab.research.google.com/github/graykode/nlp-tutorial/blob/master/4-1.Seq2Seq/Seq2Seq_Torch.ipynb)
- 4-2. [Seq2Seq with Attention](https://github.com/graykode/nlp-tutorial/tree/master/4-2.Seq2Seq(Attention)) - **Translate**
  - Paper - [Neural Machine Translation by Jointly Learning to Align and Translate(2014)](https://arxiv.org/abs/1409.0473)
  - Colab - [Seq2Seq(Attention)_Tensor.ipynb](https://colab.research.google.com/github/graykode/nlp-tutorial/blob/master/4-2.Seq2Seq(Attention)/Seq2Seq(Attention)_Tensor.ipynb), [Seq2Seq(Attention)_Torch.ipynb](https://colab.research.google.com/github/graykode/nlp-tutorial/blob/master/4-2.Seq2Seq(Attention)/Seq2Seq(Attention)_Torch.ipynb)
- 4-3. [Bi-LSTM with Attention](https://github.com/graykode/nlp-tutorial/tree/master/4-3.Bi-LSTM(Attention)) - **Binary Sentiment Classification**
  - Colab - [Bi_LSTM(Attention)_Tensor.ipynb](https://colab.research.google.com/github/graykode/nlp-tutorial/blob/master/4-3.Bi-LSTM(Attention)/Bi_LSTM(Attention)_Tensor.ipynb), [Bi_LSTM(Attention)_Torch.ipynb](https://colab.research.google.com/github/graykode/nlp-tutorial/blob/master/4-3.Bi-LSTM(Attention)/Bi_LSTM(Attention)_Torch.ipynb)


#### 5. Model based on Transformer

- 5-1.  [The Transformer](https://github.com/graykode/nlp-tutorial/tree/master/5-1.Transformer) - **Translate**
  - Paper - [Attention Is All You Need(2017)](https://arxiv.org/abs/1706.03762)
  - Colab - [Transformer_Torch.ipynb](https://colab.research.google.com/github/graykode/nlp-tutorial/blob/master/5-1.Transformer/Transformer_Torch.ipynb), [Transformer(Greedy_decoder)_Torch.ipynb](https://colab.research.google.com/github/graykode/nlp-tutorial/blob/master/5-1.Transformer/Transformer(Greedy_decoder)_Torch.ipynb)
- 5-2. [BERT](https://github.com/graykode/nlp-tutorial/tree/master/5-2.BERT) - **Classification Next Sentence & Predict Masked Tokens**
  - Paper - [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding(2018)](https://arxiv.org/abs/1810.04805)
  - Colab - [BERT_Torch.ipynb](https://colab.research.google.com/github/graykode/nlp-tutorial/blob/master/5-2.BERT/BERT_Torch.ipynb)

|           Model            |              Example               |   Framework   | Lines(torch/tensor) |
| :------------------------: | :--------------------------------: | :-----------: | :-----------------: |
|            NNLM            |         Predict Next Word          | Torch, Tensor |        67/83        |
|     Word2Vec(Softmax)      |   Embedding Words and Show Graph   | Torch, Tensor |        77/94        |
|          TextCNN           |      Sentence Classification       | Torch, Tensor |        94/99        |
|          TextRNN           |         Predict Next Step          | Torch, Tensor |        70/88        |
|          TextLSTM          |            Autocomplete            | Torch, Tensor |        73/78        |
|          Bi-LSTM           | Predict Next Word in Long Sentence | Torch, Tensor |        73/78        |
|          Seq2Seq           |            Change Word             | Torch, Tensor |       93/111        |
|   Seq2Seq with Attention   |             Translate              | Torch, Tensor |       108/118       |
|   Bi-LSTM with Attention   |  Binary Sentiment Classification   | Torch, Tensor |       92/104        |
|        Transformer         |             Translate              |     Torch     |        222/0        |
| Greedy Decoder Transformer |             Translate              |     Torch     |        246/0        |
|            BERT            |            how to train            |     Torch     |        242/0        |


## Dependencies

- Python 3.5+
- Tensorflow 1.12.0+
- Pytorch 0.4.1+
- Plan to add Keras Version



# 应用

## 项目实战

- 【2020-6-2】[京东AI项目实战课](http://finance.sina.com.cn/wm/2020-06-02/doc-iircuyvi6299111.shtml)：五个阶段从理论到实践、从项目实战到面试准备的一站式教学，涵盖NLP领域核心技能（特征工程、分类模型、语法树等），前沿技术（BERT、XLNet、Seq2Seq、Transformer、ALBERT、模型蒸馏、模型压缩等）；
- ![](http://n.sinaimg.cn/sinakd202062s/263/w1000h4063/20200602/f2ae-iumkapw2054484.png)
- ![](http://n.sinaimg.cn/sinakd202062s/612/w1000h5212/20200602/0036-iumkapw2054582.jpg)

## 资料

- 更多[Demo地址](http://wqw547243068.github.io/demo)

# 结束


