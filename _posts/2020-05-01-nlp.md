---
layout: post
title:  "自然语言处理-NLP"
date:   2020-05-01 21:50:00
categories: 自然语言处理
tags: 深度学习 NLP 自然语言处理
excerpt: NLP各类知识点及工程落地方法汇总
author: 鹤啸九天
mathjax: true
---

* content
{:toc}

# 总结


# 算法理论


## 预训练模型总结

- PTMs: Pre-trained-Models in NLP，[NLP预训练模型的全面总结(持续更新中)](https://github.com/loujie0822/Pre-trained-Models/blob/master/README.md)
- 2020年3月18日，邱锡鹏老师发表了关于NLP预训练模型的综述《[Pre-trained Models for Natural Language Processing: A Survey](https://zhuanlan.zhihu.com/p/115014536?utm_source=qq&utm_medium=social&utm_oi=27211553832960#ref_1)》
- 知乎文章1:  [全面总结！PTMs：NLP预训练模型](https://zhuanlan.zhihu.com/p/115014536)  ➡️➡️ [图片下载](https://github.com/loujie0822/Pre-trained-Models/blob/master/resources/PTMs.jpg)
- 知乎文章2：[nlp中的预训练语言模型总结](https://zhuanlan.zhihu.com/p/76912493)
- 知乎文章3：[nlp中的词向量对比](https://zhuanlan.zhihu.com/p/56382372)

<img src="https://pic3.zhimg.com/80/v2-0ace60ca3d843fc9b69c6965731f288e_720w.jpg" style="zoom:20%;" />

- 对比分析，摘自：[论文笔记 - Pre-trained Models for Natural Language Processing](http://www.shuang0420.com/2020/05/07/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20Pre-trained%20Models%20for%20Natural%20Language%20Processing/)，[Pre-trained Models for Natural Language Processing: A Survey](https://arxiv.org/abs/2003.08271)
   - LM（Language Modeling）是 NLP 中最常见的无监督任务，通常特指自回归或单向语言建模，BiLM 虽然结合了两个方向的语言模型，但只是两个方向的简单拼接，并不是真正意义上的双向语言模型。
   - MLM（Masked Language Modeling）可以克服传统单向语言模型的缺陷，结合双向的信息，但是 [MASK] 的引入使得预训练和 fine-tune 之间出现 gap
   - PLM（Permuted Language Modeling）则克服了这个问题，实现了双向语言模型和自回归模型的统一。
   - DAE（Denoising Autoencoder）接受部分损坏的输入，并以恢复原始输入为目标。与 MLM 不同，DAE 会给输入额外加一些噪声。
   - CTL（Contrastive Learning） 的原理是在对比中学习，其假设是一些 observed pairs of text 在语义上比随机采样的文本更为接近。CTL 比 LM 计算复杂度更低。
- 综述从四个方面（Representation Types、Architectures、Pre-training Task Types、Extensions）对现有 PTMs (Pre-trained Models) 进行了系统分类，一幅图来概括全文精华：
   - ![](http://images.shuang0420.com/images/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20Pre-trained%20Models%20for%20Natural%20Language%20Processing/taxonomy.png)

### 1、论文汇总：

PTMs-Papers:

1. https://github.com/thunlp/PLMpapers
2. https://github.com/tomohideshibata/BERT-related-papers
3. https://github.com/cedrickchee/awesome-bert-nlp
4. https://bertlang.unibocconi.it/
5. https://github.com/jessevig/bertviz

### 2. PTMs单模型解读

1. 自监督学习：[Self-Supervised Learning 入门介绍](https://zhuanlan.zhihu.com/p/108625273)
2. 自监督学习：[Self-supervised Learning 再次入门](https://zhuanlan.zhihu.com/p/108906502)
3. 词向量总结：[nlp中的词向量对比：word2vec/glove/fastText/elmo/GPT/bert](https://zhuanlan.zhihu.com/p/56382372)
4. 词向量总结：[从Word Embedding到Bert模型—自然语言处理中的预训练技术发展史](https://zhuanlan.zhihu.com/p/49271699)
5. ELMo解读：[关于ELMo的若干问题整理记录](https://zhuanlan.zhihu.com/p/82602015)
6. BERT解读： [Bert时代的创新：Bert应用模式比较及其它](https://zhuanlan.zhihu.com/p/65470719)
7. XLNET解读：[XLNet:运行机制及和Bert的异同比较](https://zhuanlan.zhihu.com/p/70257427)
8. XLNET解读：[XLnet：比Bert更强大的预训练模型](https://zhuanlan.zhihu.com/p/71759544)
9. RoBERTa解读：[RoBERT: 没错，我就是能更强——更大数据规模和仔细调参下的最优BERT](https://zhuanlan.zhihu.com/p/75629127)
10. 预训练语言模型总结：[nlp中的预训练语言模型总结(单向模型、BERT系列模型、XLNet)](https://zhuanlan.zhihu.com/p/76912493)
11. 预训练语言模型总结：[8篇论文梳理BERT相关模型进展与反思](https://zhuanlan.zhihu.com/p/81157740)
12. ELECTRA解读: [ELECTRA: 超越BERT, 19年最佳NLP预训练模型](https://zhuanlan.zhihu.com/p/89763176)
13. 模型压缩 LayerDrop:[结构剪枝：要个4层的BERT有多难？](https://zhuanlan.zhihu.com/p/93207254)
14. 模型压缩 BERT-of-Theseus:[bert-of-theseus，一个非常亲民的bert压缩方法](https://zhuanlan.zhihu.com/p/112787764)
15. 模型压缩 TinyBERT:[比 Bert 体积更小速度更快的 TinyBERT](https://zhuanlan.zhihu.com/p/94359189)
16. 模型压缩总结：[BERT 瘦身之路：Distillation，Quantization，Pruning](https://zhuanlan.zhihu.com/p/86900556)


## 机器翻译

- 【2020-6-5】[机器翻译：统计建模与深度学习方法](https://opensource.niutrans.com/mtbook/index.html)，[ppt地址](https://github.com/NiuTrans/MTBook/blob/master/slides)
- ![](https://opensource.niutrans.com/guideline.png)

- 【2020-6-10】Google官方示例：[基于注意力的神经机器翻译](https://www.tensorflow.org/tutorials/text/nmt_with_attention?hl=zh-cn)
   - ![](https://tensorflow.org/images/spanish-english.png)

## 文本生成

### 综述

- 文本生成被称为NLG，目标是根据输入数据生成自然语言的文本。
   - NLP领域使用更多的一般是NLU（Nature Language Understanding 自然语言理解）类任务，如文本分类、命名实体识别等，NLU的目标则是将自然语言文本转化成结构化数据。
   - NLU和NLG两者表向上是一对相反的过程，但其实是紧密相连的，甚至目前很多NLU的任务都受到了生成式模型中表示方法的启发，更多只在最终任务上有所区别
- 文本生成，广义上只要输出是自然语言文本的各类任务都属于这个范畴
   - 端到端文本生成应用领域
   ![](https://p0.meituan.net/travelcube/5166cd647d076e3cea26972cbe9a332e75935.png)

### 技术方案

- 文本生成包含文本表示和文本生成两个关键的部分，既可以独立建模，也可以通过框架完成端到端的训练

#### 文本生成
- (1) seq2seq
   - 2014年提出的Seq2Seq Model，是解决这类问题一个非常通用的思路，本质是将输入句子或其中的词Token做Embedding后，输入循环神经网络中作为源句的表示，这一部分称为Encoder；另一部分生成端在每一个位置同样通过循环神经网络，循环输出对应的Token，这一部分称为Decoder。通过两个循环神经网络连接Encoder和Decoder，可以将两个平行表示连接起来。
   - ![](https://p1.meituan.net/travelcube/71ee857a5f58390785c10738d57d5c7970847.png)
- (2) attention
   - 本质思想是获取两端的某种权重关系，即在Decoder端生成的词和Encoder端的某些信息更相关。它也同样可以处理多模态的问题，比如Image2Text任务，通过CNN等将图片做一个关键特征的向量表示，将这个表示输出到类似的Decoder中去解码输出文本，视频语音等也使用同样的方式
   - ![](https://p0.meituan.net/travelcube/c64908b07137477135f9b7aa2927daea170277.png)

- Encoder-Decoder是一个非常通用的框架，它同样深入应用到了文本生成的三种主流方法，分别是规划式、抽取式和生成式，下面看下这几类方法各自的优劣势：
   - 规划式：根据结构化的信息，通过语法规则、树形规则等方式规划生成进文本中，可以抽象为三个阶段。宏观规划解决“说什么内容”，微观规划解决“怎么说”，包括语法句子粒度的规划，以及最后的表层优化对结果进行微调。
   - 抽取式：顾名思义，在原文信息中抽取一部分作为输出。可以通过编码端的表征在解码端转化为多种不同的分类任务，来实现端到端的优化。
      - 优势: 控制力极强、准确率较高，特别适合新闻播报等模版化场景。控制力极强，对源内容相关性好，改变用户行文较少，也不容易造成体验问题，可以直接在句子级别做端到端优化
      - 劣势是很难做到端到端的优化，损失信息上限也不高。
         - 在优化评估上，首先标题创意衡量的主观性很强，线上Feeds的标注数据也易受到其他因素的影响，比如推荐排序本身；其次，训练预测数据量差异造成OOV问题非常突出，分类任务叠加噪音效果提升非常困难。对此，我们重点在语义＋词级的方向上来对点击/转化率做建模，同时辅以线上E&E选优的机制来持续获取标注对，并提升在线自动纠错的能力。
         - 在受限上，抽取式虽然能直接在Seq级别对业务目标做优化，但有时候也须兼顾阅读体验，否则会形成一些“标题党”，亦或造成与原文相关性差的问题。对此，我们抽象了预处理和质量模型，来通用化处理文本创意内容的质控，独立了一个召回模块负责体验保障。并在模型结构上来对原文做独立表示，后又引入了Topic Feature Context来做针对性控制。
      - ![](https://p0.meituan.net/travelcube/94790c9b54cf6ad34fc800c1579416c3130763.png)
   - 生成式：通过编码端的表征，在解码端完成序列生成的任务，可以实现完全的端到端优化，可以完成多模态的任务。其在泛化能力上具有压倒性优势，但劣势是控制难度极大，建模复杂度也很高。
- 目前的主流的评估方法主要基于数据和人工评测。基于数据可以从不同角度衡量和训练目标文本的相近程度，如基于N-Gram匹配的BLUE和ROUGE等，基于字符编辑距离（Edit Distance）等，以及基于内容Coverage率的Jarcard距离等。基于数据的评测，在机器翻译等有明确标注的场景下具有很大的意义，这也是机器翻译领域最先有所突破的重要原因。但对于我们创意优化的场景来说，意义并不大，我们更重要的是优化业务目标，多以线上的实际效果为导向，并辅以人工评测。
- 另外，值得一提的是，近两年也逐渐涌现了很多利用GAN（Generative Adversarial Networks，生成对抗网络）的相关方法，来解决文本生成泛化性多样性的问题。有不少思路非常有趣，也值得尝试，只是GAN对于NLP的文本生成这类离散输出任务在效果评测指标层面，与传统的Seq2Seq模型还存在一定的差距，可视为一类具有潜力的技术方向。

#### 文本表示

- 整个2018年有两方面非常重要的工作进展：
   - Contextual Embedding：该方向包括一系列工作，如最佳论文Elmo(Embeddings from Language Models)，OpenAI的GPT(Generative Pre-Training)，以及谷歌大力出奇迹的BERT(Bidirectional Encoder Representations from Transformers)。解决的核心问题，是如何利用大量的没标注的文本数据学到一个预训练的模型，并通过通过这个模型辅助在不同的有标注任务上更好地完成目标。传统NLP任务深度模型，往往并不能通过持续增加深度来获取效果的提升，但是在表示层面增加深度，却往往可以对句子做更好的表征，它的核心思想是利用Embedding来表征上下文的的信息。但是这个想法可以通过很多种方式来实现，比如ELMo，通过双向的LSTM拼接后，可以同时得到含上下文信息的Embedding。而Transformer则在Encoder和Decoder两端，都将Attention机制都应用到了极致，通过序列间全位置的直连，可以高效叠加多层（12层），来完成句子的表征。这类方法可以将不同的终端任务做一个统一的表示，大大简化了建模抽象的复杂度。我们的表示也经历了从RNN到拥抱Attention的过程。
   
![](https://p1.meituan.net/travelcube/5f1b394d23c4fe5be6dfe885028a8668332821.png)

图5 GPT ELMo BERT模型结构
   - Tree-Based Embedding：另外一个流派则是通过树形结构进行建模，包括很多方式如传统的语法树，在语法结构上做Tree Base的RNN，用根结点的Embedding即可作为上下文的表征。Tree本身可以通过构造的方式，也可以通过学习的方式（比如强化学习）来进行构建。最终Task效果，既和树的结构（包括深度）有关，也受“表示”学习的能力影响，调优难度比较大。在我们的场景中，人工评测效果并不是很好，仍有很大继续探索的空间。

### 信息流中的应用

- 【2020-5-26】[大众点评信息流基于文本生成的创意优化实践](https://tech.meituan.com/2019/03/14/information-flow-creative-optimization-practices.html)
- 核心目标与推荐问题相似，提升包括点击率、转化率在内的通用指标，同时需要兼顾考量产品的阅读体验包括内容的导向性等
- 信息流中落地重点包括三个方向：
   - 文本创意：在文本方面，既包括了面向内容的摘要标题、排版改写等，也包括面向商户的推荐文案及内容化聚合页。它们都广泛地应用了文本表示和文本生成等技术，也是本文的主要方向。
   - 图像创意：图像方面涉及到首图或首帧的优选、图像的动态裁剪，以及图像的二次生成等。
   - 其他创意：包括多类展示理由（如社交关系等）、元素创意在内的额外补充信息。
![](https://p0.meituan.net/travelcube/a94ee4867832c8b91a9bae495d09eaf084451.png)

- 文本创意优化，在业务和技术上分别面临着不同的挑战。
   - 首先业务侧，启动创意优化需要两个基础前提：
      - 第一，衔接好创意优化与业务目标，因为并不是所有的创意都能优化，也不是所有创意优化都能带来预期的业务价值，方向不对则易蹚坑。
      - 第二，创意优化转化为最优化问题，有一定的Gap。其不同于很多分类排序问题，本身相对主观，所谓“一千个人眼中有一千个哈姆雷特”，创意优化能不能达到预期的业务目标，这个转化非常关键。
   - 其次，在技术层面，业界不同的应用都面临不一样的挑战，并且尝试和实践对应的解决方案。对文本创意生成来说，我们面临的最大的挑战包括以下三点：
      - 带受限的生成 生成一段流畅的文本并非难事，关键在于根据不同的场景和目标能控制它说什么、怎么说。这是目前挑战相对较大的一类问题，在我们的应用场景中都面临这个挑战。
      - 业务导向 生成能够提升业务指标、贴合业务目标的内容。为此，对内容源、内容表示与建模上提出了更高的要求。
      - 高效稳定 这里有两层含义，第一层是高效，即模型训练预测的效果和效率；第二层是稳定，线上系统应用，需要具备很高的准确率和一套完善的质量提升方案。

## 多轮对话改写

- [ACL 2019 | 使用表达改写提升多轮对话系统效果](https://www.aminer.cn/research_report/5d527dd4d5e908133c946b07)
- [代码实现](https://github.com/chin-gyou/dialogue-utterance-rewriter/issues/11#issuecomment-627871399)
![](http://zhengwen.aminer.cn/a1.png)
- 模型框架
![](http://zhengwen.aminer.cn/a2.png)
- 对于任务导向型对话系统和闲聊型对话系统均有效果提升，实现了用更成熟的单轮对话技术解决多轮对话问题。
![](http://zhengwen.aminer.cn/a10.png)
- [Transformer多轮对话改写实践](https://zhuanlan.zhihu.com/p/137127209)
   - 介绍了多轮对话存在指代和信息省略的问题，同时提出了一种新方法-抽取式多轮对话改写，可以更加实用的部署于线上对话系统，并且提升对话效果
- 日常的交流对话中
   - 30%的对话会包含指代词。比如“它”用来指代物，“那边”用来指代地址
   - 同时有50%以上的对话会有信息省略
- 对话改写的任务有两个：1这句话要不要改写；2 把信息省略和指代识别出来。对于baseline论文放出的数据集，有90%的数据都是简单改写，也就是满足任务2，只有信息省略或者指代词。少数改写语句比较复杂，本文训练集剔除他们，但是验证集保留。
![](https://pic1.zhimg.com/80/v2-d80efd57b81c6ece955a247ca7247db4_1440w.jpg)
- Transformers结构可以通过attention机制有效提取指代词和上下文中关键信息的配对，最近也有一篇很好的工作专门用Bert来做指代消岐[2]。经过transformer结构提取文本特征后，模型结构及输出如下图。
![](https://pic4.zhimg.com/80/v2-0c4a789b68c60c8279dbd98fc18b5b2b_1440w.jpg)
- 输出五个指针中：
   - 关键信息的start和end专门用来识别需要为下文做信息补全或者指代的词；
   - 补全位置用来预测关键信息(start-end)插入在待改写语句的位置，实验中用插入位置的下一个token来表示；
   - 指代start和end用来识别带改写语句出现的指代词。
   - 当待改写语句中不存在指代词或者关键信息的补全时，指代的start和end将会指向cls，同理补全位置也这样。如同阅读理解任务中不存在答案一样，这样的操作在做预测任务时，当指代和补全位置的预测最大概率都位于cls时就可以避免改写，从而保证了改写的稳定性。
- 效果评估
   - 准确度
   ![](https://pic3.zhimg.com/80/v2-75faf2bed618cf5170efa56d65cd88e2_1440w.jpg)
   - 性能
   ![](https://pic3.zhimg.com/80/v2-75faf2bed618cf5170efa56d65cd88e2_1440w.jpg)
   - 对数据集的依赖
   ![](https://pic2.zhimg.com/80/v2-b22df166f10a6716b3db01e303f1a721_1440w.jpg)
   - 对负样本(不需要改写样本)的识别.基于指针抽取的方法对负样本的识别效果会更好。同时根据对长文本的改写效果观察，生成式改写效果较差。
   ![](https://pic3.zhimg.com/80/v2-b653c7da5923236991b6b0c5f973703a_1440w.jpg)
- 示例
   - 从左到右依次是A1|B1|A2|算法改写结果|用户标注label
>- 你知道板泉井水吗 | 知道 | 她是歌手 | 板泉井水是歌手 | 板泉井水是歌手
>- 乌龙茶 | 乌龙茶好喝吗 | 嗯好喝 | 嗯乌龙茶好喝 | 嗯乌龙茶好喝
>- 武林外传 | 超爱武林外传的 | 它的导演是谁 | 武林外传的导演是谁 | 武林外传的导演是谁
>- 李文雯你爱我吗 | 李文雯是哪位啊 | 她是我女朋友 | 李文雯是我女朋友 | 李文雯是我女朋友
>- 舒马赫 | 舒马赫看球了么 | 看了 | 舒马赫看了 | 舒马赫看球了

- 结论
   - 抽取式文本改写和生成式改写效果相当
   - 抽取式文本改写速度上绝对优于生成式
   - 抽取式文本改写对训练数据依赖少
   - 抽取式文本改写对负样本识别准确率略高于生成式

## 阅读理解

- [在阅读理解这件事上，AI已甩人类几条街？](https://www.huxiu.com/article/233577.html)
   - 机器在阅读理解的评分上超过人类，也许是NLP发展历程上的一次重大突破，意味着机器在“指标”上对人类的胜利，机器也确实可以在限定场景下有超过人类的表现。但这终究是一场“指标性”胜利，想要做到能理解会思考，机器还有“万里长征路”要走。
- [斯坦福陈丹琦博士论文 | 156页PDF讲解【神经网络阅读理解】](https://zhuanlan.zhihu.com/p/52622693)


### 简介

- [A Gentle Introduction to Text Summarization in Machine Learning](https://blog.floydhub.com/gentle-introduction-to-text-summarization-in-machine-learning/)
- 阅读理解分类：
   - （1）提取式 Extraction-based summarization
      - ![](https://blog.floydhub.com/content/images/2019/04/extractive.gif)
   - （2）生成式 Abstraction-based summarization
      - ![](https://blog.floydhub.com/content/images/2019/04/Screenshot-2019-04-12-at-17.45.04.png)
   - （3）组合 ——综合以上两种方法
      - the pointer-generator network that gets the best of both worlds by combining both extraction(pointing) and abstraction(generating)
      - ![](https://blog.floydhub.com/content/images/2019/04/coverage.gif)

### 数据集

- 数据集分为四个类别：
   - 第一种是完形填空式；
   - 第二种是多选式；
   - 第三种是原文片段；
   - 第四种是答案由人类总结而来

- [阅读理解进阶三部曲——关键知识、模型性能提升、产品化落地](https://www.toutiao.com/i6630964969640821262/?tt_from=mobile_qq&utm_campaign=client_share&timestamp=1544680034&app=news_article&utm_source=mobile_qq&iid=53104048203&utm_medium=toutiao_android&group_id=6630964969640821262)


### 经典模型

- 阅读理解的经典 Model，主要包括：
   - Allen AI 提出的 BIDAF 
   - 微软提出的 R-NET 
   - Google 提出的 QANet 
   - 最近刷榜的 GPT & BERT 

# Demo集合

- 【2020-5-19】NLP各类应用Demo汇总（Colab实现），[The Super Duper NLP Repo](https://notebooks.quantumstat.com/)，如文本生成、对话、问答、翻译等
![](https://image.jiqizhixin.com/uploads/editor/de27f3bd-2740-4097-b6d6-7340aeeb32cd/302.png)

<iframe src='https://notebooks.quantumstat.com/' frameborder='0' scrolling='no' allowfullscreen="true"></iframe>


# 工程落地

## [nlp-tutorial](https://github.com/graykode/nlp-tutorial)

<p align="center"><img width="100" src="https://upload.wikimedia.org/wikipedia/commons/thumb/1/11/TensorFlowLogo.svg/225px-TensorFlowLogo.svg.png" />  <img width="100" src="https://media-thumbs.golden.com/OLqzmrmwAzY1P7Sl29k2T9WjJdM=/200x200/smart/golden-storage-production.s3.amazonaws.com/topic_images/e08914afa10a4179893eeb07cb5e4713.png" /></p>

`nlp-tutorial` is a tutorial for who is studying NLP(Natural Language Processing) using **TensorFlow** and **Pytorch**. Most of the models in NLP were implemented with less than **100 lines** of code.(except comments or blank lines)



## Curriculum - (Example Purpose)

#### 1. Basic Embedding Model

- 1-1. [NNLM(Neural Network Language Model)](https://github.com/graykode/nlp-tutorial/tree/master/1-1.NNLM) - **Predict Next Word**
  - Paper -  [A Neural Probabilistic Language Model(2003)](http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)
  - Colab - [NNLM_Tensor.ipynb](https://colab.research.google.com/github/graykode/nlp-tutorial/blob/master/1-1.NNLM/NNLM_Tensor.ipynb), [NNLM_Torch.ipynb](https://colab.research.google.com/github/graykode/nlp-tutorial/blob/master/1-1.NNLM/NNLM_Torch.ipynb)
- 1-2. [Word2Vec(Skip-gram)](https://github.com/graykode/nlp-tutorial/tree/master/1-2.Word2Vec) - **Embedding Words and Show Graph**
  - Paper - [Distributed Representations of Words and Phrases
    and their Compositionality(2013)](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)
  - Colab - [Word2Vec_Tensor(NCE_loss).ipynb](https://colab.research.google.com/github/graykode/nlp-tutorial/blob/master/1-2.Word2Vec/Word2Vec_Skipgram_Tensor(NCE_loss).ipynb), [Word2Vec_Tensor(Softmax).ipynb](https://colab.research.google.com/github/graykode/nlp-tutorial/blob/master/1-2.Word2Vec/Word2Vec_Skipgram_Tensor(Softmax).ipynb), [Word2Vec_Torch(Softmax).ipynb](https://colab.research.google.com/github/graykode/nlp-tutorial/blob/master/1-2.Word2Vec/Word2Vec_Skipgram_Torch(Softmax).ipynb)
- 1-3. [FastText(Application Level)](https://github.com/graykode/nlp-tutorial/tree/master/1-3.FastText) - **Sentence Classification**
  - Paper - [Bag of Tricks for Efficient Text Classification(2016)](https://arxiv.org/pdf/1607.01759.pdf)
  - Colab - [FastText.ipynb](https://colab.research.google.com/github/graykode/nlp-tutorial/blob/master/1-3.FastText/FastText.ipynb)

#### 2. CNN(Convolutional Neural Network)

- 2-1. [TextCNN](https://github.com/graykode/nlp-tutorial/tree/master/2-1.TextCNN) - **Binary Sentiment Classification**
  - Paper - [Convolutional Neural Networks for Sentence Classification(2014)](http://www.aclweb.org/anthology/D14-1181)
  - Colab - [TextCNN_Tensor.ipynb](https://colab.research.google.com/github/graykode/nlp-tutorial/blob/master/2-1.TextCNN/TextCNN_Tensor.ipynb), [TextCNN_Torch.ipynb](https://colab.research.google.com/github/graykode/nlp-tutorial/blob/master/2-1.TextCNN/TextCNN_Torch.ipynb)
- 2-2. DCNN(Dynamic Convolutional Neural Network)

#### 3. RNN(Recurrent Neural Network)

- 3-1. [TextRNN](https://github.com/graykode/nlp-tutorial/tree/master/3-1.TextRNN) - **Predict Next Step**
  - Paper - [Finding Structure in Time(1990)](http://psych.colorado.edu/~kimlab/Elman1990.pdf)
  - Colab - [TextRNN_Tensor.ipynb](https://colab.research.google.com/github/graykode/nlp-tutorial/blob/master/3-1.TextRNN/TextRNN_Tensor.ipynb), [TextRNN_Torch.ipynb](https://colab.research.google.com/github/graykode/nlp-tutorial/blob/master/3-1.TextRNN/TextRNN_Torch.ipynb)
- 3-2. [TextLSTM](https://github.com/graykode/nlp-tutorial/tree/master/3-2.TextLSTM) - **Autocomplete**
  - Paper - [LONG SHORT-TERM MEMORY(1997)](https://www.bioinf.jku.at/publications/older/2604.pdf)
  - Colab - [TextLSTM_Tensor.ipynb](https://colab.research.google.com/github/graykode/nlp-tutorial/blob/master/3-2.TextLSTM/TextLSTM_Tensor.ipynb), [TextLSTM_Torch.ipynb](https://colab.research.google.com/github/graykode/nlp-tutorial/blob/master/3-2.TextLSTM/TextLSTM_Torch.ipynb)
- 3-3. [Bi-LSTM](https://github.com/graykode/nlp-tutorial/tree/master/3-3.Bi-LSTM) - **Predict Next Word in Long Sentence**
  - Colab - [Bi_LSTM_Tensor.ipynb](https://colab.research.google.com/github/graykode/nlp-tutorial/blob/master/3-3.Bi-LSTM/Bi_LSTM_Tensor.ipynb), [Bi_LSTM_Torch.ipynb](https://colab.research.google.com/github/graykode/nlp-tutorial/blob/master/3-3.Bi-LSTM/Bi_LSTM_Torch.ipynb)

#### 4. Attention Mechanism

- 4-1. [Seq2Seq](https://github.com/graykode/nlp-tutorial/tree/master/4-1.Seq2Seq) - **Change Word**
  - Paper - [Learning Phrase Representations using RNN Encoder–Decoder
    for Statistical Machine Translation(2014)](https://arxiv.org/pdf/1406.1078.pdf)
  - Colab - [Seq2Seq_Tensor.ipynb](https://colab.research.google.com/github/graykode/nlp-tutorial/blob/master/4-1.Seq2Seq/Seq2Seq_Tensor.ipynb), [Seq2Seq_Torch.ipynb](https://colab.research.google.com/github/graykode/nlp-tutorial/blob/master/4-1.Seq2Seq/Seq2Seq_Torch.ipynb)
- 4-2. [Seq2Seq with Attention](https://github.com/graykode/nlp-tutorial/tree/master/4-2.Seq2Seq(Attention)) - **Translate**
  - Paper - [Neural Machine Translation by Jointly Learning to Align and Translate(2014)](https://arxiv.org/abs/1409.0473)
  - Colab - [Seq2Seq(Attention)_Tensor.ipynb](https://colab.research.google.com/github/graykode/nlp-tutorial/blob/master/4-2.Seq2Seq(Attention)/Seq2Seq(Attention)_Tensor.ipynb), [Seq2Seq(Attention)_Torch.ipynb](https://colab.research.google.com/github/graykode/nlp-tutorial/blob/master/4-2.Seq2Seq(Attention)/Seq2Seq(Attention)_Torch.ipynb)
- 4-3. [Bi-LSTM with Attention](https://github.com/graykode/nlp-tutorial/tree/master/4-3.Bi-LSTM(Attention)) - **Binary Sentiment Classification**
  - Colab - [Bi_LSTM(Attention)_Tensor.ipynb](https://colab.research.google.com/github/graykode/nlp-tutorial/blob/master/4-3.Bi-LSTM(Attention)/Bi_LSTM(Attention)_Tensor.ipynb), [Bi_LSTM(Attention)_Torch.ipynb](https://colab.research.google.com/github/graykode/nlp-tutorial/blob/master/4-3.Bi-LSTM(Attention)/Bi_LSTM(Attention)_Torch.ipynb)


#### 5. Model based on Transformer

- 5-1.  [The Transformer](https://github.com/graykode/nlp-tutorial/tree/master/5-1.Transformer) - **Translate**
  - Paper - [Attention Is All You Need(2017)](https://arxiv.org/abs/1706.03762)
  - Colab - [Transformer_Torch.ipynb](https://colab.research.google.com/github/graykode/nlp-tutorial/blob/master/5-1.Transformer/Transformer_Torch.ipynb), [Transformer(Greedy_decoder)_Torch.ipynb](https://colab.research.google.com/github/graykode/nlp-tutorial/blob/master/5-1.Transformer/Transformer(Greedy_decoder)_Torch.ipynb)
- 5-2. [BERT](https://github.com/graykode/nlp-tutorial/tree/master/5-2.BERT) - **Classification Next Sentence & Predict Masked Tokens**
  - Paper - [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding(2018)](https://arxiv.org/abs/1810.04805)
  - Colab - [BERT_Torch.ipynb](https://colab.research.google.com/github/graykode/nlp-tutorial/blob/master/5-2.BERT/BERT_Torch.ipynb)

|           Model            |              Example               |   Framework   | Lines(torch/tensor) |
| :------------------------: | :--------------------------------: | :-----------: | :-----------------: |
|            NNLM            |         Predict Next Word          | Torch, Tensor |        67/83        |
|     Word2Vec(Softmax)      |   Embedding Words and Show Graph   | Torch, Tensor |        77/94        |
|          TextCNN           |      Sentence Classification       | Torch, Tensor |        94/99        |
|          TextRNN           |         Predict Next Step          | Torch, Tensor |        70/88        |
|          TextLSTM          |            Autocomplete            | Torch, Tensor |        73/78        |
|          Bi-LSTM           | Predict Next Word in Long Sentence | Torch, Tensor |        73/78        |
|          Seq2Seq           |            Change Word             | Torch, Tensor |       93/111        |
|   Seq2Seq with Attention   |             Translate              | Torch, Tensor |       108/118       |
|   Bi-LSTM with Attention   |  Binary Sentiment Classification   | Torch, Tensor |       92/104        |
|        Transformer         |             Translate              |     Torch     |        222/0        |
| Greedy Decoder Transformer |             Translate              |     Torch     |        246/0        |
|            BERT            |            how to train            |     Torch     |        242/0        |


## Dependencies

- Python 3.5+
- Tensorflow 1.12.0+
- Pytorch 0.4.1+
- Plan to add Keras Version



# 应用


## 资料

- 更多[Demo地址](http://wqw547243068.github.io/demo)

# 结束


